{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24_ZPIMMZx5R"
   },
   "source": [
    "# Часть 1. Сегментация\n",
    "\n",
    "### Задача\n",
    "\n",
    "Обучите нейронную сеть сегментировать границы клеток.\n",
    "\n",
    "В этой задаче вам не будут предоставлены какие-либо фрагменты кода, только входные данные и целевая метрика - intersection-over-union (IoU).\n",
    "\n",
    "Вы должны обучить свою нейронную сеть предсказывать маску краевых пикселей (пикселей в ground truth изображениях со значением больше 0).\n",
    "\n",
    "Используйте все, что вы узнали на данный момент:\n",
    "* любые архитектуры для семантической сегментации\n",
    "* аугментации (понадобятся, так как в трейн выборке всего 41 изображение)\n",
    "* fine-tuning\n",
    "\n",
    "Не разрешается только одна вещь: тренировать сеть на тест сете.\n",
    "\n",
    "Финальное решение будет состоять из jupyter ноутбука с кодом (для обучения сети + любые эксперименты с данными) и архива с изображениями png с предсказаниями сети для тестовых изображений (одноканальные изображения, 0 - для некраевых пикселей, любое ненулевое значение для краевых пикселей).\n",
    "\n",
    "Хорошая сеть должна уметь сегментировать изображения с iou >= 0.29. Это не строгий критерий, но попытайтесь получить аналогичные или лучшие числа.\n",
    "\n",
    "Практические заметки:\n",
    "* В датасете присутствует сильный дисбаланс классов, поэтому предсказания будут \"смещены\" в сторону \"нулевого\" класса. Вы можете либо настроить минимальный порог вероятности для класса «граница», либо добавить вес классу в лосс функции.\n",
    "* Датасет - маленький, активно используйте аугментации: вращение, отражение, случайный контраст и яркость\n",
    "* Лучше потратье время на эксперименты с нейронной сетью, чем на постпроцессинг (т.е. test-time augmentation).\n",
    "* Имейте в виду, что архитектура сети определяет receptive field пикселей на выходе. Если размер картинки на вхож меньше, чем receptive field пикселя на выходе, вы можете попробовать \"выкинуть\" сколько-то слоёв из сети без потери качества. Вполне нормально изменять \"готовые\" (of-the-shelf) архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNJ0DlSRZx5S"
   },
   "outputs": [],
   "source": [
    "### Download the dataset ###\n",
    "!wget https://www.dropbox.com/s/jy34yowcf85ydba/data.zip?dl=0 -O data.zip\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btAFQIMhZx5V"
   },
   "outputs": [],
   "source": [
    "### Визуализируем данные ###\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "\n",
    "# Human HT29 colon-cancer cells\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "im = skimage.img_as_ubyte(io.imread('BBBC018_v1_images-fixed/train/00735-actin.DIB.bmp'))\n",
    "plt.imshow(im)\n",
    "plt.subplot(1,2,2)\n",
    "mask = skimage.img_as_ubyte(io.imread('BBBC018_v1_outlines/train/00735-cells.png'))\n",
    "plt.imshow(mask, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIqI34zlZx5Z"
   },
   "outputs": [],
   "source": [
    "### Метрика ###\n",
    "def calc_iou(prediction, ground_truth):\n",
    "    n_images = len(prediction)\n",
    "    intersection, union = 0, 0\n",
    "    for i in range(n_images):\n",
    "        intersection += np.logical_and(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum() \n",
    "        union += np.logical_or(prediction[i] > 0, ground_truth[i] > 0).astype(np.float32).sum()\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OktocnMv0NvI"
   },
   "outputs": [],
   "source": [
    "# ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3. Deep Image Prior (DIP)\n",
    "\n",
    "\n",
    "### Основная идея\n",
    "\n",
    "Идея статьи состоит в том, что сверточные сети задают хороший \"prior\" на изображения. Подробнее в [1].\n",
    "\n",
    "Представьте, что у вас есть \"испорченное\" изображение $x_0$ и оригинальное хорошее изображение $x$.\n",
    "\n",
    "$x_0$ может представлять зашумленное изображение (задача denoising), либо часть изображения отсутствует (задача inpainting), либо например изображение маленького размера (задача super-resolution)\n",
    "\n",
    "Зададим $x$ следующим образом:\n",
    "\n",
    "$x = f_\\theta(\\mathcal{z})$\n",
    "\n",
    "где $f$ - нейронная сеть, $z$ - фиксированный input\n",
    "\n",
    "Будем восстанавливать $x$ с помощью следующей оптимизационной процедуры:\n",
    "\n",
    "1. Задаем и фиксируем $z$, как тензор состоящий например из равномерного шума, высотой и шириной как изображение $x$, но с числом каналов, например 32\n",
    "2. Инциализиурем веса сети $\\theta$\n",
    "3. Обновляем веса сети с помощью оптимизации функции ошибки $E$:\n",
    "\n",
    "$$\n",
    "\\min_\\theta E(f_\\theta(z), x_0).\n",
    "$$\n",
    "\n",
    "4. Повторяем шаг 3 (процедуру оптимизации) $M$ раз, и останавливаем её\n",
    "5. Смотрим на результат: $f_\\theta(\\mathcal{z})$\n",
    "\n",
    "\n",
    "### Задача\n",
    "\n",
    "Главная цель - вопроизвести результаты статьи [1].\n",
    "\n",
    "1. выполните denoising изображений в папке \"data/denoising\" ($x_0 = x + \\text{гауссовский шум}$),\n",
    "2. выполните inpainting изображений в папке \"data/inpaiting\", (примените лосс только на известный участок изображения, используя предоставленную маску)  ($x_0 = x \\text{ c вырезанной маской}$)\n",
    "\n",
    "используя Mean Square Error (MSE) как функцию ошибки $E$,\n",
    "\n",
    "с помощью UNet-подобной сети (4 downsampling слоя, и 16, 32, 64, 128 фильтра на выходе каждого блока соответственно).\n",
    "\n",
    "Как сделать подобную архитектуру:\n",
    "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5\n",
    "\n",
    "\n",
    "### Примечания\n",
    "\n",
    "- Поиграйтесь с числом итераций $M$, используемых для early stopping, и найдите оптимальные значения для denoising и inpainting задач. Код и результаты должны быть представлены в этом ноутбуке.\n",
    "- Используйте Adam как оптимизатор по умолчанию, но можете спокойно воспользоваться любым другим и даже можете попробовать другую архитектуру сети.\n",
    "\n",
    "### Ссылки\n",
    "\n",
    "[1] Ulyanov et. al., \"Deep Image Prior\", CVPR 2018, https://arxiv.org/abs/1711.10925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the dataset ###\n",
    "!wget https://www.dropbox.com/s/si5o4dp4qa59cyy/data.zip?dl=0 -O data.zip\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnDBcqfCZx5d"
   },
   "source": [
    "# Часть 2. Variational Autoencoder (VAE) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVX5lMj6Zx5e"
   },
   "source": [
    "Оригинальная статья http://arxiv.org/abs/1312.6114\n",
    "\n",
    "В этой части мы обучим автоенкодер и вариационный автоенкодер на датасете \"Labeled Faces in the Wild\" dataset (LFW) (http://vis-www.cs.umass.edu/lfw/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TFl-9lOZx5e"
   },
   "source": [
    "### Подготовьте данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xct-cu--Zx5f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Bu7XgvmzZx5h",
    "outputId": "0615422b-afae-4497-b118-8925dc6a6f63"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/hw3_19/homework03/lfw_dataset.py -O lfw_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "UkuybWiKZx5j"
   },
   "outputs": [],
   "source": [
    "#@title Utility functions\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.io\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_lfw_dataset(attrs_name = \"lfw_attributes.txt\",\n",
    "                      images_name = \"lfw-deepfunneled\",\n",
    "                      raw_images_name = \"lfw\",\n",
    "                      use_raw=False,\n",
    "                      dx=80,dy=80,\n",
    "                      dimx=45,dimy=45\n",
    "    ): # sad smile\n",
    "\n",
    "    #download if not exists\n",
    "    if (not use_raw) and not os.path.exists(images_name):\n",
    "        print(\"images not found, donwloading...\")\n",
    "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
    "        print(\"extracting...\")\n",
    "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
    "        print(\"done\")\n",
    "        assert os.path.exists(images_name)\n",
    "    \n",
    "    if use_raw and not os.path.exists(raw_images_name):\n",
    "        print(\"images not found, donwloading...\")\n",
    "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw.tgz -O tmp.tgz\")\n",
    "        print(\"extracting...\")\n",
    "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
    "        print(\"done\")\n",
    "        assert os.path.exists(raw_images_name)\n",
    "\n",
    "    if not os.path.exists(attrs_name):\n",
    "        print(\"attributes not found, downloading...\")\n",
    "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
    "        print(\"done\")\n",
    "\n",
    "    #read attrs\n",
    "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "\n",
    "\n",
    "    #read photos\n",
    "    dirname = raw_images_name if use_raw else images_name\n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(dirname):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "\n",
    "    #mass-merge\n",
    "    #(photos now have same order as attributes)\n",
    "    df_attrs['imagenum'] = df_attrs['imagenum'].astype(np.int64)\n",
    "    df = pd.merge(df_attrs, photo_ids, on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
    "\n",
    "    #image preprocessing\n",
    "    all_photos = df['photo_path'].apply(lambda img: skimage.io.imread(img))\\\n",
    "                                 .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
    "                                 .apply(lambda img: skimage.img_as_ubyte(skimage.transform.resize(img,[dimx,dimy])))\n",
    "\n",
    "    all_photos = np.stack(all_photos.values).astype('uint8')\n",
    "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "    \n",
    "    return all_photos,all_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "O8txqM2djcaS",
    "outputId": "c4b2558f-5227-4c7f-faa3-462ae7626007"
   },
   "outputs": [],
   "source": [
    "data, attrs = fetch_lfw_dataset(dimx=36,dimy=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4iBQpDfZx5l"
   },
   "outputs": [],
   "source": [
    "data = data/255\n",
    "np.savez(\"real.npz\", Pictures=data.reshape(data.shape[0], 36*36*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqec1JkSZx5n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = data[:10000].reshape((10000, -1))\n",
    "print(X_train.shape)\n",
    "X_val = data[10000:].reshape((-1, X_train.shape[1]))\n",
    "print(X_val.shape)\n",
    "\n",
    "image_h = data.shape[1]\n",
    "image_w = data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uH8b2iyqZx5p"
   },
   "source": [
    "For simplicity we want all values of the data to lie in the interval $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOjNorBwZx5p"
   },
   "outputs": [],
   "source": [
    "X_train = np.float32(X_train)\n",
    "X_val = np.float32(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvO4eTOVZx5s"
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.5 * n_col, 1.7 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZVL7m3eZx5u"
   },
   "outputs": [],
   "source": [
    "plot_gallery(X_train, image_h, image_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6DpvRYmZx5w"
   },
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.Tensor(X_train), torch.zeros(X_train.shape[0],)) # pseudo labels needed to define TensorDataset\n",
    "train_loader = data_utils.DataLoader(train, batch_size=100, shuffle=True)\n",
    "\n",
    "val = data_utils.TensorDataset(torch.Tensor(X_val), torch.zeros(X_val.shape[0],))\n",
    "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rg7IQc_iZx5x"
   },
   "source": [
    "## Autoencoder\n",
    "\n",
    "В чем суть вариационного автоенкодера со всеми сложными формулами и регуляризациями? Чтобы ощутить разницу обучите сначала автоенкодер:\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png\" alt=\"Autoencoder\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5mDIVgHZx5y"
   },
   "outputs": [],
   "source": [
    "dimZ = 100 # Учитывая, что задача реконструкции лица, какой размер скрытого представления кажется разумным?\n",
    "\n",
    "# Определите декодер и енкодер как сети с 1-м скрытым полносвязным слоем\n",
    "# (это будет означать что в каждой сети будет 2 полносвзяных слоя)\n",
    "# Используйте ReLU на активациях скрытых слоёв\n",
    "# GlorotUniform инициализация для W\n",
    "# Zero инициализация для biases\n",
    "# Удобно добавить sigmoid активацию на выход сети, чтобы получить нормализованный output\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        #TODO\n",
    "        \n",
    "        # self.encoder = \n",
    "        # self.decoder =\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #TODO\n",
    "        \n",
    "        # latent_code = \n",
    "        # reconstruction = \n",
    "        \n",
    "        return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRvtFNPTZx5z"
   },
   "outputs": [],
   "source": [
    "# Создаём MSE loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "autoencoder = Autoencoder().cuda()\n",
    "\n",
    "# используем Adam оптимизиатор\n",
    "optimizer = optim.Adam(autoencoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksaeLLMSZx51",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Обучите autoencoder\n",
    "# Отобразите прогресс реконструкции изображения и падение лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1q8hW11gZx53",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Отобразите реконструкции\n",
    "for j, data in enumerate(val_loader, 0):\n",
    "    inp = data[0].cuda()\n",
    "    pred, _ = autoencoder(inp)\n",
    "    plot_gallery([data[0].numpy(), pred.data.cpu().numpy()], image_h, image_w, n_row=1, n_col=2)\n",
    "    if (j >= 9):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4OzNnTkZx55"
   },
   "source": [
    "Реконструкция не так уж и плоха, да?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwalrtXkZx56"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKZK_2RfZx56"
   },
   "outputs": [],
   "source": [
    "for i, (putin, y) in enumerate(val_loader):\n",
    "    if i == 2754:\n",
    "        break\n",
    "plt.imshow(putin.numpy().reshape((image_w, image_w, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djHncHaAZx57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.suptitle('Twin farm')\n",
    "for i in range(len(image_progress[:20])):\n",
    "    plt.subplots_adjust(bottom=0.0, left=.1, right=.9, top=.50, hspace=.15)\n",
    "    plt.subplot(6, 5, 5*(i//5) + i % 5 + 1)\n",
    "    plt.imshow(image_progress[i].clamp(0,1).data.cpu().numpy().reshape(image_w, image_h, 3))\n",
    "    plt.title('Epoch = {}'.format(i * 5 + 1))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69-Q9RgpZx59"
   },
   "source": [
    "Давайте насемплим несколько случайных латентных векторов $z$ и сделаем inference - реконструируем изображения из $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T55uuFXWZx59"
   },
   "outputs": [],
   "source": [
    "z = (np.random.randn(25, dimZ)*0.5).astype('float32')\n",
    "output = autoencoder.decoder(torch.from_numpy(z).cuda()).clamp(0, 1)\n",
    "plot_gallery(output.data.cpu().numpy(), image_h, image_w, n_row=5, n_col=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbkkzpMwZx5_"
   },
   "source": [
    "Если мы будем семплить $z$ из нормального распределения, мы в итоге получим на выходе все возможные лица? Как вы считаете?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZlGfb8BZx5_"
   },
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N68bfBtiZx6A"
   },
   "source": [
    "Байесовский подход в глубоком обучении рассматривает всё в терминах распределений. Теперь наш енкодер будет генерировать не просто $z$, но и апостериорное распределение $q(z|x)$. В нашем случае распределение $q$ является гауссовским распределением $N(\\mu, \\sigma)$ с параметрами $\\mu$, $\\sigma$. Технически, первое отличие заключается в том, что вам нужно разделить bottleneck слой на два слоя. Один полносвязный слой будет генерировать вектор $\\mu$, а другой - вектор $\\sigma$. Reparametrization trick должен быть реализован с помощью **gaussian_sampler** слоя, который генерирует случайный вектор $\\epsilon$ и возвращает $z=\\mu+\\sigma\\epsilon \\sim N(\\mu, \\sigma)$.\n",
    "\n",
    "Поскольку наш декодер также является функцией, которая генерирует распределение, нам необходимо выполнить такое же разбиение для выходного слоя. При тестировании модели мы будем смотреть только на средние значения, поэтому одной частью выходных данных будет фактический выходной сигнал автоэнкодера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5CxSWMWZx6A"
   },
   "source": [
    "Реализуйте простейшую версию VAE - один $z$ на вход. Можно также попробовать рассмотреть выборку из нескольких $z$  на один вход и далее усреднить их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kRqzXdBZx6B"
   },
   "outputs": [],
   "source": [
    "# чтобы сравниться с autoencoder выберите такой же dimZ как и до этого\n",
    "dimZ = 100\n",
    "\n",
    "# напишите сеть\n",
    "# можно посмотреть на код семинара, либо https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        #TODO\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            std = logsigma.exp()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #TODO\n",
    "        # return reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkBBlJQzZx6C"
   },
   "source": [
    "Функционал, который будем оптимизировать для VAE имеет собственное название - variational lowerbound. Мы будем его максимизировать. Вот он (один $z$ на вход $x$):\n",
    "\n",
    "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
    "\n",
    "Реализуйте две функции, одна из которых будет считать KL-дивергенцию, а другая log-likelihood вашего output. Вот необходимая математика для удобства:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
    "$$\\log p_{\\theta}(x|z) = \\sum_{i=1}^{dimX}\\log p_{\\theta}(x_i|z)=\\sum_{i=1}^{dimX} \\log \\Big( \\frac{1}{\\sigma_i\\sqrt{2\\pi}}e^{-\\frac{(\\mu_i-x)^2}{2\\sigma_i^2}} \\Big)=...$$\n",
    "\n",
    "Не забывайте, что вы используете $\\log\\sigma$ на вход. Почему не просто $\\sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmNT4AL7Zx6D"
   },
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    return 0\n",
    "\n",
    "def log_likelihood(x, mu, logsigma):\n",
    "    return 0\n",
    "\n",
    "def loss_vae(x, mu_gen, logsigma_gen, mu_z, logsigma_z):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdcOxvpjZx6F"
   },
   "source": [
    "Теперь учим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9FcS-khzZx6F"
   },
   "outputs": [],
   "source": [
    "# обучите VAE\n",
    "# Отобразите прогресс реконструкции изображения и падение лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TC2YILWOZx6H"
   },
   "outputs": [],
   "source": [
    "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=True)\n",
    "vae.eval()\n",
    "for j, data in enumerate(val_loader, 0):\n",
    "    input = data[0].cuda()\n",
    "    reconstruction_mu, _, _, _ = vae(input)\n",
    "    plot_gallery([data[0].numpy(), reconstruction_mu.data.cpu().numpy()], image_h, image_w, n_row=1, n_col=2)\n",
    "    if (j >= 9):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hBy3QcBZx6J"
   },
   "source": [
    "И теперь насемплим с помощью VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqk7uRQ2Zx6J"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Насэмплите (сгенерируйте) изображения из выученного распределения\n",
    "# 1) Sample z ~ N(0,1)\n",
    "# 2) Sample from N(decoder_mu(z), decoder_sigma(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3v1fz9osZx6L"
   },
   "source": [
    "Даже если на практике вы не видите большой разницы между AE и VAE, или VAE еще хуже, маленький \"Байес\" внутри вас должен прыгать от радости прямо сейчас.\n",
    "\n",
    "В VAE вы можете по-настоящему семплить из распределения изображений $p(x)$, тогда как в AE нет простого и правильного способа сделать это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "X6R8w25OZx6P"
   },
   "source": [
    "## И напоследок!\n",
    "\n",
    "Если вам удалось обучить свои автоэнкодеры и они что-то узнали о мире, то пришло время воспользоваться этим. Как вы могли заметить, в наборе данных есть атрибуты лица. Нас интересует параметр-колонка \"Smiling\", но можете поробовать и другие! Вот первая задача:\n",
    "\n",
    "1) Извлеките атрибут \"Smilling\" и создайте два набора изображений: 10 улыбающихся и 10 не улыбающихся.\n",
    "\n",
    "2) Вычислите скрытые (латентные) представления для каждого изображения в «улыбающемся» наборе и усредните эти вектора. Сделайте то же самое для \"не улыбающегося\" набора. Вы тем самым нашли **\"векторное представление\"** атрибутов \"smile\" и \"no smile\".\n",
    "\n",
    "3) Вычислите разницу: вектор «улыбка» минус вектор «не улыбка».\n",
    "\n",
    "3) Теперь проверьте, работает ли **«арифметика признаков (фичей)»**. Возьмите лицо без улыбки, закодируйте его с помощью екнодера, добавьте к нему разницу из пункта 3) и подавйте на вход декодеру. Проверьте работает ли в случае AE? в случае VAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nbRQOiOhS_E"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "81qg_hZ8Zx6L",
    "pCJKnRqSZx6N"
   ],
   "name": "homework_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
