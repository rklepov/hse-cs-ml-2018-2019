{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch - фреймворк на питоне для машинного / глубокого обучения\n",
    "\n",
    "- Ускоренные на GPU операции\n",
    "- Автоматическое дифференцирование\n",
    "- Модули для нейронных сетей\n",
    "\n",
    "Этот урок научит вас основам работы с тензорами и сетями в Pytorch.\n",
    "\n",
    "![](karpathy.png)\n",
    "\n",
    "Установка https://pytorch.org/get-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# если гпу у вас несколько укажите номер гпу, которую хотите использовать\n",
    "# номер свободной можно посмотреть с помощью nvidia-smi в терминале\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензоры являются фундаментальным объектом для массивов данных. Наиболее распространенные типы, которые вы будете использовать, это `IntTensor` и `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем пустой тензор\n",
    "x = torch.FloatTensor(2,3)\n",
    "print(x)\n",
    "# Инициализируем нулями\n",
    "x.zero_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем numpy array (seed для воспроизведения результатов)\n",
    "np.random.seed(123)\n",
    "np_array = np.random.random((2,3))\n",
    "print(torch.FloatTensor(np_array))\n",
    "print(torch.from_numpy(np_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создаем тензор с случайными данными (seed для воспроизведения результатов)\n",
    "torch.manual_seed(123)\n",
    "x=torch.randn(2,3)\n",
    "print(x)\n",
    "# Экспортируем numpy array\n",
    "x_np = x.numpy()\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Особые тензоры (смотрим документацию)\n",
    "print(torch.eye(3))\n",
    "print(torch.ones(2,3))\n",
    "print(torch.zeros(2,3))\n",
    "print(torch.arange(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все тензоры имеют размер `size` и тип `type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.FloatTensor(3,4)\n",
    "print(x.size())\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Математика, Линейная алгебра, и индексирование (обзор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математика Pytorch и линейная алгебра в Pytorch похожа на NumPy. Операторы переопределяются, поэтому вы можете использовать стандартные математические операторы (`+`, `-` и т.д.) и ожидать в результате тензор. Смотрим документацию Pytorch для полного списка доступных функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0,5).float()\n",
    "print(torch.sum(x))\n",
    "print(torch.sum(torch.exp(x)))\n",
    "print(torch.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексирование в Pytorch похоже на индексирование в numpy. Смотрим документацию Pytorch для деталей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,2)\n",
    "print(x)\n",
    "print(x[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU и GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор можно копировать между CPU и GPU. Важно, чтобы все, что участвует в расчете, было на одном устройстве.\n",
    "\n",
    "Эта часть кода может не сработать, если у вас нет доступного GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "x = torch.rand(3,2)\n",
    "print(x)\n",
    "# copy to GPU\n",
    "y = x.cuda()\n",
    "print(y)\n",
    "# copy back to CPU\n",
    "z = y.cpu()\n",
    "print(z)\n",
    "# get CPU tensor as numpy array\n",
    "print(z.numpy())\n",
    "# cannot get GPU tensor as numpy array directly\n",
    "try:\n",
    "    y.numpy()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операции между тензорами на GPU и CPU не будут выполнены. Операции требуют, чтобы все аргументы были на одном устройстве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,5)  # CPU tensor\n",
    "y = torch.rand(5,4).cuda()  # GPU tensor\n",
    "try:\n",
    "    torch.mm(x,y)  # Operation between CPU and GPU fails\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типичный код должен включать в себя операторы `if` или использовать вспомогательные функции, чтобы он мог работать с GPU или без него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put tensor on CUDA if available\n",
    "x = torch.rand(3,2)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "\n",
    "# Do some calculations\n",
    "y = x ** 2 \n",
    "\n",
    "# Copy to CPU if on GPU\n",
    "if y.is_cuda:\n",
    "    y = y.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобным методом является `new`, который создает новый тензор на том же устройстве, что и другой тензор. Его следует использовать для создания тензоров, когда это возможно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand(3,2)\n",
    "x2 = x1.new(1,2)  # create cpu tensor\n",
    "print(x2)\n",
    "x1 = torch.rand(3,2).cuda()\n",
    "x2 = x1.new(1,2)  # create cuda tensor\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчеты, выполняемые на GPU, могут быть во много раз быстрее, чем просто в numpy. Однако numpy по-прежнему оптимизирован для процессора и во много раз быстрее, чем циклы python `for`. Numpy вычисления могут быть быстрее, чем вычисления GPU для небольших массивов из-за стоимости взаимодействия с GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "# Create random data\n",
    "x = torch.rand(1000,64)\n",
    "y = torch.rand(64,32)\n",
    "number = 10000  # number of iterations\n",
    "\n",
    "def square():\n",
    "    z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
    "\n",
    "# Time CPU\n",
    "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
    "# Time GPU\n",
    "x, y = x.cuda(), y.cuda()\n",
    "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дифференцирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензоры обеспечивают автоматическое дифференцирование\n",
    "\n",
    "Что нужно знать:\n",
    "\n",
    "- Тензоры, по которым вы дифференцируетесь, должны иметь `require_grad = True`\n",
    "- Вызвать `.backward ()` для скалярных переменных, которые вы дифференцируете\n",
    "- Чтобы дифференцировать вектор, сначала суммируйте его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create differentiable tensor\n",
    "x = torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
    "# Calculate y=sum(x**2)\n",
    "y = x**2\n",
    "# Calculate gradient (dy/dx=2x)\n",
    "y.sum().backward()\n",
    "# Print values\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дифференцирование накапливает градиенты. Иногда это то, что вы хотите, а иногда нет. **Обязательно обнуляйте градиенты между батчами при выполнении градиентного спуска, иначе вы получите странные результаты!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable\n",
    "x=torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
    "# Differentiate\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Differentiate again (accumulates gradient)\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Zero gradient before differentiating\n",
    "x.grad.data.zero_()\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что Тензор с градиентом не может быть экспортирован напрямую в numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
    "x.numpy() # raises an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Причина в том, что pytorch запоминает граф всех вычислений для выполнения дифференцирования. Для интеграции в этот граф необработанные данные внутренне обертываются в класс Tensor (как, например, переменная). Вы можете отсоединить тензор от графа, используя метод **.detach()**, который возвращает тензор с теми же данными, но для параметра require_grad установлено значение False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(torch.arange(0,4).float(), requires_grad=True)\n",
    "y=x**2\n",
    "z=y**2\n",
    "z.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другая причина использовать этот метод заключается в том, что для обновления графа может потребоваться много памяти. Если вы находитесь в ситуации, где у вас есть тензор который вам не нужно дифференцировать, подумайте об отделении его от графа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch предоставляет основу для разработки модулей нейронной сети. Они заботятся о многих вещах, главной из которых является упаковка и отслеживание списка параметров для вас.\n",
    "У вас есть несколько способов построения и использования сети, предлагая различные компромиссы между свободой и простотой.\n",
    "\n",
    "torch.nn предоставляет базовые однослойные сети, такие как линейные (перцептрон) и слои активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.arange(0,32).float()\n",
    "net = torch.nn.Linear(32,10)\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все объекты nn.Module могут использоваться в качестве компонентов больших сетей! Вот как можно построить свою собственную сеть. Самый простой способ - использовать класс nn.Sequential.\n",
    "\n",
    "Вы также можете создать свой собственный класс, который наследует nn.Module. Метод forawrd должен точно определять, что происходит при прохождении через слой с учетом входных данных. Это позволяет более точно определять поведение, чем просто накладывать слои один за другим, если это необходимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple sequential network (`nn.Module` object) from layers (other `nn.Module` objects).\n",
    "# Here a MLP with 2 layers and sigmoid activation.\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(32,128),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(128,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a more customizable network module (equivalent here)\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    # you can use the layer sizes as initialization arguments if you want to\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input_val):\n",
    "        h = input_val\n",
    "        h = self.layer1(h)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        return h\n",
    "\n",
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть отслеживает параметры, и вы можете получить к ним доступ через метод **parameters()**, который возвращает python генератор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры имеют тип Parameter, который в основном является оберткой для тензора. Как Pytorch получает параметры вашей сети? Это просто все атрибуты типа Parameter в вашей сети. Более того, если атрибут имеет тип nn.Module, его параметры добавляются в параметры вашей сети! Вот почему, когда вы определяете сеть путем добавления базовых компонентов, таких как nn.Linear, вам никогда не придется явно определять параметры.\n",
    "\n",
    "Однако, если вы находитесь в случае, когда ни один из стандартных модулей в Pytorch не делает то, что вам нужно, вы можете определить параметры явно (такое бывает редко). Для записи давайте создадим предыдущий MLP с персонализированными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetworkWithParams(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(MyNetworkWithParams,self).__init__()\n",
    "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
    "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
    "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
    "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
    "        return output\n",
    "\n",
    "net = MyNetworkWithParams(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры полезны тем, что они должны быть всеми весами сети, которые будут оптимизированы во время обучения. Если вам нужно было использовать тензор в вашем вычислительном графе, который вы хотите оставить постоянным, просто определите его как обычный тензор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module также предоставляет функции потерь, например кросс-энтропия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
    "y = torch.tensor([0,3,9])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.CrossEntropyLoss выполняет как softmax, так и саму кросс-энтропию: имея $output$ размера $(n,d)$ и $y$ размера $n$ и значения $0,1,...,d-1$, он вычисляет $\\sum_{i=0}^{n-1}log(s[i,y[i]])$ где $s[i,j] = \\frac{e^{output[i,j]}}{\\sum_{j'=0}^{d-1}e^{output[i,j']}}$\n",
    "\n",
    "Вы также можете объединить nn.LogSoftmax и nn.NLLLoss, чтобы получить тот же результат. Обратите внимание, что все они используют log-softmax, а не softmax, для стабильности вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent\n",
    "criterion2 = nn.NLLLoss()\n",
    "sf = nn.LogSoftmax()\n",
    "output = net(x)\n",
    "loss = criterion(sf(output),y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы выполнить backpropagation, просто выполните **loss.backward()**! Это обновит градиенты во всех дифференцируемых тензорах в графе, который, в частности, включает в себя все параметры сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Check that the parameters now have gradients\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if I forward prop and backward prop again, gradients accumulate :\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)\n",
    "\n",
    "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
    "net.zero_grad()\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы делали backpropagation, но все еще не выполняли градиентный спуск. Давайте определим оптимизатор на параметрах сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Parameters before gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Parameters after gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a training loop, we should perform many GD iterations.\n",
    "n_iter = 1000\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
    "    output = net(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(x)\n",
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы знаете, как тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of keys to weights using `state_dict`\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28,256),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(256,10))\n",
    "print(net.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dictionary\n",
    "torch.save(net.state_dict(),'test.pth')\n",
    "# load a dictionary\n",
    "net.load_state_dict(torch.load('test.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common issues to look out for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = nn.Linear(4,2)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# правильно\n",
    "net = nn.Linear(4,2)\n",
    "x = torch.tensor([1.,2.,3.,4.])\n",
    "# x = torch.tensor([1,2,3,4]).float()\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не путаем матричное и поэлементное умножение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2* torch.ones(2,2)\n",
    "y = 3* torch.ones(2,2)\n",
    "print(x * y)\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4,5)\n",
    "y = torch.arange(5)\n",
    "print(x.size(), y.size())\n",
    "print(x+y)\n",
    "y = torch.arange(4).view(-1,1)\n",
    "print(x.size(), y.size())\n",
    "print(x+y)\n",
    "y = torch.arange(4)\n",
    "print(x.size(), y.size())\n",
    "print(x+y) # exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View и Transpose ведут себя по-разному"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "print(x.t())\n",
    "print(x.view(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,120))\n",
    "x = torch.ones(256,2048)\n",
    "y = torch.zeros(256).long()\n",
    "\n",
    "# правильно добавить:\n",
    "# x = x.cuda()\n",
    "# y = y.cuda()\n",
    "\n",
    "net.cuda()\n",
    "x.cuda()\n",
    "crit=nn.CrossEntropyLoss()\n",
    "out = net(x)\n",
    "loss = crit(out,y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# неправильный вариант, параметры скрытых слоев не учитываются и не будут обучаться\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "    \n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out\n",
    "\n",
    "net = MyNet(2)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# правильный вариант\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "        self.hidden = nn.ModuleList(self.hidden)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out\n",
    "    \n",
    "net = MyNet(2)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
