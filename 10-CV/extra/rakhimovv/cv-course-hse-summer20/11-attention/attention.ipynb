{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desctiption bewlow is taken from appendix in DETR.\n",
    "\n",
    "**Multi-head** The general form of _multi-head attention_ with M heads of dimension d is a function with the following signature (using $d^{\\prime}=\\frac{d}{M}$ (default), and giving matrix/tensors sizes in underbrace)\n",
    "\n",
    "$$\\text { mh-attn }: \\underbrace{X_{\\mathrm{q}}}_{d \\times N_{\\mathrm{q}}}, \\underbrace{X_{\\mathrm{kv}}}_{d \\times N_{\\mathrm{kv}}}, \\underbrace{T}_{M \\times 3 \\times d^{\\prime} \\times d}, \\underbrace{L}_{d \\times d} \\mapsto \\underbrace{\\tilde{X}_{\\mathrm{q}}}_{d \\times N_{\\mathrm{q}}}$$\n",
    "\n",
    "where $X_q$ is the query sequence of length $N_{\\mathrm{q}}, X_{\\mathrm{kv}}$ is the key-value sequence of length $N_{\\mathrm{kv}}$ (with the same number of channels $d$ for simplicity of exposition), $T$ is the weight tensor to compute the so-called query, key and value embeddings, and $L$ is a projection matrix. The output is the same size as the query sequence. To fix the vocabulary before giving details, multi-head self-attention ($\\operatorname{mh-s-attn}$) is the special case $X_{\\mathrm{q}}=X_{\\mathrm{kv}}$, i.e.\n",
    "\n",
    "$$\\operatorname{mh-s-attn} (X, T, L)=\\operatorname{mh-attn} (X, X, T, L)$$\n",
    "\n",
    "The multi-head attention is simply the concatenation of M single attention\n",
    "heads followed by a projection with L. The common practice is to use residual connections, dropout and layer normalization. In other words, denoting $\\tilde{X}_{\\mathrm{q}} = \\operatorname{mh-attn}\\left(X_{\\mathrm{q}}, X_{\\mathrm{kv}}, T, L\\right)$ and $X_{\\mathrm{q}}^{\\prime}$ the concatenation of attention heads, we have\n",
    "\n",
    "$$X_{\\mathrm{q}}^{\\prime}=\\left[\\operatorname{attn}\\left(X_{\\mathrm{q}}, X_{\\mathrm{kv}}, T_{1}\\right) ; \\ldots ; \\operatorname{attn}\\left(X_{\\mathrm{q}}, X_{\\mathrm{kv}}, T_{M}\\right)\\right]$$\n",
    "\n",
    "$$\\tilde{X}_{\\mathrm{q}}=\\operatorname{layernorm}\\left(X_{\\mathrm{q}}+\\operatorname{dropout}\\left(L X_{\\mathrm{q}}^{\\prime}\\right)\\right)$$\n",
    "\n",
    "where [;] denotes concatenation on the channel axis.\n",
    "\n",
    "**Single head** An attention head with weight tensor $T^{\\prime} \\in \\mathbb{R}^{3 \\times d^{\\prime} \\times d}$, denoted by $\\operatorname{attn}\\left(X_{\\mathrm{q}}, X_{\\mathrm{kv}}, T^{\\prime}\\right)$, depends on additional positional encoding $P_{\\mathrm{q}} \\in \\mathbb{R}^{d \\times N_{\\mathrm{q}}}$ and\n",
    "$P_{\\mathrm{kv}} \\in \\mathbb{R}^{d \\times N_{\\mathrm{kv}}}$. It starts by computing so-called query, key and value embeddings after adding the query and key positional encodings:\n",
    "\n",
    "$$[Q ; K ; V]=\\left[T_{1}^{\\prime}\\left(X_{\\mathrm{q}}+P_{\\mathrm{q}}\\right) ; T_{2}^{\\prime}\\left(X_{\\mathrm{kv}}+P_{\\mathrm{kv}}\\right) ; T_{3}^{\\prime} X_{\\mathrm{kv}}\\right]$$\n",
    "\n",
    "where $T^{\\prime}$ is the concatenation of $T_{1}^{\\prime}, T_{2}^{\\prime}, T_{3}^{\\prime}$. The _attention weights_ $\\alpha$ are then computed based on the softmax of dot products between queries and keys, so that each element of the query sequence attends to all elements of the key-value sequence ($i$ is a query index and $j$ a key-value index):\n",
    "\n",
    "$$\\alpha_{i, j}=\\frac{e^{\\frac{1}{\\sqrt{d^{\\prime}}} Q_{i}^{T} K_{j}}}{Z_{i}} \\text{ where } Z_{i}=\\sum_{j=1}^{N_{\\mathrm{kv}}} e^{\\frac{1}{\\sqrt{d^{\\prime}}} Q_{i}^{T} K_{j}}$$\n",
    "\n",
    "The final output is the aggregation of values weighted by attention weights: The $i$-th row is given by $\\operatorname{attn}_{i}\\left(X_{\\mathrm{q}}, X_{\\mathrm{kv}}, T^{\\prime}\\right)=\\sum_{j=1}^{N_{\\mathrm{kv}}} \\alpha_{i, j} V_{j}$.\n",
    "\n",
    "**Feed-forward network (FFN) layers** The original transformer alternates multi-head attention and so-called FFN layers, which are effectively multi-layer 1x1 convolutions, which have $M d$ input and output channels in our case. The FFN we consider is composed of two-layers of 1x1 convolutions with ReLU activations. There is also a residual connection/dropout/layernorm after the two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_head):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temper = math.sqrt(d_head)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        :param q: query (M, b, n_q, d_head)\n",
    "        :param k: key (M, b, n_kv, d_head)\n",
    "        :param v: value (M, b, n_kv, d_head)\n",
    "        \"\"\"\n",
    "        attn = torch.matmul(q, k.transpose(2, 3)) / self.temper  # (M, b, n_q, n_kv)\n",
    "        attn = torch.softmax(attn, dim=3)  # (M, b, n_q, n_kv)\n",
    "        output = torch.matmul(attn, v)  # (M, b, n_q, d_head)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, M, d, d_head):\n",
    "        \"\"\"\n",
    "        :param M: number of parallel attention heads\n",
    "        :param d: input hidden size\n",
    "        :param d_head: number of hidden units in one head (d' in text above)\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.M = M\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        self.w_q = nn.Parameter(torch.FloatTensor(M, d, d_head), requires_grad=True)\n",
    "        self.w_k = nn.Parameter(torch.FloatTensor(M, d, d_head), requires_grad=True)\n",
    "        self.w_v = nn.Parameter(torch.FloatTensor(M, d, d_head), requires_grad=True)\n",
    "        self.attention = ScaledDotProductAttention(d_head)\n",
    "        self.proj = nn.Linear(M * d_head, d, bias=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(d)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal_(self.w_q)\n",
    "        init.xavier_normal_(self.w_k)\n",
    "        init.xavier_normal_(self.w_v)\n",
    "        init.xavier_normal_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x_q, x_kv=None, p_q=None, p_kv=None):\n",
    "        \"\"\"\n",
    "        :param x_q: query sequence (b, n_q, d)\n",
    "        :param x_kv: key-value sequence (b, n_kv, d)\n",
    "        :param p_q: positional embeddings for x_q (1, n_q, d)\n",
    "        :param p_kv: positional embeddings for x_kv (1, n_kv, d)\n",
    "        \"\"\"\n",
    "        if x_kv is None:\n",
    "            x_kv = x_q\n",
    "        \n",
    "        assert (x_q.size(0), x_kv.size(2)) == (x_q.size(0), x_kv.size(2))\n",
    "        b, n_q, d = x_q.size()\n",
    "        b, n_kv, d = x_kv.size()\n",
    "        \n",
    "        if p_q is None:\n",
    "            p_q = x_q.new_zeros(1, n_q, d)\n",
    "        if p_kv is None:\n",
    "            p_kv = x_kv.new_zeros(1, n_kv, d)\n",
    "        \n",
    "        residual = x_q\n",
    "        \n",
    "        x_q = x_q.view(1, b * n_q, d).expand(self.M, b * n_q, d)\n",
    "        q = torch.bmm(x_q, self.w_q).view(self.M, b, n_q, self.d_head)  # (M, b, n_q, d_head)\n",
    "        \n",
    "        x_kv = x_kv.view(1, b * n_kv, d).expand(self.M, b * n_kv, d)\n",
    "        k = torch.bmm(x_kv, self.w_k).view(self.M, b, n_kv, self.d_head)  # (M, b, n_kv, d_head)\n",
    "        v = torch.bmm(x_kv, self.w_v).view(self.M, b, n_kv, self.d_head)  # (M, b, n_kv, d_head)\n",
    "        \n",
    "        out = self.attention(q, k, v).view(self.M * b, n_q, self.d_head)  # (M * b, n_q, d_head)\n",
    "        out = torch.cat(torch.split(out, b, dim=0), dim=-1)  # (b, n_q, M * d_head)\n",
    "        out = self.layer_norm(residual + self.dropout(self.proj(out)))  # (b, n_q, d)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward network (FFN) layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        self.ffn_layers = nn.Sequential(nn.Linear(d, d), nn.ReLU(True), nn.Linear(d, d))\n",
    "        self.layer_norm = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(x + self.ffn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10\n",
    "mha = MultiHeadAttention(2, d, 10)\n",
    "x_q = torch.rand(2, 40, d)\n",
    "x_kv = torch.rand(2, 5, d)\n",
    "mha(x_q, x_kv).shape\n",
    "\n",
    "# one_encoder_block = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-local neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/AlexHex7/Non-local_pytorch/blob/master/Non-Local_pytorch_0.4.1_to_1.1.0/lib/non_local_embedded_gaussian.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните код для multi-head attention и тот что ниже.\n",
    "\n",
    "Концептуально non-local block это signle-head attention, только вместо _layer normalization_ у нас _batch normaliztion_ и нормализация на выходе происходит _до_ skip connection в отличии от кода выше, где _после_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NonLocalBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
    "        super(_NonLocalBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [1, 2, 3]\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "            bn = nn.BatchNorm3d\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "            bn = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_nd = nn.Conv1d\n",
    "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
    "            bn = nn.BatchNorm1d\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class NONLocalBlock1D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock1D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=1, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "\n",
    "\n",
    "class NONLocalBlock2D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=2, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "\n",
    "\n",
    "class NONLocalBlock3D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock3D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=3, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 20])\n",
      "torch.Size([2, 3, 20, 20])\n",
      "torch.Size([2, 3, 8, 20, 20])\n",
      "torch.Size([2, 3, 20])\n",
      "torch.Size([2, 3, 20, 20])\n",
      "torch.Size([2, 3, 8, 20, 20])\n",
      "torch.Size([2, 3, 20])\n",
      "torch.Size([2, 3, 20, 20])\n",
      "torch.Size([2, 3, 8, 20, 20])\n",
      "torch.Size([2, 3, 20])\n",
      "torch.Size([2, 3, 20, 20])\n",
      "torch.Size([2, 3, 8, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n",
    "    img = torch.zeros(2, 3, 20)\n",
    "    net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n",
    "    out = net(img)\n",
    "    print(out.size())\n",
    "\n",
    "    img = torch.zeros(2, 3, 20, 20)\n",
    "    net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n",
    "    out = net(img)\n",
    "    print(out.size())\n",
    "\n",
    "    img = torch.randn(2, 3, 8, 20, 20)\n",
    "    net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n",
    "    out = net(img)\n",
    "    print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other types of attention besides ScaledDotProductAttention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired by https://arxiv.org/abs/1904.05873 (An Empirical Study of Spatial Attention Mechanisms in Deep Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyContentAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, M, d_head):\n",
    "        super(KeyContentAttention, self).__init__()\n",
    "        self.u = nn.Parameter(torch.FloatTensor(M, 1, d_head), requires_grad=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal_(self.u)\n",
    "\n",
    "    def forward(self, k, v, B, M=None):\n",
    "        \"\"\"\n",
    "        :param k: key (M, b, n_kv, d_head)\n",
    "        :param v: value (M, b, n_kv, d_head)\n",
    "        \"\"\"\n",
    "\n",
    "        na, b, thw, da = v.size()\n",
    "        attn = torch.bmm(self.u, k.transpose(1, 2))  # na x 1 x bthw\n",
    "        attn = attn.view(na, b, 1, thw).expand(na, b, thw, thw)\n",
    "        attn = torch.softmax(attn, dim=3)  # na, b, thw, thw\n",
    "        output = torch.matmul(attn, v)  # na, b, thw, da\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadKeyContentOnlyAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, M, d, d_head):\n",
    "        \"\"\"\n",
    "        :param M: number of parallel attention heads\n",
    "        :param d: input hidden size\n",
    "        :param d_head: number of hidden units in one head (d' in text above)\n",
    "        \"\"\"\n",
    "        super(MultiHeadKeyContentOnlyAttention, self).__init__()\n",
    "        self.M = M\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        self.w_k = nn.Parameter(torch.FloatTensor(M, d, d_head), requires_grad=True)\n",
    "        self.w_v = nn.Parameter(torch.FloatTensor(M, d, d_head), requires_grad=True)\n",
    "        self.attention = KeyContentAttention(M, d_head)\n",
    "        self.proj = nn.Linear(M * d_head, d, bias=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(d)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal_(self.w_k)\n",
    "        init.xavier_normal_(self.w_v)\n",
    "        init.xavier_normal_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x_q):\n",
    "        \"\"\"\n",
    "        :param x_q: query sequence (b, n_q, d)\n",
    "        \"\"\"\n",
    "        \n",
    "        residual = x_q\n",
    "        \n",
    "        x_kv = x_q\n",
    "        b, n_kv, d = x_kv.size()  # n_kv = n_q\n",
    "        \n",
    "        x_kv = x_kv.view(1, b * n_kv, d).expand(self.M, b * n_kv, d)\n",
    "        k = torch.bmm(x_kv, self.w_k).view(self.M, b, n_kv, self.d_head)  # (M, b, n_kv, d_head)\n",
    "        v = torch.bmm(x_kv, self.w_v).view(self.M, b, n_kv, self.d_head)  # (M, b, n_kv, d_head)\n",
    "        \n",
    "        out = self.attention(q, k, v).view(self.M * b, n_q, self.d_head)  # (M * b, n_q, d_head)\n",
    "        out = torch.cat(torch.split(out, b, dim=0), dim=-1)  # (b, n_q, M * d_head)\n",
    "        out = self.layer_norm(residual + self.dropout(self.proj(out)))  # (b, n_q, d)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "d = 10\n",
    "mhka = MultiHeadKeyContentOnlyAttention(2, d, 10)\n",
    "x_q = torch.rand(2, 40, d)\n",
    "x_kv = torch.rand(2, 5, d)\n",
    "mha(x_q, x_kv).shape\n",
    "\n",
    "# one_encoder_block = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
