{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стилизация изображений (A Neural Algorithm of Artistic Style)\n",
    "\n",
    "https://arxiv.org/abs/1508.06576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "True\n",
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.executing_eagerly())\n",
    "print(tf.keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исходные изображения и параметры алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_IMG_PATH = 'data/moscow.jpg'\n",
    "STYLE_IMG_PATH = 'data/monet.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задаем размеры выходного изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем размеры входного изображения\n",
    "height, width = cv2.imread(CONTENT_IMG_PATH).shape[:-1]\n",
    "\n",
    "# ограничиваем размер выходного изображения\n",
    "# для ускорения процесса обработки\n",
    "IMG_NROWS = min(400, height)\n",
    "IMG_NCOLS = int(width * IMG_NROWS / height)\n",
    "\n",
    "IMG_SHAPE = (IMG_NROWS, IMG_NCOLS, 3)\n",
    "\n",
    "print('Output shape: %sx%sx%s' % IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем исходные изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.imread(CONTENT_IMG_PATH)[...,::-1])\n",
    "plt.title('Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.imread(STYLE_IMG_PATH)[...,::-1])\n",
    "plt.title('Style Image - %s' % STYLE_IMG_PATH)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции предобработки изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# препроцессинг изображения для отправки в сеть vgg19\n",
    "def preprocess_image(path):\n",
    "    # все изображения приводятся к одному размеру для ускорения обработки батчем\n",
    "    img = cv2.imread(path)[...,::-1]\n",
    "    img = cv2.resize(img, dsize=(IMG_NCOLS, IMG_NROWS))\n",
    "    # применяем предобработку vgg19 \n",
    "    img = vgg19.preprocess_input(img, data_format='channels_last')\n",
    "    return np.expand_dims(img, axis=0)  # добавляем измерение батча\n",
    "\n",
    "# восстанавливаем изображение после предобработки vgg19\n",
    "def deprocess_image(x):\n",
    "    x = x.reshape(IMG_SHAPE)\n",
    "    # возвращаем смещение относительно среднего (препроцессинг vgg19)\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем предобученную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = vgg19.VGG19(weights='imagenet',\n",
    "                                input_shape=IMG_SHAPE,\n",
    "                                include_top=False)\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем выходы для копировния стиля и содержания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_LAYERS = ['block5_conv2']\n",
    "\n",
    "STYLE_LAYERS = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "content_outputs = [feature_extractor.get_layer(layer).output\n",
    "                   for layer in CONTENT_LAYERS]\n",
    "\n",
    "style_outputs = [feature_extractor.get_layer(layer).output\n",
    "                  for layer in STYLE_LAYERS]\n",
    "\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "model = tf.keras.Model(inputs=feature_extractor.input,\n",
    "                       outputs={'style': style_outputs,\n",
    "                                'content': content_outputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Задаем функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_WEIGHT = 1e-2\n",
    "CONTENT_WEIGHT = 1e4\n",
    "\n",
    "# матрица ковариации\n",
    "def gram_matrix(features):\n",
    "    # assert tf.rank(features) == 3\n",
    "    features = tf.transpose(features, (2, 0, 1))\n",
    "    features = tf.keras.backend.batch_flatten(features)\n",
    "    # вычисляем ковариации между каналами\n",
    "    gram = tf.matmul(features, features, transpose_b=True)\n",
    "    return gram / features.shape[-1]\n",
    "\n",
    "# функция потерь для стиля вычисляется на основе матрицы ковариации\n",
    "def style_loss(style, result):\n",
    "    gram_style = gram_matrix(style)\n",
    "    gram_result = gram_matrix(result)\n",
    "    return tf.reduce_mean((gram_style - gram_result) ** 2)\n",
    "\n",
    "# функция потерь для содержания\n",
    "def content_loss(content, result):\n",
    "    return tf.reduce_mean((content - result) ** 2)\n",
    "\n",
    "@tf.function\n",
    "def train_step(content_image, style_image, result_image):\n",
    "    with tf.GradientTape() as g:\n",
    "        # объединяем изображения в батч\n",
    "        input_batch = tf.concat([content_image,\n",
    "                                 style_image,\n",
    "                                 result_image], axis=0)\n",
    "        \n",
    "        # получаем значения на промежуточных\n",
    "        # слоях VGG для каждого из изображений\n",
    "        outputs = model(input_batch)\n",
    "        \n",
    "        # вычисляем значение функции потерь\n",
    "        content_losses = [content_loss(out[0], out[2])\n",
    "                          for out in outputs['content']]\n",
    "        \n",
    "        style_losses = [style_loss(out[1], out[2])\n",
    "                        for out in outputs['style']]\n",
    "        \n",
    "        loss = CONTENT_WEIGHT * tf.math.add_n(content_losses) / len(content_losses)\n",
    "        loss += STYLE_WEIGHT * tf.math.add_n(style_losses) / len(style_losses)\n",
    "    \n",
    "    # возвращаем значение функции потерь и \n",
    "    # градиенты для результирующего изображения\n",
    "    return loss, g.gradient(loss, result_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем изображения для обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для визуализация обрабатываемых изображений\n",
    "def show_images(content_image, style_image, result_image, horizontal=True):\n",
    "    rows, cols = (1, 3) if horizontal else (3, 1)\n",
    "    \n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.imshow(deprocess_image(content_image.numpy()))\n",
    "    plt.title('Content')\n",
    "\n",
    "    plt.subplot(rows, cols, 2)\n",
    "    plt.imshow(deprocess_image(style_image.numpy()))\n",
    "    plt.title('Style')\n",
    "\n",
    "    plt.subplot(rows, cols, 3)\n",
    "    plt.imshow(deprocess_image(result_image.numpy()))\n",
    "    plt.title('Result')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем изображения и подготоваливаем к прогону через vgg\n",
    "content_image = preprocess_image(CONTENT_IMG_PATH)\n",
    "content_image = tf.constant(content_image, name='content')\n",
    "\n",
    "style_image = preprocess_image(STYLE_IMG_PATH)\n",
    "style_image = tf.constant(style_image, name='style')\n",
    "\n",
    "# инициализируем результирующее изображение исходным\n",
    "result_image = preprocess_image(CONTENT_IMG_PATH)\n",
    "result_image = tf.Variable(result_image, name='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем загруженные изображения\n",
    "show_images(content_image, style_image, result_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запускаем процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем оптимизатор\n",
    "opt = tf.optimizers.Adam(learning_rate=0.75, beta_1=0.99, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 15\n",
    "for i in range(ITERATIONS):\n",
    "    # определяем значение функции потерь и градиенты для результирующего изображения\n",
    "    loss, grad = train_step(content_image, style_image, result_image)\n",
    "    step = opt.apply_gradients([(grad, result_image)])\n",
    "    print('[%03d] %.3f' % (step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем результат\n",
    "show_images(content_image, style_image, result_image, horizontal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.imread('./data/moscow.jpg')[...,::-1])\n",
    "plt.title('Original');\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(plt.imread('./data/styled_monet_0.png'))\n",
    "plt.title('Monet 0');\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(plt.imread('./data/styled_monet_1.png'))\n",
    "plt.title('Monet 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.imread('./data/moscow.jpg')[...,::-1])\n",
    "plt.title('Original');\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(plt.imread('./data/styled_picasso_0.png'))\n",
    "plt.title('Picasso 0');\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(plt.imread('./data/styled_picasso_1.png'))\n",
    "plt.title('Picasso 1');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
