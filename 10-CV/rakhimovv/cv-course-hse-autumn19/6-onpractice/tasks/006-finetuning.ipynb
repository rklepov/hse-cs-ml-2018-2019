{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CNNs\n",
    "\n",
    "В этом ноутбуке научимся файнтюнить CNN, обученную на задаче A для решения другой задачи B. Такой подход также носит название **transfer learning**.\n",
    "\n",
    "Итого:\n",
    "\n",
    "1. Рассмотрим задачу B (кошка или собака на изображении).\n",
    "2. Загрузим CNN обученную решать задачу А (1000-class [ImageNet](http://image-net.org/) classification).\n",
    "3. Научимся предсказывать классы, используя классическое машинное обучение и признаки, полученные с помощью CNN (CNN feature etractor).\n",
    "4. Заменим последний слой у CNN и будем тренировать только его для решения задачи B.\n",
    "5. Натренируем (*файнтюним*) всю сеть для решения задачи B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dogs vs Cats Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "if not os.path.exists(\"dogs_vs_cats.train.zip\"):\n",
    "    urlretrieve(\n",
    "        \"https://www.dropbox.com/s/ae1lq6dsfanse76/dogs_vs_cats.train.zip?dl=1\",\n",
    "        \"dogs_vs_cats.train.zip\")\n",
    "\n",
    "if not os.path.exists(\"dogs_vs_cats\"):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"dogs_vs_cats.train.zip\", 'r') as archive:\n",
    "        archive.extractall(\"dogs_vs_cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим на train / val и помещаем в отдельные папки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"dogs_vs_cats\"\n",
    "\n",
    "os.makedirs(os.path.join(dataset_root, \"train\", \"dog\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, \"train\", \"cat\"), exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(dataset_root, \"val\", \"dog\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_root, \"val\", \"cat\"), exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(os.path.join(dataset_root, \"train\")):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        class_name = filename[:3] # \"dog\" or \"cat\"\n",
    "        image_idx = int(filename[4:-4])\n",
    "        if image_idx % 40 == 0:\n",
    "            split = \"train\"\n",
    "        elif image_idx % 40 == 1:\n",
    "            split = \"val\"\n",
    "        else:\n",
    "            os.remove(os.path.join(dataset_root, \"train\", filename))\n",
    "            continue\n",
    "        \n",
    "        os.rename(\n",
    "            os.path.join(dataset_root, \"train\", filename),\n",
    "            os.path.join(dataset_root, split, class_name, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    os.path.join(dataset_root, \"train\"),\n",
    "    transform=None) # or =torchvision.transforms.ToTensor())\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(\n",
    "    os.path.join(dataset_root, \"val\"),\n",
    "    transform=None) # or =torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[100]\n",
    "print(type(image))\n",
    "print(\"cat\" if label == 0 else \"dog\")\n",
    "print(\"Image size:\", image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNNs pretrained on the *ImageNet* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 The ImageNet challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ImageNet class labels\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "class_names = sorted(list(requests.get(LABELS_URL).json().items()), key=lambda x: int(x[0]))\n",
    "class_names = list(map(lambda x: x[1], class_names))\n",
    "n_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(\"Total %d classes; examples:\" % n_classes)\n",
    "for _ in range(10):\n",
    "    class_idx = random.randint(0, len(class_names))\n",
    "    print(\"Class %d: %s\" % (class_idx, class_names[class_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Pretrained CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torchvision.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для нашего примера воспользуемся сетью [\"SqueezeNet\"](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py):\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*xji5NAhX6m3Nk7BmR_9GFw.png\" width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torchvision.models.SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.models.SqueezeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torchvision.models.squeezenet1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.squeezenet1_0(\n",
    "    pretrained=True,\n",
    "    num_classes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызваем `.eval()` если хотим делать инференс; если хотим тренировать модель, вызываем `.train()`. Эти методы меняет поведение некоторыз слоев, таких как BatchNorm и Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем запустить модель на случайном наборе данных\n",
    "# чтобы убедиться, что она не падает\n",
    "# примечание: SqueezeNet на обучении принимала картинки размером 224x224\n",
    "# в torchvision до версии 0.4 нужно было убедиться что картинка на вход имеет именно такой размер\n",
    "# в torchvision версии 0.4 и выше картинка может иметь другой размер\n",
    "sample_input = torch.randn(5, 3, 224, 224)\n",
    "sample_output = model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output is class scores:\n",
    "# (number of images per batch) x (number of classes)\n",
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output.min(), sample_output.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем предсказать класс для картинок из `./sample_images/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://pytorch.org/docs/stable/torchvision/models.html\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_to_pytorch(image):\n",
    "    return torch.tensor(image.getdata()).view(image.size + (3,))\n",
    "\n",
    "def predict(image):\n",
    "    \"\"\"\n",
    "        image: PIL image\n",
    "        \n",
    "        returns: ImageNet class probabilities\n",
    "    \"\"\"\n",
    "    image = PIL_to_pytorch(image)\n",
    "    image = image.permute(2, 0, 1)\n",
    "    image = image.to(torch.float32)\n",
    "    image /= 255.0\n",
    "    \n",
    "    # normalize\n",
    "    image -= imagenet_mean[:, None, None]\n",
    "    image /= imagenet_std [:, None, None]\n",
    "    \n",
    "    # add singleton batch dimension with [None]\n",
    "    prediction = model(image[None])\n",
    "    # 1 x 1000\n",
    "    prediction = prediction.softmax(1)\n",
    "    \n",
    "    # remove singleton dimension with [0]\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "image_path = \"sample_Images/albatross.jpg\"\n",
    "\n",
    "image = PIL.Image.open(image_path).resize((224, 224))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = predict(image)\n",
    "\n",
    "top_probabilities, top_indices = torch.topk(probabilities, 10)\n",
    "print(\"10 most probable classes are:\")\n",
    "for probability, class_idx in zip(top_probabilities, top_indices):\n",
    "    print(\"%.4f: %s\" % (probability, class_names[class_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use classical machine learning with a pretrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это последний блок нашей модели (можно убедиться в этом, просмотрев метод `forward()` в [исходниках](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что блок принимает 512-канальное изображение и превращает его в 1000-канальное с помощью линейного слоя (Conv2d с ядром 1x1), и берет среднее значение в каждом канале, чтобы получить логиты итоговых классов.\n",
    "\n",
    "Давайте из всех слоев блока оставим только average pooling, тем самым модель будет возвращать сырые признаки, которые имеют также название **embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_pooling = model.classifier[3]\n",
    "\n",
    "# число \"новых классов\" 512:\n",
    "model.num_classes = model.classifier[1].in_channels\n",
    "\n",
    "model.classifier = average_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(5, 3, 224, 224)\n",
    "sample_embedding = model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти сырые эмбеддинги уже могут служить хорошими признаками для классификации кот / собака.\n",
    "\n",
    "Но сначала зададим загрузчики картинок с правильным препроцессингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    os.path.join(dataset_root, \"train\"),\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=imagenet_mean, std=imagenet_std)]\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(\n",
    "    os.path.join(dataset_root, \"val\"),\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=imagenet_mean, std=imagenet_std)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=2, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=2, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user tqdm\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем эмбеддинги для всего датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.empty((len(train_dataset), sample_embedding.shape[1]), dtype=np.float32)\n",
    "y_train = np.empty(len(train_dataset), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (image_batch, labels_batch) in enumerate(tqdm(train_loader)):\n",
    "        embedding_subarray = X_train[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "        labels_subarray    = y_train[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "        \n",
    "        embeddings = model(image_batch.cuda())\n",
    "        np.copyto(embedding_subarray, embeddings.cpu().numpy())\n",
    "        np.copyto(labels_subarray, labels_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.empty((len(val_dataset), sample_embedding.shape[1]), dtype=np.float32)\n",
    "y_val = np.empty(len(val_dataset), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (image_batch, labels_batch) in enumerate(tqdm(val_loader)):\n",
    "        embedding_subarray = X_val[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "        labels_subarray    = y_val[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "        \n",
    "        embeddings = model(image_batch.cuda())\n",
    "        np.copyto(embedding_subarray, embeddings.cpu().numpy())\n",
    "        np.copyto(labels_subarray, labels_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренируем классификатор на этих эмбеддингах\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "\n",
    "classifier = sklearn.ensemble.RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy: %.2f\" % (sklearn.metrics.accuracy_score(y_train, classifier.predict(X_train)) * 100))\n",
    "print(\"Val accuracy: %.2f\" % (sklearn.metrics.accuracy_score(y_val, classifier.predict(X_val)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Replace the last layer and retrain it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход выше не очень удобен так как нам нужно запускать отдельный алгоритм.\n",
    "\n",
    "Более удобный подход это натренировать новую сетку поверх предобученной CNN.\n",
    "\n",
    "Тем самым мы получаем **end-to-end** пайплайн: единственная CNN предсказывает именно то что нам нужно.\n",
    "\n",
    "Здесь мы заменим последний слой сетки на простую лог регрессию (один линейный слой), так что точность не будует сильно выше, чем в случае с random forest. Однако, вы можете сделать более мощный классифкатор, например multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a custom layer to reshape a tensor into batch of vectors\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.flatten(1)\n",
    "    \n",
    "model.classifier = torch.nn.Sequential(\n",
    "    average_pooling,\n",
    "    Flatten(),\n",
    "    torch.nn.BatchNorm1d(512),\n",
    "    torch.nn.Linear(512, 1),\n",
    ").cuda()\n",
    "\n",
    "model.num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам необходимо тренировать только последний слой, так что остальные мы \"замораживаем\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad_(False)\n",
    "\n",
    "trainable_parameters = list(model.classifier.parameters())\n",
    "for parameter in trainable_parameters:\n",
    "    parameter.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем `model.classifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(trainable_parameters, lr=1e-3, weight_decay=1e-4)\n",
    "loss = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    \"\"\" Compute accuracy on a dataset \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            probabilities = model(images.cuda()).cpu().flatten()\n",
    "            predictions = (probabilities > 0.5).long()\n",
    "\n",
    "            total += len(labels)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "def train(model, dataloader, loss, optimizer):\n",
    "    \"\"\" Train for one epoch \"\"\"\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for idx, (images, labels) in enumerate(dataloader):\n",
    "        probabilities = model(images.cuda()).cpu().flatten()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = (probabilities > 0.5).long()\n",
    "            total += len(labels)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        loss_value = loss(probabilities, labels.float())\n",
    "        total_loss += loss_value.item() * len(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return correct / total, total_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    train_accuracy, train_loss = train(model, train_loader, loss, optimizer)\n",
    "    val_accuracy = validate(model, val_loader)\n",
    "    print(\"Epoch %d, train %.2f%% (loss %.4f), val %.2f%%\" \\\n",
    "            % (epoch, train_accuracy * 100, train_loss, val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fine-tune the whole network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Замороженная часть\" уже является хорошим feature экстрактором. Однако, она не имеет представление о гашем датасете. Давайте птерь обучим все веса, чтобы получить дополнительную точность. Этот процесс называется **fine-tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размораживаем веса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как оригинальные веса хорошо подобраны, нам нужно использовать менее агрессивный оптимизатор (например SGD с momentum) и гораздо более маленький learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=1e-3, momentum=0.9,\n",
    "    nesterov=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    train_accuracy, train_loss = train(model, train_loader, loss, optimizer)\n",
    "    val_accuracy = validate(model, val_loader)\n",
    "    print(\"Epoch %d, train %.2f%% (loss %.4f), val %.2f%%\" \\\n",
    "            % (epoch, train_accuracy * 100, train_loss, val_accuracy * 100))\n",
    "    \n",
    "    # gradually reduce learning rate\n",
    "    for group in optimizer.param_groups:\n",
    "        group['lr'] *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
