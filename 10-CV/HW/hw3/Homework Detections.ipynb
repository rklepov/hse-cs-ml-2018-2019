{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "В этом домашнее задании вам будет необходимо реализовать свой one-stage детектор на якорях (аналогично тому, что мы делали на лекции) на небольшом датасете и оценить его качество.\n",
    "\n",
    "Работать будем с тем же датасетом, что и в первом домашнем задании.\n",
    "http://www.anefian.com/research/face_reco.htm\n",
    "\n",
    "В данных лежит 50 папок в каждой из которых 15 фотографий. В текстовых файлах с лейблами записано 4 числа и номер папки. Первые два числа обозначают верхний левый угол коробки, правая часть числа обозначает нижний правый угол.\n",
    "\n",
    "Код для кодировки разметки можно взять из семинарского ноутбука.\n",
    "\n",
    "Так как данных очень мало изпользуйте предобученный vgg16, дообучайте только последние пару слоев. (Не забудьте, что часть слоев еще придется отрезать).\n",
    "\n",
    "Добавьте аугментацию флипами и кропами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кусочки кода с семинара, которые будут полезны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код в помощь\n",
    "IMG_HEIGHT = ?\n",
    "IMG_WIDTH = ?\n",
    "\n",
    "features = keras.applications.vgg16.VGG16(include_top=False,\n",
    "                                          weights='imagenet',\n",
    "                                          input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "feature_tensor = features.layers[-1].output\n",
    "\n",
    "for layer in features.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер якорей подберите, исходя из размеров коробок в разметке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SHAPE = (feature_tensor.shape[1].value,\n",
    "                 feature_tensor.shape[2].value)\n",
    "\n",
    "GRID_STEP_H = IMG_HEIGHT / FEATURE_SHAPE[0]\n",
    "GRID_STEP_W = IMG_WIDTH / FEATURE_SHAPE[1]\n",
    "\n",
    "ANCHOR_WIDTH = ?\n",
    "ANCHOR_HEIGHT = ?\n",
    "\n",
    "ANCHOR_CENTERS = np.mgrid[GRID_STEP_H/2:IMG_HEIGHT:GRID_STEP_H,\n",
    "                          GRID_STEP_W/2:IMG_WIDTH:GRID_STEP_W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для работы с якорями и разметкой. Самое просто - это разобраться какой формат разметки ожидается на входе и привести свою разметку к этому формату. Тогда не придется писать много кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(rect, x_scale, y_scale, anchor_x, anchor_y,\n",
    "        anchor_w=ANCHOR_WIDTH, anchor_h=ANCHOR_HEIGHT):\n",
    "    \n",
    "    rect_x1 = (rect['x'] - rect['width'] / 2) * x_scale\n",
    "    rect_x2 = (rect['x'] + rect['width'] / 2) * x_scale\n",
    "    \n",
    "    rect_y1 = (rect['y'] - rect['height'] / 2) * y_scale\n",
    "    rect_y2 = (rect['y'] + rect['height'] / 2) * y_scale\n",
    "    \n",
    "    anch_x1, anch_x2 = anchor_x - anchor_w / 2, anchor_x + anchor_w / 2\n",
    "    anch_y1, anch_y2 = anchor_y - anchor_h / 2, anchor_y + anchor_h / 2\n",
    "    \n",
    "    dx = (min(rect_x2, anch_x2) - max(rect_x1, anch_x1))\n",
    "    dy = (min(rect_y2, anch_y2) - max(rect_y1, anch_y1))\n",
    "    \n",
    "    intersection = dx * dy if (dx > 0 and dy > 0) else 0.\n",
    "    \n",
    "    anch_square = (anch_x2 - anch_x1) * (anch_y2 - anch_y1)\n",
    "    rect_square = (rect_x2 - rect_x1) * (rect_y2 - rect_y1)\n",
    "    union = anch_square + rect_square - intersection\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def encode_anchors(annotation, img_shape, iou_thr=0.5):\n",
    "    encoded = np.zeros(shape=(FEATURE_SHAPE[0],\n",
    "                              FEATURE_SHAPE[1], 5), dtype=np.float32)\n",
    "    x_scale = 1. * IMG_WIDTH / img_shape[1]\n",
    "    y_scale = 1. * IMG_HEIGHT / img_shape[0]\n",
    "    for rect in annotation['annotations']:\n",
    "        scores = []\n",
    "        for row in range(FEATURE_SHAPE[0]):\n",
    "            for col in range(FEATURE_SHAPE[1]):\n",
    "                anchor_x = ANCHOR_CENTERS[1, row, col]\n",
    "                anchor_y = ANCHOR_CENTERS[0, row, col]\n",
    "                score = iou(rect, x_scale, y_scale, anchor_x, anchor_y)\n",
    "                scores.append((score, anchor_x, anchor_y, row, col))\n",
    "        \n",
    "        scores = sorted(scores, reverse=True)\n",
    "        if scores[0][0] < iou_thr:\n",
    "            scores = [scores[0]]  # default anchor\n",
    "        else:\n",
    "            scores = [e for e in scores if e[0] > iou_thr]\n",
    "\n",
    "        for score, anchor_x, anchor_y, row, col in scores:\n",
    "            dx = (anchor_x - rect['x'] * x_scale) / ANCHOR_WIDTH\n",
    "            dy = (anchor_y - rect['y'] * y_scale) / ANCHOR_HEIGHT\n",
    "            dw = (ANCHOR_WIDTH - rect['width'] * x_scale) / ANCHOR_WIDTH\n",
    "            dh = (ANCHOR_HEIGHT - rect['height'] * y_scale) / ANCHOR_HEIGHT\n",
    "            encoded[row, col] = [1., dx, dy, dw, dh]\n",
    "            \n",
    "    return encoded\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def decode_prediction(prediction, conf_thr=0.1):\n",
    "    rectangles = []\n",
    "    for row in range(FEATURE_SHAPE[0]):\n",
    "        for col in range(FEATURE_SHAPE[1]):\n",
    "            logit, dx, dy, dw, dh = prediction[row, col]\n",
    "            conf = _sigmoid(logit)\n",
    "            if conf > conf_thr:\n",
    "                anchor_x = ANCHOR_CENTERS[1, row, col]\n",
    "                anchor_y = ANCHOR_CENTERS[0, row, col]\n",
    "                rectangles.append({'x': anchor_x - dx * ANCHOR_WIDTH,\n",
    "                                   'y': anchor_y - dy * ANCHOR_HEIGHT,\n",
    "                                   'width': ANCHOR_WIDTH - dw * ANCHOR_WIDTH,\n",
    "                                   'height': ANCHOR_HEIGHT - dh * ANCHOR_HEIGHT,\n",
    "                                   'conf': conf})\n",
    "    return rectangles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "def confidence_loss(y_true, y_pred):\n",
    "    conf_loss = K.binary_crossentropy(y_true[..., 0], \n",
    "                                      y_pred[..., 0],\n",
    "                                      from_logits=True)\n",
    "    return conf_loss\n",
    "\n",
    "def smooth_l1(y_true, y_pred):\n",
    "    abs_loss = K.abs(y_true[..., 1:] - y_pred[..., 1:])\n",
    "    square_loss = 0.5 * K.square(y_true[..., 1:] - y_pred[..., 1:])\n",
    "    mask = K.cast(K.greater(abs_loss, 1.), 'float32')\n",
    "    total_loss = (abs_loss - 0.5) * mask + 0.5 * square_loss * (1. - mask)\n",
    "    return K.sum(total_loss, axis=-1)\n",
    "\n",
    "def total_loss(y_true, y_pred, neg_pos_ratio=3):\n",
    "    batch_size = K.shape(y_true)[0]\n",
    "    \n",
    "    # TODO: добавьте функцию потерь для классификации детекции\n",
    "    \n",
    "    y_true = K.reshape(y_true, (batch_size, -1, 5))\n",
    "    y_pred = K.reshape(y_pred, (batch_size, -1, 5))\n",
    "\n",
    "    # confidence loss\n",
    "    conf_loss = confidence_loss(y_true, y_pred)\n",
    "    \n",
    "    # smooth l1 loss\n",
    "    loc_loss = smooth_l1(y_true, y_pred)\n",
    "    \n",
    "    # positive examples loss\n",
    "    pos_conf_loss = K.sum(conf_loss * y_true[..., 0], axis=-1)\n",
    "    pos_loc_loss = K.sum(loc_loss * y_true[..., 0], axis=-1)\n",
    "    \n",
    "    # negative examples loss\n",
    "    anchors = K.shape(y_true)[1]\n",
    "    num_pos = K.sum(y_true[..., 0], axis=-1)\n",
    "    num_pos_avg = K.mean(num_pos)\n",
    "    num_neg = K.min([neg_pos_ratio * (num_pos_avg) + 1., K.cast(anchors, 'float32')])\n",
    "    \n",
    "    # hard negative mining\n",
    "    neg_conf_loss, _ = tf.nn.top_k(conf_loss * (1. - y_true[..., 0]),\n",
    "                                   k=K.cast(num_neg, 'int32'))\n",
    "\n",
    "    neg_conf_loss = K.sum(neg_conf_loss, axis=-1)\n",
    "    \n",
    "    # total conf loss\n",
    "    total_conf_loss = (neg_conf_loss + pos_conf_loss) / (num_neg + num_pos + 1e-32)\n",
    "    loc_loss = pos_loc_loss / (num_pos + 1e-32)\n",
    "    \n",
    "    return total_conf_loss + 0.5 * loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итого:\n",
    "* Заставьте encoding разметки работать на ваших данных\n",
    "* Добавьте голову с выходом сетки\n",
    "* Добавьте аугментацию\n",
    "* Обучите сеть и визуализируйте результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
