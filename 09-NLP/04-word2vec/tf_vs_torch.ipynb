{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch vs Tensorflow \n",
    "\n",
    "В этой тетрадке мы попробуем посмотреть на разницу между двумя фреймворками, а также заглянуть в будущее и посмотреть на tensorflow 2.0.\n",
    "\n",
    "Решать будем глупую задачку по конвертации градусов по цельсию в градусы по фаренгейту. По данным будем пытаться восстановить формулу: \n",
    "\n",
    "$$ f = c \\times 1.8 + 32 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40.0 degrees Celsius = -40.0 degrees Fahrenheit\n",
      "-10.0 degrees Celsius = 14.0 degrees Fahrenheit\n",
      "0.0 degrees Celsius = 32.0 degrees Fahrenheit\n",
      "8.0 degrees Celsius = 46.0 degrees Fahrenheit\n",
      "15.0 degrees Celsius = 59.0 degrees Fahrenheit\n",
      "22.0 degrees Celsius = 72.0 degrees Fahrenheit\n",
      "38.0 degrees Celsius = 100.0 degrees Fahrenheit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "celsius    = np.array([-40, -10,  0,  8, 15, 22,  38],  dtype=float)\n",
    "fahrenheit = np.array([-40,  14, 32, 46, 59, 72, 100],  dtype=float)\n",
    "\n",
    "for i,c in enumerate(celsius):\n",
    "    print(\"{} degrees Celsius = {} degrees Fahrenheit\".format(c, fahrenheit[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Главное отличие:__ \n",
    "\n",
    "Оба фреймворка работают с тензорами и смотрят на любую модель, как на граф вычислений. Существенная разница заключается в том, что в случае Tensorflow граф определён статистически. Взаимодействие с внешним миром происходит через сессии и плейсхолдеры. Вне сессии мы не можем ничего вычислять. Это усложняет дебаг. \n",
    "\n",
    "В случае PyTorch граф определён динамически. Его узлы можно выполнять в любые моменты как угодно и где угодно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делай раз, Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf  # подгружаем первую версию библиотеки \n",
    "tf.disable_v2_behavior()           # отключаем функционал второй \n",
    "                                   # теперь код, написанный на версии tf 1.x должен работать \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущие потери: 153.62617\n",
      "Текущие потери: 148.2056\n",
      "Текущие потери: 142.8606\n",
      "Текущие потери: 137.60123\n",
      "Текущие потери: 132.43892\n",
      "Текущие потери: 127.38669\n",
      "Текущие потери: 122.45928\n",
      "Текущие потери: 117.67331\n",
      "Текущие потери: 113.047386\n",
      "Текущие потери: 108.60223\n",
      "\n",
      "Коэффициенты: 1.0066519 0.9923882\n",
      "Прогнозы: [-38.688873    1.0066519  38.717403 ]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "# плейсхолдеры для данных \n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# параметры модели \n",
    "a = tf.Variable(tf.zeros([1]), name='bias')\n",
    "b = tf.Variable(tf.zeros([1]), name='k')\n",
    "\n",
    "# модель \n",
    "y_hat = b*x + a\n",
    "\n",
    "# функция потерь и метод оптимизации\n",
    "loss = tf.sqrt(tf.reduce_sum((y - y_hat)**2))\n",
    "opt = tf.train.AdamOptimizer(learning_rate = 0.1)\n",
    "\n",
    "step = opt.minimize(loss)\n",
    "\n",
    "# процедура обучения \n",
    "# открываем вычислительную сессию \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # инициализировали все переменные \n",
    "    tf.global_variables_initializer().run() \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        cur_loss = sess.run(loss, feed_dict={x:celsius, y:fahrenheit})\n",
    "        print('Текущие потери:', cur_loss)\n",
    "        \n",
    "        # шаг оптимизации \n",
    "        sess.run(step, feed_dict={x:celsius, y:fahrenheit})\n",
    "        \n",
    "    print('\\nКоэффициенты:', a.eval()[0], b.eval()[0])\n",
    "    print('Прогнозы:', sess.run(y_hat, feed_dict={x:[-40,0,38]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делай два, pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "epochs = 10\n",
    "\n",
    "# плейсхолдеры для данных \n",
    "x = torch.from_numpy(celsius).float()\n",
    "y = torch.from_numpy(fahrenheit).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-40., -10.,   0.,   8.,  15.,  22.,  38.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# параметры модели \n",
    "b = torch.tensor([0.,0.], requires_grad=True) \n",
    "\n",
    "# вспомогательный аргумент - сигнал, что надо брать производную по тензору\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель (обычно задают в виде класса, см. ниже)\n",
    "def forward(x, b):\n",
    "    return b[0] + b[1]*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция потерь и метод оптимизации\n",
    "opt = torch.optim.Adam([b], lr=0.01)\n",
    "\n",
    "def loss(pred, target):\n",
    "    squares = (pred - target) ** 2\n",
    "    return squares.mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forward(x, b)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58.0652, grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_val = loss(y_pred, y)\n",
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2ccbac9b5777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss_val.backward( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.6963, -19.9209])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущие потери: 56.017284\n",
      "Текущие потери: 55.81415\n",
      "Текущие потери: 55.61136\n",
      "Текущие потери: 55.408897\n",
      "Текущие потери: 55.206787\n",
      "Текущие потери: 55.005028\n",
      "Текущие потери: 54.80364\n",
      "Текущие потери: 54.60261\n",
      "Текущие потери: 54.401966\n",
      "Текущие потери: 54.201702\n"
     ]
    }
   ],
   "source": [
    "# обучение (никаких сессий)\n",
    "for i in range(epochs):\n",
    "\n",
    "    y_pred = forward(x, b)\n",
    "    loss_val = loss(y_pred, y)\n",
    "    \n",
    "    loss_val.backward( )  # взяли производную от функции \n",
    "    # print(b.grad) # градиент будет храниться внутри тензора!!! \n",
    "    \n",
    "    opt.step() # шаг спуска \n",
    "    \n",
    "    opt.zero_grad() # в пай-торче градиенты накапливаются при вычислении, надо занулять\n",
    "    # при двух шагах результатом в a.grad будет сумма двух градиентов, поэтому надо его занулять \n",
    "    # на каждом шагу \n",
    "    \n",
    "    print('Текущие потери:', loss_val.data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20051186, 0.19965349], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.1867, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(40, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Коэффициенты: [0.20051186 0.19965349]\n",
      "Прогнозы: 8.186651\n"
     ]
    }
   ],
   "source": [
    "print('\\nКоэффициенты:', b.detach().numpy())\n",
    "print('Прогнозы:', forward(40, b).detach().numpy())\n",
    "\n",
    "# detach() отключает необходимость искать градиент и позволяет перетащить тензор в numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно модели собираеют в виде классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchFarengateNet(\n",
       "  (fc1): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (act1): Sigmoid()\n",
       "  (fc2): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TorchFarengateNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(TorchFarengateNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n",
    "        self.act1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = TorchFarengateNet(3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-40., -10.,   0.,   8.,  15.,  22.,  38.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# плейсхолдеры для данных \n",
    "x = torch.from_numpy(celsius).float()\n",
    "y = torch.from_numpy(fahrenheit).float()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-40.],\n",
       "       [-10.],\n",
       "       [  0.],\n",
       "       [  8.],\n",
       "       [ 15.],\n",
       "       [ 22.],\n",
       "       [ 38.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celsius[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-40.],\n",
       "        [-10.],\n",
       "        [  0.],\n",
       "        [  8.],\n",
       "        [ 15.],\n",
       "        [ 22.],\n",
       "        [ 38.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(1) # обычно работают со столбиками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-40.],\n",
       "        [ 14.],\n",
       "        [ 32.],\n",
       "        [ 46.],\n",
       "        [ 59.],\n",
       "        [ 72.],\n",
       "        [100.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze_(1) #inplace преобразование\n",
    "y.unsqueeze_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9405],\n",
       "        [0.9248],\n",
       "        [0.6001],\n",
       "        [0.3096],\n",
       "        [0.2957],\n",
       "        [0.2937],\n",
       "        [0.2933]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельно задают функцию потерь и оптимизатор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, target):\n",
    "    squares = (pred - target) ** 2\n",
    "    return squares.mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После циклом запускают обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущие потери: 3350.0095\n",
      "Текущие потери: 3348.2947\n",
      "Текущие потери: 3346.5825\n",
      "Текущие потери: 3344.8706\n",
      "Текущие потери: 3343.158\n",
      "Текущие потери: 3341.4453\n",
      "Текущие потери: 3339.7332\n",
      "Текущие потери: 3338.024\n",
      "Текущие потери: 3336.3171\n",
      "Текущие потери: 3334.6155\n",
      "Текущие потери: 3332.9192\n",
      "Текущие потери: 3331.2288\n",
      "Текущие потери: 3329.5437\n",
      "Текущие потери: 3327.8618\n",
      "Текущие потери: 3326.1782\n",
      "Текущие потери: 3324.4883\n",
      "Текущие потери: 3322.7866\n",
      "Текущие потери: 3321.069\n",
      "Текущие потери: 3319.3325\n",
      "Текущие потери: 3317.5667\n",
      "Текущие потери: 3315.758\n",
      "Текущие потери: 3313.889\n",
      "Текущие потери: 3311.938\n",
      "Текущие потери: 3309.8792\n",
      "Текущие потери: 3307.683\n",
      "Текущие потери: 3305.3198\n",
      "Текущие потери: 3302.768\n",
      "Текущие потери: 3300.0247\n",
      "Текущие потери: 3297.111\n",
      "Текущие потери: 3294.0723\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in range(30):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model.forward(x)\n",
    "    loss_val = loss(y_pred, y)\n",
    "\n",
    "    loss_val.backward()\n",
    "    print('Текущие потери:', loss_val.data.numpy())\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tensorflow 2.0](https://www.tensorflow.org/beta)\n",
    "\n",
    "Это попытка решить проблему. Фреймворк прошёл альфа-тестирование. Сейчас доступна бета-версия. Поддержка первой версии библиотеки будет постепенно прекращаться. Чтобы попробовать библиотеку:\n",
    "\n",
    "```\n",
    "pip3 install tensorflow==2.0.0-beta1 \n",
    "```\n",
    "\n",
    "__На всякий случай удалите старую версию tensorflow!!! Иначе при установке могут возникнуть какие-нибудь несовместимости и ошибки.__\n",
    "\n",
    "Основные отличия: \n",
    "\n",
    "* Анексия Keras. Он фактически стал частью библиотеки. \n",
    "* Появился режим с eager execution как в pytorch\n",
    "* Упрощение API путем очистки устаревших API и уменьшения дублирования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# что происходило в первой версии \n",
    "a = tf.constant([1, 2])\n",
    "b = tf.constant([3, 4])\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "c = a + b\n",
    "c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут сделать рестарт тетрадки и заново подгрузить библиотеку \n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4 6], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2])\n",
    "b = tf.constant([3, 4])\n",
    "\n",
    "print(a + b)  # вычисление ребра произошло сразу! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7, shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10, shape=(2,), dtype=int32, numpy=array([ 9, 16], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рубрика эксперименты! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
