{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковые модели\n",
    "\n",
    "**Языковая модель** *(language model, LM)* позволяет оценить вероятность последовательности слов (токенов). \n",
    "\n",
    "$$P(W)=P\\left(w_{1}, w_{2}, w_{3}, \\dots, w_{n}\\right)$$\n",
    "\n",
    "Как следствие, с помощью языковой модели можно предсказать вероятность следующего слова в последовательности.\n",
    "\n",
    "$$P\\left(w_{5} | w_{1}, w_{2}, w_{3}, w_{4}\\right)$$\n",
    "\n",
    "\n",
    "Какая последовательность вероятнее? \n",
    "\n",
    "* *Когда тебе 900 лет исполнится, тоже не молодо будешь выглядеть ты. (с)*\n",
    "* *Когда тебе исполнится 900 лет, ты тоже будешь выглядеть не молодо.*\n",
    "\n",
    "### Применение\n",
    "\n",
    "* Генерация текста\n",
    "* Распознавание речи\n",
    "* OCR и распознавание рукописного текста\n",
    "* Предиктивный ввод\n",
    "* Машинный перевод\n",
    "* Исправление опечаток\n",
    "* Определениt языка\n",
    "* Определение части речи (POS-tagging)\n",
    "* ...\n",
    "\n",
    "[Презентация Мурата Апишева](http://www.machinelearning.ru/wiki/images/5/5d/Mel_lain_msu_nlp_sem_2.pdf) с более подробными объяснениями и более сложными теоретическими вещами про языковые модели. Кое-что из этой презентации есть в теоретической части этой тетрадки.\n",
    "\n",
    "[Хороший тьюториал](https://towardsdatascience.com/learning-nlp-language-models-with-real-data-cdff04c51c25) по языковым моделям на *towardsdatascience*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Счетные языковые модели\n",
    "\n",
    "## Модель N-грамм\n",
    "\n",
    "Очень простая, но мощная языковая модель, которой Журафски и Мартин посвятили целую главу (третью) в  [\"Speech and Language Processing\"](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf).\n",
    "\n",
    "### Цепное правило\n",
    "\n",
    "Мы можем оценить вероятность всей последовательности слов как произведение условных вероятностей ее частей. Пусть $w_1,\\ldots,w_n$ (или $w_{1}^{n}$) – последовательность слов. Тогда ее вероятность можно оценить следующим образом:\n",
    "\n",
    "$$ P(w_{1}, \\ldots, w_{n})=\\prod_{i=1}^{n} P(w_{i} | w_{1}, \\ldots, w_{i-1}) \\approx \\prod_{i=1}^{n} P(w_{i} | w_{i-(n-1)}, \\ldots, w_{i-1}) =\\prod_{i=1}^{n} P\\left(w_{i} | w_{1}^{i-1}\\right)$$\n",
    "\n",
    "### Марковское свойство n-ного порядка\n",
    "Запоминаем не всю цепочку, а только $n-1$ предшествующих слов. Тогда вероятностью i-того слова $w_i$ в контексте предшествущих $i − 1$ слов можно считать вероятность этого слова в сокращенном контексте предшествущих $n − 1$ слов.\n",
    "\n",
    "> Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. (Jurafsky & Martin)\n",
    "\n",
    "* Модель униграмм: $P(w_i)$\n",
    "* Модель биграмм: $P(w_i | w_{i-1})$\n",
    "* Модель триграмм: $P(w_i | w_{i-1} w_{i-2})$\n",
    "\n",
    "\n",
    "### Модель униграмм (модель мешка слов)\n",
    "\n",
    "В такой модели вероятность слова в последовательности зависит исключительно от вероятности этого слова в корпусе, т.е. контекст не учитывается. Вероятность последовательности рассчитывается как произведение вероятностей всех слов в ней.\n",
    "\n",
    "$$ P(w_{1}, \\ldots, w_{n}) \\approx \\prod_{i=1}^{n} P(w_{i}) $$\n",
    "\n",
    "* P(огурец | жадина говядина соленый) $\\approx$ P(огурец)\n",
    "* P(жадина говядина соленый огурец) $\\approx$ P(жадина) $\\times$ P(говядина) $\\times$ P(соленый) $\\times$ P(огурец)\n",
    "\n",
    "### Модель биграмм\n",
    "\n",
    "Каждое слово зависит только от одного предыдущего слова: \n",
    "\n",
    "$$ P\\left(w_{i} | w_{1}, w_{2}, \\ldots, w_{i-1}\\right) \\approx P\\left(w_{i} | w_{i-1}\\right) $$\n",
    "\n",
    "Вероятность всей последовательности — произведение условных вероятностей каждого элемента.\n",
    "\n",
    "$$ P(w_{1}, \\ldots, w_{n})  \\approx \\prod_{i=1}^{n} P(w_{i} | w_{i-1}) $$\n",
    "\n",
    "\n",
    "* P(огурец | жадина говядина соленый) $\\approx$ P(огурец | соленый)\n",
    "* P(жадина говядина соленый огурец) $\\approx$ P(жадина | <начало предложения>) $\\times$ P(говядина | жадина) $\\times$ P(соленый | говядина) $\\times$ P(огурец | соленый) $\\times$ P(<конец предложения> | огурец)\n",
    "\n",
    "\n",
    "### Модель триграмм\n",
    "\n",
    "Каждое слово зависит от двух предыдущих слов.\n",
    "\n",
    "$P\\left(w_{i} | w_{1}, w_{2}, \\ldots,  w_{i-2}, w_{i-1}\\right) \\approx P\\left(w_{i} |w_{i-2} w_{i-1}\\right)$\n",
    "\n",
    "P(огурец | жадина говядина соленый) $\\approx$ P(огурец | говядина соленый)\n",
    "\n",
    "\n",
    "### Метод максимального правдоподобия \n",
    "\n",
    "**Метод максимального правдоподобия** (*maximum likelihood estimate, MLE)* — способ оценить вероятность N-граммы с помощью корпуса. Рассчитывается как частотность N-граммы в корпусе, нормированная таким образом, чтобы значение лежало в пределах от 0 до 1. \n",
    "\n",
    "$$ P(w_{i} | w_{i-(n-1)}, \\ldots, w_{i-1})=\\frac{\\texttt{count}(w_{i-(n-1)}, \\ldots, w_{i-1}, w_{i})}{\\texttt{count}(w_{i-(n-1)}, \\ldots, w_{i-1})} $$\n",
    "\n",
    "В модели биграмм:\n",
    "\n",
    "$$P_{MLE}(w_i | w_{i-1}) = \\frac{\\texttt{count}(w_{i-1} w_i )}{\\texttt{count}(w_{i-1} )}$$\n",
    "\n",
    "#### Задание\n",
    "\n",
    "Дан корпус. Посчитайте MLE биграмм «жадина говядина», «соленый огурец», «турецкий барабан».\n",
    "\n",
    "* жадина говядина соленый огурец\n",
    "* жадина говядина зеленый огурец\n",
    "* жадина говядина турецкий барабан кто на нем играет тот рыжий таракан\n",
    "* жадина говядина немецкий барабан\n",
    "* жадина говядина пустая шоколадина\n",
    "* жадина говядина соленый огурец на полу валяется никто его не ест\n",
    "\n",
    "\n",
    "## Сглаживание\n",
    "\n",
    "Подробнее см. раздел 3.4 (стр. 49) в [Speech & Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf).\n",
    "\n",
    "### Зачем?\n",
    "* Огранниченность корпуса\n",
    "* Занижена вероятность\n",
    "* Вероятность равна нулю\n",
    "\n",
    "### Методы\n",
    "* Сглаживание Лапласа (add-one)\n",
    "* Сглаживание Кнесера-Нея (Kneser-Ney)\n",
    "* Сглаживание Виттена-Белла (Witten-Bell)\n",
    "* Сглаживание Гуда-Тьюринга (Good-Turing)\n",
    "* Интерполяция\n",
    "* Откат (backoff)\n",
    "\n",
    "### Аддитивное сглаживание Лапласа\n",
    "\n",
    "Просто добавляем некоторый коэффициент $\\alpha$ к встречаемости каждой N-граммы. Например, при $\\alpha=1$,\n",
    "\n",
    "$$ P(w_i | w_{i-1}) = \\frac{\\texttt{count}(w_{i-1} w_i ) + \\alpha}{\\texttt{count}(w_{i-1} ) + \\alpha V} $$\n",
    "\n",
    "Частный случай add-one сглаживания в униграммной модели:\n",
    "\n",
    "$$P\\left(w_{i}\\right)=\\frac{c_{i}}{N} \\approx P_{\\text { Laplace }}\\left(w_{i}\\right)=\\frac{c_{i}+1}{N+V}$$\n",
    "\n",
    "\n",
    "* $c_{i}$ — частотность слова\n",
    "* $N$ — количество слов в корпусе\n",
    "* $V$ — объем словаря\n",
    "\n",
    "\n",
    "### Откат \n",
    "\n",
    "**Katz smoothing** *(простой откат):* если не получается применить модель высокого порядка, пробуем для данного слова модель меньшего порядка с понижающим коэффициентом. Например, мы работаем с моделью триграмм, но триграммы \"жадина говядина соленый\" в нашем корпусе нет, и мы не можем оценить ее вероятность. Тогда ищем биграмму \"говядина соленый\" и берем ее вероятность с понижающим коэффициентом $\\lambda_{1}$, а если нет и ее — берем униграммную вероятность \"соленый\" с понижающим коэффициентом $\\lambda_{2}$. Но получим не вероятностное распределение!\n",
    "\n",
    "\n",
    "## Оценка качества языковой модели\n",
    "\n",
    "**Перплексия** — метрика оценки языковой модели, показывающая, насколько хорошо модель предсказывает выборку. Она определяется как обратная вероятность тестового корпуса, нормализованная на количество слов. Чем ниже значение перплексии, тем лучше.\n",
    "\n",
    "$$PP(\\texttt{LM}) = b^{{-{\\frac  {1}{N}}\\sum _{{i=1}}^{N}\\log _{b}\\texttt{LM}(x_{i})}}$$,\n",
    "\n",
    "* $N$ — длина корпуса\n",
    "* $x_i$ — i-тое слово в корпусе\n",
    "* LM(x) — предсказание вероятности языковой моделью\n",
    "* b — некоторая константа, обычно 2.\n",
    "\n",
    "Например, есть тестовая последовательность $W=w_{1} w_{2} \\dots w_{N}$. Перплексия модели биграмм на данной последовательности будет рассчитываться так (подробнее см. в учебнике [\"Speech and Language Processing\"](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) (Daniel Jurafsky, James H. Martin), стр. 44.)\n",
    "\n",
    "$$\n",
    "\\operatorname{PP}(W)=\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P\\left(w_{i} | w_{i-1}\\right)}}\n",
    "$$\n",
    "\n",
    "\n",
    "## Модели N-грамм в NLTK\n",
    "\n",
    "Вероятностные распределения в NLTK: https://www.nltk.org/_modules/nltk/probability.html\n",
    "\n",
    "<img src=\"./img/freqdist.png\" width=\"600\" align=\"left\">\n",
    "\n",
    "<img src=\"./img/condfreqdist.png\" width=\"650\" align=\"left\">\n",
    "\n",
    "<img src=\"./img/condfreqdist2.png\" width=\"600\" align=\"left\">\n",
    "\n",
    "<img src=\"./img/mleprobdist.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_aachenosaurus_',\n",
       " '_aardonyx_',\n",
       " '_abdallahsaurus_',\n",
       " '_abelisaurus_',\n",
       " '_abrictosaurus_',\n",
       " '_abrosaurus_',\n",
       " '_abydosaurus_',\n",
       " '_acanthopholis_',\n",
       " '_achelousaurus_',\n",
       " '_acheroraptor_']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "pad = '_'\n",
    "\n",
    "with open('./data/dinos.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "names = [pad+name.strip().lower()+pad for name in data]\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3072, 2487, 539, 548, 913, 1081, 1710, 2285, 2123, 1704, 341, 266, 85, 171, 617, 944, 852, 552, 111, 328, 360, 37, 55, 141, 41, 60, 23]\n"
     ]
    }
   ],
   "source": [
    "chars = [char for name in names for char in name]\n",
    "freq = FreqDist(chars)\n",
    "\n",
    "print(list(freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 27 samples and 2487 outcomes>\n"
     ]
    }
   ],
   "source": [
    "cfreq = ConditionalFreqDist(bigrams(chars))\n",
    "print(cfreq['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a|_) = 0.0555\n",
      "p(r|_) = 0.0534\n",
      "p(s|_) = 0.4705\n"
     ]
    }
   ],
   "source": [
    "cprob = ConditionalProbDist(cfreq, MLEProbDist)\n",
    "print('p(a|_) = %1.4f' %cprob['a'].prob('_'))\n",
    "print('p(r|_) = %1.4f' %cprob['r'].prob('_'))\n",
    "print('p(s|_) = %1.4f' %cprob['s'].prob('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a) = 0.1354\n"
     ]
    }
   ],
   "source": [
    "l = sum([freq[char] for char in freq])\n",
    "\n",
    "def unigram_prob(char):\n",
    "    return freq[char] / l\n",
    "\n",
    "print('p(a) = %1.4f' %unigram_prob('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно порождать случайные символы с учётом предыдущих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cprob['u'].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "\n",
    "1. Напишите функцию для генерации нового имени динозавра фиксированной длины. \n",
    "2. Обучите модель триграмм на любом большом тексте. Какой корпус понадобится: лемматизированный или нет? Понадобится ли пунктуация? Можно взять:\n",
    "    * данные какой-нибудь газеты (например, \"Полярный круг\", которая выложена в папке data)\n",
    "    * любое большое литературное произведение\n",
    "    * словарь пословиц и поговорок В.И. Даля (лежит в папке data)\n",
    "    * и т.д.\n",
    "3. Напишите функцию, которая будет оценивать вероятность следующего слова для данной последовательности и функцию, которая будет предсказывать самое вероятное следующее слово для данной последовательности. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейросетевые модели\n",
    "\n",
    "В нейросетевых языковых моделях нейросеть используется как классификатор для предсказания следующего слова по предыдущим. \n",
    "\n",
    "## Нейронные сети прямого распространения (FNN)\n",
    "\n",
    "> A feedforward neural LM is a standard feedforward network that takes as inputat timeta representation of some number of previous words ($w_{t−1}, w_{t−2}$, etc) andoutputs a probability distribution over possible next words.  Thus — like the n-gram LM — the feedforward neural LM approximates the probability of a word given theentire prior context $P(w_{t}|w^{t−1}_{1})$ by approximating based on the N previous words.  (Martin & Jurafsky)\n",
    "\n",
    "$$\n",
    "P\\left(w_{t} | w_{1}^{t-1}\\right) \\approx P\\left(w_{t} | w_{t-N+1}^{t-1}\\right)\n",
    "$$\n",
    "\n",
    "Подробнее о том, как работает FFN и как написать ее с нуля и потестировать на задаче классификации тестов, см. [вот в этой тетрадке](https://github.com/ancatmara/data-science-nlp/blob/master/4.%20NN%20from%20Scratch.ipynb). Очень полезно будет сделать задание, которое дано в конце!\n",
    "\n",
    "![](img/ffn.png)\n",
    "\n",
    "## Рекуррентные нейронные сети (RNN)\n",
    "\n",
    "**Рекуррентная нейронная сеть** *(recurrent neural network, RNN)* — вид нейронных сетей, где связи между элементами образуют направленную последовательность. \n",
    "\n",
    "> A recurrent neural network is any network that contains is a cycle within its network connections. That is, any network where the value of a unit is directly, or indirectly, dependent on its own output as an input. (Jurafsky & Martin)\n",
    "\n",
    "Т.е. RNN содержат не только прямые, но и обратные связи. Наличие таких внутренних циклов делает RNN удобными и эффективными для обработки последовательностей. Вот так выглядит RNN в развертке.\n",
    "\n",
    "![](http://neerc.ifmo.ru/wiki/images/thumb/0/05/RNN.png/1599px-RNN.png)\n",
    "\n",
    "### Развертка RNN во времени (Jurafsky & Martin)\n",
    "\n",
    "![](img/rnn_jurafsky.png)\n",
    "\n",
    "### Чем они хороши RNN?\n",
    "\n",
    "* позволяют обобщать значения слов (если мы создадим эмбеддинги в процессе обучения нейросети или же подадим предобученные эмбеддинги на вход)\n",
    "* позволяют уйти от Марковских допущений и учитывать предысторию произвольной длины\n",
    "* не нужно сглаживание\n",
    "* скрытый слой с предыдущего шага — контекст\n",
    "\n",
    "### Biderectional RNN\n",
    "\n",
    "В одноcторонней RNN скрытое состояние в заданное время t — это все, что нейросеть знает о последовательности на данный момент. Это можно считать левым контекстом.\n",
    "\n",
    "$$ h_{t}^{\\text {forward}}=\\operatorname{SRN}_{\\text {forward}}\\left(x_{1} : x_{t}\\right) $$\n",
    "\n",
    "Мы можем получить точно такой же вектор для правого контекста, обучив нейросеть на нашей последовательности, выстроенной в обратном порядке.\n",
    "\n",
    "$$ h_{t}^{\\text {backward}}=\\operatorname{SRN}_{\\text {backward}}\\left(x_{n} : x_{t}\\right) $$\n",
    "\n",
    "Если объединить эти две независимые друг от друга RNN, получится **Bi-RNN** (иллюстрация из Jurafsky & Martin).\n",
    "\n",
    "![](img/bi-rnn.png)\n",
    "\n",
    "\n",
    "### Что почитать?\n",
    "\n",
    "* Главное чтение — гл. 7 (особенно раздел 7.5, стр. 145) и 9 (стр. 177) в [Speech & Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf).\n",
    "* Хорошее краткое описание RNN есть [в вики-конспектах университета ИТМО](http://neerc.ifmo.ru/wiki/index.php?title=%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8).\n",
    "* [Статья на Medium'e](https://medium.com/nyu-a3sr-data-science-team/simple-recurrent-neural-networks-2df362ae0a39) (по-английски)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика\n",
    "\n",
    "Напишем RNN для генерации названий динозавтров на PyTorch. Вот так будет выглядеть архитектура нашей сети.\n",
    "\n",
    "![rnn](img/dinos3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinosDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('./data/dinos.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content))\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.lines = content.splitlines()\n",
    "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        #teacher forcing\n",
    "        x_str = line\n",
    "        y_str = line[1:] + '\\n'\n",
    "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
    "        y = torch.empty(len(x_str), dtype=torch.long)\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aardonyx\n"
     ]
    }
   ],
   "source": [
    "print(trn_ds.lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "print(trn_ds.ch_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "tensor([ 1, 18,  4, 15, 14, 25, 24,  0])\n"
     ]
    }
   ],
   "source": [
    "x, y = trn_ds[1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "        combined = torch.cat([h_prev, x], dim = 1) # конкатенируем вектора состояния и входа\n",
    "        h = torch.tanh(self.dropout(self.i2h(combined)))\n",
    "        y = self.i2o(combined)\n",
    "        return h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_idxs):\n",
    "    [print(trn_ds.idx_to_ch[x], end='') for x in sample_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    word_size=0\n",
    "    newline_idx = trn_ds.ch_to_idx['\\n']\n",
    "    with torch.no_grad():\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        start_char_idx = random.randint(1, trn_ds.vocab_size-1)\n",
    "        indices = [start_char_idx]\n",
    "        x[0, start_char_idx] = 1\n",
    "        predicted_char_idx = start_char_idx\n",
    "        \n",
    "        while predicted_char_idx != newline_idx and word_size != 50:\n",
    "            h_prev, y_pred = model(h_prev, x)\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    "            predicted_char_idx = idx\n",
    "            \n",
    "            word_size += 1\n",
    "        \n",
    "        if word_size == 50:\n",
    "            indices.append(newline_idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            h_prev, y_pred = model(h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataset='dinos', epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print('Epoch:{}'.format(e))\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "zlorya\n",
      "gepaic\n",
      "ttxas\n",
      "rbpanrus\n",
      "lftagaoaurusuusus\n",
      "rcterauhus\n",
      "lwscnrus\n",
      "bblrbaurus\n",
      "elcltvaurus\n",
      "mtlianhrus\n",
      "pbtrsllhurus\n",
      "krstaauius\n",
      "brltaauras\n",
      "mtlhbmcrus\n",
      "jasrsanrusaurus\n",
      "\n",
      "Epoch:2\n",
      "mamiahraeurls\n",
      "onanesaurus\n",
      "nyrreoshurus\n",
      "vytiosncrur\n",
      "osashsalruc\n",
      "ldtgmcglrus\n",
      "harssaurus\n",
      "uslsouaus\n",
      "qgsaonocrus\n",
      "zkhtlnaumus\n",
      "gucsrsaurusaurusaaroe\n",
      "esatisa\n",
      "zsnaohiuros\n",
      "qivoudssaurus\n",
      "cyrmrsoarur\n",
      "\n",
      "Epoch:3\n",
      "ytbpisaurui\n",
      "yctgmcglrus\n",
      "xastsaurus\n",
      "osgrruagna\n",
      "qtboisauruc\n",
      "taranasaurus\n",
      "qttolkoppturus\n",
      "yaniangurus\n",
      "iarianaurus\n",
      "danwosaurus\n",
      "ornnuoraurds\n",
      "jacsasaurus\n",
      "jalgsnacras\n",
      "goudrseurus\n",
      "pyrjriuaus\n",
      "\n",
      "Epoch:4\n",
      "ngsaurua\n",
      "zrnaraurum\n",
      "hamyrolrus\n",
      "oiosttsistoc\n",
      "chabtataunus\n",
      "mamgsaunus\n",
      "yxnteskurus\n",
      "urytsosncrus\n",
      "drcsisaurua\n",
      "fetgicierus\n",
      "lasosnaurus\n",
      "nisosapaurus\n",
      "gfuaasabrus\n",
      "xiarbnsurus\n",
      "zsngosaurus\n",
      "\n",
      "Epoch:5\n",
      "buaisauris\n",
      "euadopalhor\n",
      "varasrrudtos\n",
      "uluscysus\n",
      "xuanicaurus\n",
      "iaeoobaurus\n",
      "gtbiusourus\n",
      "jenniuaurus\n",
      "anranlerus\n",
      "papmbanrus\n",
      "xsanuosaurus\n",
      "shurusaurur\n",
      "wicgschuris\n",
      "chachopl\n",
      "yucgxrnurus\n",
      "\n",
      "Epoch:6\n",
      "tesooturus\n",
      "vamichs\n",
      "voltcerl\n",
      "epathnasaurus\n",
      "drroniosaurus\n",
      "wubgocgsaurus\n",
      "runaodourus\n",
      "ansosaurus\n",
      "zrriysisns\n",
      "ongchsaurus\n",
      "monapaurus\n",
      "famuosdsiurus\n",
      "eoytospucus\n",
      "elrcontcrus\n",
      "epatioaurus\n",
      "\n",
      "Epoch:7\n",
      "fuioseurus\n",
      "lxrhnsrcaurus\n",
      "grltahauhus\n",
      "jibcoantssurus\n",
      "zepooyoistu\n",
      "henbfsaurus\n",
      "qulamgourus\n",
      "kiviugossaurus\n",
      "vqnntpcaurus\n",
      "aurucauras\n",
      "drngasaurus\n",
      "nrtiicsosuurus\n",
      "eagpappcsaujus\n",
      "dandsaurus\n",
      "utotgossaurus\n",
      "\n",
      "Epoch:8\n",
      "sinrus\n",
      "fin\n",
      "elpcrnrcauras\n",
      "sinrasaurus\n",
      "kutponoosuurus\n",
      "bamecaurus\n",
      "nabrncaurus\n",
      "rubsrsturus\n",
      "olostus\n",
      "cirpbopaaurus\n",
      "hadpiaideuros\n",
      "veuotaurus\n",
      "morhysosaurus\n",
      "iatarnt\n",
      "jannanliurus\n",
      "\n",
      "Epoch:9\n",
      "phtosaurus\n",
      "orootoriutai\n",
      "aapuccurus\n",
      "araoltrris\n",
      "kitiucsourus\n",
      "urysaurus\n",
      "jianoasaurus\n",
      "uanettras\n",
      "yltntcosei\n",
      "ckrcyopsaucus\n",
      "waoaolt\n",
      "garoapesaurus\n",
      "quntcsopsaurus\n",
      "glrovang\n",
      "omocths\n",
      "\n",
      "Epoch:10\n",
      "gasgasaurus\n",
      "carwtrnsstis\n",
      "monyoraures\n",
      "naeocgurms\n",
      "gnaihsaurus\n",
      "dvntcsusaurus\n",
      "runtos\n",
      "venalocootar\n",
      "gmajhsnalrus\n",
      "wiucssneisshut\n",
      "gostanakrus\n",
      "betaetocamtss\n",
      "auciscurus\n",
      "urespntusauris\n",
      "valocoocerus\n",
      "\n",
      "Epoch:11\n",
      "hiksgnao\n",
      "eoyprestus\n",
      "xlnnysnors\n",
      "kinaisarntirus\n",
      "yoerknasaurus\n",
      "nrtoristotuntouaaurus\n",
      "queuaatiasaurus\n",
      "jaitiuhdurus\n",
      "urixturus\n",
      "tgoaisaurus\n",
      "lonammgores\n",
      "dotnxhoosaurus\n",
      "prlotsbaurus\n",
      "prltaliuius\n",
      "ngjaxdotaurus\n",
      "\n",
      "Epoch:12\n",
      "zilsosturus\n",
      "saoraseurus\n",
      "hapgancrnanrus\n",
      "frsatssaurus\n",
      "sirrosaurus\n",
      "marescashangonaurus\n",
      "lotgrsoonaurus\n",
      "cirscaurus\n",
      "tonrcersaurus\n",
      "absarturus\n",
      "irngrsaurus\n",
      "wubipauros\n",
      "drcdrnaonhuros\n",
      "thvovaurus\n",
      "rosnysaurus\n",
      "\n",
      "Epoch:13\n",
      "xianoasaurus\n",
      "a\n",
      "zilspacrus\n",
      "oructsaurus\n",
      "suurostcrus\n",
      "drcsbtaasacrus\n",
      "dictantluaurus\n",
      "jonnysrauras\n",
      "tanrcluros\n",
      "fnaoeturis\n",
      "litlycroosaurus\n",
      "ilnosaraurus\n",
      "eluberobaurus\n",
      "juanyouiourus\n",
      "vinytisaurus\n",
      "\n",
      "Epoch:14\n",
      "zetataurus\n",
      "zancsaurus\n",
      "surshssurus\n",
      "xixngoso\n",
      "onichudaus\n",
      "kasiaiasaurus\n",
      "vuntgrsoptor\n",
      "kysesotagncat\n",
      "silramounus\n",
      "tenascgusaurus\n",
      "olopoysoulu\n",
      "anrantcrus\n",
      "fcsiajmaurus\n",
      "kiviucsaurus\n",
      "zhxspauras\n",
      "\n",
      "Epoch:15\n",
      "kaloasaurus\n",
      "uamftrris\n",
      "ikuoudaurus\n",
      "shntosauros\n",
      "uaisaurus\n",
      "irnaonoisaurus\n",
      "juansoianoovdurus\n",
      "kinamochurus\n",
      "ur\n",
      "xiangickapturus\n",
      "cooatossurus\n",
      "jaanagraoniurus\n",
      "foeuclas\n",
      "weuluddurus\n",
      "humyuqhuros\n",
      "\n",
      "Epoch:16\n",
      "daloasguan\n",
      "jiandungaturus\n",
      "kasttopaurus\n",
      "nottaoraurus\n",
      "euacoeasaurus\n",
      "banxstsrus\n",
      "lepnivoshurus\n",
      "tantasaurus\n",
      "bancsaurus\n",
      "ctitespurus\n",
      "savrriuros\n",
      "ocasasaurus\n",
      "wanetngasaurus\n",
      "xlsniisneupurus\n",
      "dlnabtataurus\n",
      "\n",
      "Epoch:17\n",
      "uajqtun\n",
      "aubisosaurus\n",
      "atritourus\n",
      "welalrdauros\n",
      "mibhesipaorox\n",
      "ptdttoraurus\n",
      "xinnyacnaurus\n",
      "bucashapaurus\n",
      "rantosaurus\n",
      "jrnnzunauris\n",
      "calocaurus\n",
      "ribohsaurus\n",
      "nyntasotgosaurus\n",
      "losaenanaurus\n",
      "iaslcaurus\n",
      "\n",
      "Epoch:18\n",
      "zudgtaurus\n",
      "thmmorturus\n",
      "eahnaesaosr\n",
      "xconaonoalaurus\n",
      "ructosaurus\n",
      "xolnsucaurus\n",
      "fukuahos\n",
      "xiatghaaorus\n",
      "verotodaurus\n",
      "moruaricaurus\n",
      "ecathalep\n",
      "qiasaiuntashurus\n",
      "kiyooouras\n",
      "qaenasauaesacrus\n",
      "mectbnyspurus\n",
      "\n",
      "Epoch:19\n",
      "ulusaurus\n",
      "osantanaes\n",
      "wucctnaurus\n",
      "qasgiuaurus\n",
      "hikponysaurus\n",
      "thcesataurus\n",
      "raneorectesaus\n",
      "fususaurus\n",
      "wenpucgk\n",
      "lepeonoerus\n",
      "hiismnathaurus\n",
      "xonngrtgusaurus\n",
      "opamtsrus\n",
      "ncslceurus\n",
      "iscguaurus\n",
      "\n",
      "Epoch:20\n",
      "weloonuurus\n",
      "falnaaurus\n",
      "ladricideurus\n",
      "heviydqtor\n",
      "drrgysgosagrus\n",
      "auburtcrus\n",
      "foeuiaerosaus\n",
      "rttonasaurus\n",
      "prraonaleurus\n",
      "fesibjgauros\n",
      "nixlsasturus\n",
      "eoyshrltaor\n",
      "amsasaurus\n",
      "uajitroasaurus\n",
      "hutsemsus\n",
      "\n",
      "Epoch:21\n",
      "gsnhuras\n",
      "ocephops\n",
      "markandolalros\n",
      "kosastsaurus\n",
      "psnntocrud\n",
      "wseogtcrus\n",
      "klhuniasaurus\n",
      "nstoalosnturus\n",
      "galganoasaurus\n",
      "sanfsaurus\n",
      "ptnueosaurus\n",
      "eytposscrus\n",
      "aubusterus\n",
      "weltinaubaurus\n",
      "junngosgvurast\n",
      "\n",
      "Epoch:22\n",
      "iinaraurus\n",
      "laphanesaurus\n",
      "eumuesotsaurus\n",
      "ilnotauras\n",
      "fasauaasacrus\n",
      "enataiusoeaurus\n",
      "cinytorsxaor\n",
      "ngtargtaurun\n",
      "yesnpaurus\n",
      "crlotopaurus\n",
      "xinnnanamrus\n",
      "vesacopatops\n",
      "harbotltasaurus\n",
      "vexookurus\n",
      "ocgsaurus\n",
      "\n",
      "Epoch:23\n",
      "zrhaidsaurus\n",
      "hynvdsoosaurus\n",
      "yprouaragauaurus\n",
      "grnanneurus\n",
      "sauosaurus\n",
      "brrhytisaurus\n",
      "xinasntdauros\n",
      "lrhhasaurus\n",
      "xirnggskltaurus\n",
      "henchsarpturus\n",
      "apcuraerus\n",
      "yrtasaurus\n",
      "jiyngosecrus\n",
      "prctiscauras\n",
      "pslecochyntisltolaurus\n",
      "\n",
      "Epoch:24\n",
      "xinnasilauius\n",
      "waloicaurus\n",
      "osaithodeurus\n",
      "ronwopsstas\n",
      "uahsaurup\n",
      "xinajaogsaurus\n",
      "usasturus\n",
      "nixolirs\n",
      "glnagiprus\n",
      "lbpharaurus\n",
      "lantosdspurus\n",
      "wextnoshlrus\n",
      "sicrosaurus\n",
      "saugocoirus\n",
      "desrongosaurus\n",
      "\n",
      "Epoch:25\n",
      "quaih\n",
      "onocturas\n",
      "bibihkuras\n",
      "wdultaurus\n",
      "clonysaurus\n",
      "zhceudiisaurui\n",
      "xepngasalur\n",
      "jbonnoniurus\n",
      "wlnuang\n",
      "eltbopeirus\n",
      "velonkchurus\n",
      "zaurillnosturus\n",
      "uainaurus\n",
      "quallncaurus\n",
      "wudguro\n",
      "\n",
      "Epoch:26\n",
      "brtnncsaurus\n",
      "qusdlraurus\n",
      "xramgoagataurus\n",
      "hyptesturus\n",
      "thxunosoc\n",
      "jianiauasamgurus\n",
      "ceectansls\n",
      "nttoomosaurus\n",
      "vucipaurus\n",
      "stbconakieurus\n",
      "iluosaurus\n",
      "norovrritohs\n",
      "uags\n",
      "viltceracops\n",
      "ylasasourus\n",
      "\n",
      "Epoch:27\n",
      "wrleosaurus\n",
      "bubgnaas\n",
      "quetaasacrus\n",
      "ylasdotaurus\n",
      "ypaptotuptor\n",
      "phlakociurus\n",
      "webhhsgidaurus\n",
      "tatturaurus\n",
      "ripsocmpais\n",
      "zreqdasnco\n",
      "gtjgauaotaurus\n",
      "fpgurhtopsus\n",
      "velagodaurus\n",
      "mibhesgnaurus\n",
      "aucqshurus\n",
      "\n",
      "Epoch:28\n",
      "kysetoraaurus\n",
      "brlsceos\n",
      "jiangiaseosaurus\n",
      "epbtopysaurus\n",
      "qoansaseoaauras\n",
      "coencodgtsiurus\n",
      "beronysaurus\n",
      "kiaescsaurus\n",
      "yangunarrus\n",
      "houdsaurus\n",
      "zhyrgosoasaurus\n",
      "qiscgonapernaas\n",
      "meuptaurus\n",
      "virivturus\n",
      "onicaurus\n",
      "\n",
      "Epoch:29\n",
      "racooaaurus\n",
      "etasrsturus\n",
      "gfpinyaurus\n",
      "kinalsas\n",
      "us\n",
      "bargbnngurus\n",
      "weuiudoosaurus\n",
      "gunirsainaurus\n",
      "lubeon\n",
      "weltipaurus\n",
      "itdttsgaonausaurur\n",
      "vedataran\n",
      "veriboniurus\n",
      "zhynsarosaurus\n",
      "monposapaurus\n",
      "\n",
      "Epoch:30\n",
      "ngtacsua\n",
      "okoiahrus\n",
      "oructsotoss\n",
      "ovontot\n",
      "ximahiurus\n",
      "fernaalrus\n",
      "xlanysturus\n",
      "qirngusaurun\n",
      "kiaasasaurus\n",
      "walcsgraurus\n",
      "sthtrsaurus\n",
      "trrrosapalrus\n",
      "tetaasg\n",
      "tiksnnaurus\n",
      "kucotosaurus\n",
      "\n",
      "Epoch:31\n",
      "urostcrus\n",
      "usatgtdaurus\n",
      "mohgasaptsu\n",
      "siuronaurus\n",
      "fusubosaurus\n",
      "wucconannaurus\n",
      "phtouisturus\n",
      "ilusornsaor\n",
      "ylsgrnt\n",
      "ogopaurus\n",
      "paraktsturus\n",
      "elorotipturus\n",
      "zaepeioirrus\n",
      "ypctjjaurus\n",
      "wulsurnasausausau\n",
      "\n",
      "Epoch:32\n",
      "hencsaurus\n",
      "tatgarashoiu\n",
      "qmwnuasosaurus\n",
      "conhtocl\n",
      "edatasesauruk\n",
      "xesjharatrps\n",
      "grspinaurus\n",
      "lorparaorus\n",
      "tetaisacrus\n",
      "becrbqups\n",
      "nttoraurus\n",
      "ripttanasrus\n",
      "kesagopanaugoa\n",
      "canttomaurus\n",
      "ksnysoyspcrus\n",
      "\n",
      "Epoch:33\n",
      "vneoct\n",
      "fesaangtaurus\n",
      "xunndruus\n",
      "usunuurus\n",
      "kanacosaoptmrus\n",
      "weltnacrus\n",
      "ziuciosgisaurus\n",
      "brtalidaurus\n",
      "abcoraurus\n",
      "zauahyoyesiurus\n",
      "beytiottarus\n",
      "yuanevatrua\n",
      "iaugnchurus\n",
      "veroshenrosaurus\n",
      "flracsasaurus\n",
      "\n",
      "Epoch:34\n",
      "jangsnraurus\n",
      "auctstengtotsisos\n",
      "flo\n",
      "besasaurus\n",
      "iandsaurus\n",
      "kurucosaurus\n",
      "kwsarosauris\n",
      "bateuasrua\n",
      "lephocochusus\n",
      "ususaurut\n",
      "pintuanadrus\n",
      "ceycguacros\n",
      "igcslaurus\n",
      "trtoisaurus\n",
      "totaonajsaurus\n",
      "\n",
      "Epoch:35\n",
      "zrnahitis\n",
      "quanyoudosotrqunuurus\n",
      "kalbcgsaurus\n",
      "vilapaurus\n",
      "xactqtassosaurus\n",
      "epmtoccurus\n",
      "juanaasg\n",
      "piktnicaurus\n",
      "kastnngosaurus\n",
      "crdirapeurus\n",
      "kaseanarathr\n",
      "chtitorsiurus\n",
      "hyroauoasasrus\n",
      "vetados\n",
      "xianggdsasnurus\n",
      "\n",
      "Epoch:36\n",
      "zrocoraurps\n",
      "trcgicaurus\n",
      "racorapeor\n",
      "jaohgursmaurus\n",
      "molyooscurus\n",
      "kesatis\n",
      "xannamaurus\n",
      "xacsitaurus\n",
      "roshysoshurus\n",
      "idsconscaurus\n",
      "gtlgasaurus\n",
      "loshineurus\n",
      "fusyani\n",
      "vepatosaurus\n",
      "ndtinaurus\n",
      "\n",
      "Epoch:37\n",
      "usaspurus\n",
      "zhysiosoerus\n",
      "grbsnrdauros\n",
      "juanataurus\n",
      "vironaroptus\n",
      "brtamnabsaurus\n",
      "esiaolsraurus\n",
      "potatosaurus\n",
      "fultuocaurus\n",
      "quctcaukan\n",
      "pslacrdaurus\n",
      "wunnaurus\n",
      "gnomubgnlaurus\n",
      "bahoraurus\n",
      "fasaiuurus\n",
      "\n",
      "Epoch:38\n",
      "xingpiusus\n",
      "lopaonapmurus\n",
      "paoe\n",
      "biirmiclrrus\n",
      "agrussasaurus\n",
      "esuaoranter\n",
      "euacorators\n",
      "acsastisaurus\n",
      "ropivpsauras\n",
      "saltaonperus\n",
      "tigoil\n",
      "xianzotfoosaurus\n",
      "ltniosapharu\n",
      "wrltaeinahoeha\n",
      "xianzotataurus\n",
      "\n",
      "Epoch:39\n",
      "jiyois\n",
      "quaniamasrus\n",
      "jennaohaurus\n",
      "glvpvasostor\n",
      "biypnosierus\n",
      "drcodoc\n",
      "brraoiiurus\n",
      "rivlydoshurus\n",
      "purirotagnaures\n",
      "kuaatmbiltor\n",
      "socitryarssoastou\n",
      "gnrpuaisaurus\n",
      "gubduraphor\n",
      "iatansurus\n",
      "eslcirovaurus\n",
      "\n",
      "Epoch:40\n",
      "phnahsaurus\n",
      "wrnabnqsaurus\n",
      "potcyrpnaosaurus\n",
      "auairanter\n",
      "fuaaur\n",
      "aqcunbcrus\n",
      "saurotaurus\n",
      "fvshtsucaurus\n",
      "quesaceulus\n",
      "xicgxaaurus\n",
      "brtemosaurus\n",
      "crdatfpsaurus\n",
      "junagisgkaurus\n",
      "bufottnaurus\n",
      "riptrcaurus\n",
      "\n",
      "Epoch:41\n",
      "fuluaaurus\n",
      "hulidourus\n",
      "vertthasontureuras\n",
      "fakubois\n",
      "agsoaarros\n",
      "quanyosaurus\n",
      "irngxs\n",
      "ciusaonaas\n",
      "esetadopaor\n",
      "uriaterusaos\n",
      "drngrsgwsnsiukus\n",
      "unuaopokops\n",
      "ligsenasaurus\n",
      "kusrhersaurus\n",
      "kuaeccisaurus\n",
      "\n",
      "Epoch:42\n",
      "fur\n",
      "imethectlrus\n",
      "kasuonisaurus\n",
      "nstbhocaurus\n",
      "kaitnasaurus\n",
      "yanypsaurus\n",
      "clriuosauras\n",
      "qacrdont\n",
      "pardarcs\n",
      "libranusourus\n",
      "keskcusshus\n",
      "cemarodauris\n",
      "xiancsanrus\n",
      "burueosaurus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gyroospcrus\n",
      "\n",
      "Epoch:43\n",
      "rucontgrus\n",
      "piktmr\n",
      "ntcososeots\n",
      "jinorturus\n",
      "hamlcaurus\n",
      "dacpnaonhurus\n",
      "hevoogostor\n",
      "nokysaurus\n",
      "vecesaravaar\n",
      "namnqhnaurus\n",
      "kuittosaurus\n",
      "nitoucosaurus\n",
      "bucatacrus\n",
      "ylasantosaurus\n",
      "usoiviurus\n",
      "\n",
      "Epoch:44\n",
      "ceparoguptdr\n",
      "sabmcorapops\n",
      "gosgsshurus\n",
      "vuritorantir\n",
      "nasaucasacrus\n",
      "dicoantsourus\n",
      "tetjnvosaurus\n",
      "galoaoes\n",
      "saroaonoahauasuurus\n",
      "yunhshurus\n",
      "irtaonaisaurus\n",
      "wrlaeootops\n",
      "beuiucsaurus\n",
      "ilutsaurus\n",
      "tamoasaurus\n",
      "\n",
      "Epoch:45\n",
      "uaoriur\n",
      "osamtntpaurus\n",
      "krnyongot\n",
      "glnahsaurus\n",
      "usiangsaurus\n",
      "dyptatqsaurus\n",
      "ntnoruasaurus\n",
      "jirggoqaichurus\n",
      "betntespurus\n",
      "glypnouter\n",
      "dalpcosmcrus\n",
      "kilolhesaurus\n",
      "qurnldosgys\n",
      "vintcor\n",
      "perasguaauras\n",
      "\n",
      "Epoch:46\n",
      "etaiasa\n",
      "ustt\n",
      "fustthsaurus\n",
      "visanenrus\n",
      "wapaesgbaurus\n",
      "tregusourus\n",
      "hapsovkurus\n",
      "ginalsas\n",
      "mtcdsacrus\n",
      "onasaurus\n",
      "nttrnerntrps\n",
      "drsamgang\n",
      "hulwaang\n",
      "pietelashos\n",
      "euatosaurus\n",
      "\n",
      "Epoch:47\n",
      "runtos\n",
      "rhnblsasipad\n",
      "xiangong\n",
      "qualurourus\n",
      "ceoonvoratops\n",
      "palocolosrus\n",
      "rictopaurus\n",
      "wuksrsaurus\n",
      "esitrrapter\n",
      "zaucodatods\n",
      "queicrdguit\n",
      "ososaurus\n",
      "juannucg\n",
      "paltcrnpfrus\n",
      "qucripaurus\n",
      "\n",
      "Epoch:48\n",
      "rucousaurus\n",
      "kurjtoaracrus\n",
      "onoanio\n",
      "epatnravaos\n",
      "vulotropaurus\n",
      "huptanras\n",
      "waneuaauras\n",
      "etedasasturus\n",
      "vinatoryurus\n",
      "tanhantauoud\n",
      "quhcomiurus\n",
      "zhynyasorianossurus\n",
      "palackourus\n",
      "pard\n",
      "opcoshesa\n",
      "\n",
      "Epoch:49\n",
      "usus\n",
      "rstoicseurus\n",
      "zoubgjdgsaurus\n",
      "nrmbgasgibaorus\n",
      "saurunaurus\n",
      "zhonoang\n",
      "cerasaurus\n",
      "rameyinasaurus\n",
      "usuongsaurus\n",
      "kutaoraurus\n",
      "ssccinbopaurus\n",
      "rivtrdosteryx\n",
      "tythoso\n",
      "thpaloceurus\n",
      "chacmorakrus\n",
      "\n",
      "Epoch:50\n",
      "saucsnurus\n",
      "anturtos\n",
      "nimadscrus\n",
      "nctnariiuras\n",
      "vetluesaurus\n",
      "xixngrso\n",
      "flnaasasaurus\n",
      "yahntodapa\n",
      "zuosgorosaurus\n",
      "hipoochol\n",
      "drcsdrc\n",
      "aurapaoooc\n",
      "uaiskurus\n",
      "weloroteurus\n",
      "velarpcrus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time train(model, loss_fn, optimizer, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "Измените код выше так, чтобы генерировались панграммы – имена динозавров, не содержащие повторяющихся букв."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование LSTM\n",
    "\n",
    "**Долгая краткосрочная память** *(long short-term memory; LSTM)* – особая разновидность архитектуры рекуррентных нейронных сетей, способная к обучению долговременным зависимостям. \n",
    "\n",
    "* [Подробное объяснение по-английски](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Подробное объяснение по-русски](https://habr.com/ru/company/wunderfund/blog/331310/) (перевод английской статьи)\n",
    "\n",
    "\n",
    "![lstm](img/understanding_lstms.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.linear_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_u = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_c = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, c_prev, h_prev, x):\n",
    "        combined = torch.cat([x, h_prev], 1)\n",
    "        f = torch.sigmoid(self.linear_f(combined))\n",
    "        u = torch.sigmoid(self.linear_u(combined))\n",
    "        c_tilde = torch.tanh(self.linear_c(combined))\n",
    "        c = f*c_prev + u*c_tilde\n",
    "        o = torch.sigmoid(self.linear_o(combined))\n",
    "        h = o*torch.tanh(c)\n",
    "        y = self.i2o(h)\n",
    "        \n",
    "        return c, h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        c_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        h_prev = torch.zeros_like(c_prev)\n",
    "        idx = random.randint(1, 26)\n",
    "        x = c_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        x[0, idx] = 1\n",
    "        sampled_indexes = [idx]\n",
    "        n_chars = 1\n",
    "        newline_char_idx = trn_ds.ch_to_idx['\\n']\n",
    "        while n_chars != 50 and idx != newline_char_idx:\n",
    "            c_prev, h_prev, y_pred = model(c_prev, h_prev, x)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=torch.softmax(y_pred, 1).cpu().numpy().ravel())\n",
    "            sampled_indexes.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    "            \n",
    "            n_chars += 1\n",
    "            \n",
    "            if n_chars == 50:\n",
    "                sampled_indexes.append(newline_char_idx)\n",
    "                \n",
    "    model.train()\n",
    "    return sampled_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        c_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        h_prev = torch.zeros_like(c_prev)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            c_prev, h_prev, y_pred = model(c_prev, h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "yancnaurus\n",
      "rasmasaurus\n",
      "tanynsnaurus\n",
      "lonuuaurus\n",
      "libasasaurus\n",
      "aboasaurus\n",
      "avtrgosaurus\n",
      "uttosmurus\n",
      "tetasis\n",
      "xexjaansca\n",
      "vidosiratops\n",
      "arkhumorniahodaurus\n",
      "ubetocaurus\n",
      "vikositaptor\n",
      "ssnovaurus\n",
      "\n",
      "Epoch:2\n",
      "aniamturus\n",
      "elsacrus\n",
      "tecrchyrpurus\n",
      "ceiogysgosaurus\n",
      "grdtoteo\n",
      "chbamrpaurus\n",
      "zhuciengoaurus\n",
      "mirtanmaurus\n",
      "mocdonainaurus\n",
      "jivataosn\n",
      "ugerouaurus\n",
      "perahoasgsaurus\n",
      "zatcmaor\n",
      "yunsdaurus\n",
      "coiwnihusgosaurus\n",
      "\n",
      "Epoch:3\n",
      "viaatrangosaurus\n",
      "tiuarssaurus\n",
      "grllixeameipelto\n",
      "laphaniauras\n",
      "pawamosaurus\n",
      "yotoitotan\n",
      "echopatops\n",
      "ulaoatopeobrus\n",
      "barosontosaurus\n",
      "galoenoptor\n",
      "wanlardsurus\n",
      "yitnserosaurus\n",
      "qunouuagodaurus\n",
      "fakuiaaurus\n",
      "lobitosaptor\n",
      "\n",
      "Epoch:4\n",
      "donkvaurus\n",
      "eonaurus\n",
      "dracrpaurus\n",
      "marhisaurus\n",
      "megromturus\n",
      "haanbhhanlung\n",
      "ciaocteocaurus\n",
      "zaunhengpsaurus\n",
      "majhairasaurus\n",
      "abchieodnishosaurus\n",
      "prrcwtsauras\n",
      "palodochpatops\n",
      "juandsnanoturus\n",
      "elrpurunslta\n",
      "diahrisaurus\n",
      "\n",
      "Epoch:5\n",
      "ablhsaurus\n",
      "kunyaosaurus\n",
      "synmospurus\n",
      "euaonodhess\n",
      "rashecscsaurus\n",
      "euohrpurus\n",
      "ertalaurus\n",
      "keuaarocaurus\n",
      "rocporoisus\n",
      "beochytirosa\n",
      "deahoaosaurus\n",
      "giguntosaurus\n",
      "mostelrsaurus\n",
      "utaas\n",
      "peobogopatops\n",
      "\n",
      "Epoch:6\n",
      "atchaninturus\n",
      "isgihanvsaurus\n",
      "yoagonoosaurus\n",
      "wanamasgosaurus\n",
      "kolorguurus\n",
      "taraceon\n",
      "basacoiaaurus\n",
      "eucivrocaurus\n",
      "hulxninns\n",
      "penampaoluaaaia\n",
      "varoiaraptor\n",
      "cotptlouaurus\n",
      "isaimeosaurus\n",
      "qilbomolsaurus\n",
      "subospurus\n",
      "\n",
      "Epoch:7\n",
      "futuirnmtor\n",
      "quasnueryx\n",
      "thkopaurus\n",
      "sinospurus\n",
      "veuodosamtor\n",
      "eubpnridhebs\n",
      "proearasauras\n",
      "brachylrntosaurus\n",
      "loaoeuaurus\n",
      "caudiao\n",
      "giyptdyx\n",
      "rigqsaurus\n",
      "cpbarasaurus\n",
      "uctratauitan\n",
      "tevmscosaurus\n",
      "\n",
      "Epoch:8\n",
      "eusiuruh\n",
      "nhcnuurus\n",
      "igoraurus\n",
      "ocoitoosemus\n",
      "lapnevosauras\n",
      "yaencohreeosaurus\n",
      "odoaeratops\n",
      "silroouurus\n",
      "qelnchoesaurus\n",
      "obnithoduurus\n",
      "sauromaurus\n",
      "pitopgpaurus\n",
      "ugubdrlanis\n",
      "leaogorurus\n",
      "fukuisgtsaurus\n",
      "\n",
      "Epoch:9\n",
      "xiantauaurus\n",
      "naogsuris\n",
      "nexsturus\n",
      "ilosaurus\n",
      "coaniapaurus\n",
      "jengbhanosaurus\n",
      "yuaotaurus\n",
      "ruthysourus\n",
      "iscsaurus\n",
      "walarohdslaurus\n",
      "osnithomimus\n",
      "euainacaurus\n",
      "nasoangua\n",
      "absanpurus\n",
      "drlloskysaurus\n",
      "\n",
      "Epoch:10\n",
      "chanobhangsaurus\n",
      "eolauippurus\n",
      "zuochingsaurus\n",
      "zhachengtcaurus\n",
      "useobyilrratops\n",
      "fotalwsaurus\n",
      "inclpurus\n",
      "nauqaensosaurus\n",
      "zuengsianosaurus\n",
      "abchaeocaurus\n",
      "anahamolaurus\n",
      "zuangtriis\n",
      "junngjuashurus\n",
      "fetanolcaurus\n",
      "hudlvpurus\n",
      "\n",
      "Epoch:11\n",
      "indsaurus\n",
      "wlpcondksaurus\n",
      "prrarauroconhus\n",
      "omoithonimus\n",
      "yunnahsauras\n",
      "otaithomemus\n",
      "usalosolssnrpes\n",
      "yungsoualsaurus\n",
      "juangjioanlong\n",
      "giyouetosaurus\n",
      "grltis\n",
      "ornisdisesd\n",
      "eurapelta\n",
      "vogososaptor\n",
      "futawosaurus\n",
      "\n",
      "Epoch:12\n",
      "ibgsaurus\n",
      "brnaneioaurus\n",
      "inscaurus\n",
      "rolhypaurus\n",
      "galvesaurus\n",
      "harisaurus\n",
      "kunxarssaurus\n",
      "trprhidosaurus\n",
      "nubegtasaurus\n",
      "zaophingusaurus\n",
      "hupostcrus\n",
      "coeppoadsaurus\n",
      "chanljunogsaurus\n",
      "eosontlta\n",
      "mecdslturus\n",
      "\n",
      "Epoch:13\n",
      "ribtos\n",
      "deaycnotiosuvtes\n",
      "umysiosourus\n",
      "quathualseiaurus\n",
      "nobentonaurus\n",
      "roptopaorao\n",
      "galvesaurus\n",
      "tandsiis\n",
      "raptosex\n",
      "nomegtovaurus\n",
      "hiabgresaurus\n",
      "janghoumaurus\n",
      "naspsaurus\n",
      "qunlong\n",
      "tecciario\n",
      "\n",
      "Epoch:14\n",
      "bariaasauras\n",
      "diyctgpondylus\n",
      "montovanator\n",
      "stnradon\n",
      "zhengoenithosimus\n",
      "napthvaurus\n",
      "henchuaauras\n",
      "frbadonaurus\n",
      "yutgunoosaurus\n",
      "quitbinglsaurus\n",
      "islbaurus\n",
      "wukkonojskyin\n",
      "huaxsjinngosaurus\n",
      "ntbegtasaurus\n",
      "ecouroitholaurus\n",
      "\n",
      "Epoch:15\n",
      "rinqtauros\n",
      "actasapaurus\n",
      "valbocaurus\n",
      "kuyuiraptor\n",
      "nowagtasaurus\n",
      "elroailta\n",
      "wanbtuasiaurus\n",
      "brnyorosaurus\n",
      "juazhhanssaurus\n",
      "qanelhousaurus\n",
      "mlmvonruras\n",
      "lakoesitaurus\n",
      "karrrasaurus\n",
      "wunnentrouaurus\n",
      "uleberdturus\n",
      "\n",
      "Epoch:16\n",
      "xianeona\n",
      "qubnzhuhsaurus\n",
      "wavnersr\n",
      "yiankoctosaurus\n",
      "jepghhanowaurus\n",
      "velopitaurus\n",
      "lencarasaurus\n",
      "dalhomg\n",
      "kuesusuurus\n",
      "wanlosoansaurus\n",
      "vidocoaastor\n",
      "drkhcoasaurus\n",
      "prragospurus\n",
      "fakuisaurus\n",
      "natkaodaurus\n",
      "\n",
      "Epoch:17\n",
      "yitxianosauruskrlriodon\n",
      "onnithoeimus\n",
      "zeuchengesaurus\n",
      "velopitaptor\n",
      "linaetatlsaurus\n",
      "ganemaurus\n",
      "creostnit\n",
      "eosorcosaurus\n",
      "janlhamsaurus\n",
      "jiazhousaurus\n",
      "ugroptor\n",
      "wurconea\n",
      "yarnocon\n",
      "nareosaurus\n",
      "wuwgnsssaurus\n",
      "\n",
      "Epoch:18\n",
      "venoraraurus\n",
      "ceuadhaaimaurus\n",
      "xixniangosaurus\n",
      "eorosbithosas\n",
      "brachyaoboiodon\n",
      "luntanioraurus\n",
      "lephianaurus\n",
      "tetacssaurus\n",
      "mardetoodaurus\n",
      "quezoniosaurus\n",
      "janehanosaurus\n",
      "pataetsuurus\n",
      "parnpuurosomhus\n",
      "quaogkolsaurus\n",
      "mactaruurus\n",
      "\n",
      "Epoch:19\n",
      "conptoppurus\n",
      "malsaoaurus\n",
      "sauraphyraurus\n",
      "hoxcsaurus\n",
      "anyertasaurus\n",
      "caintdcaurus\n",
      "galbiisaurus\n",
      "qungxiusaurus\n",
      "hepappurus\n",
      "qingbiusnsaurus\n",
      "nunrosaurus\n",
      "lonoosasaurus\n",
      "tetaaskes\n",
      "hunicpurus\n",
      "eisupllosaurus\n",
      "\n",
      "Epoch:20\n",
      "craancoson\n",
      "quagxialnsaurus\n",
      "dynoeplusaurus\n",
      "ankrsasaurus\n",
      "xenggolaa\n",
      "kunecocaurus\n",
      "grrtesaurus\n",
      "jitngjaascauris\n",
      "oraithomemus\n",
      "krscton\n",
      "besrltaurus\n",
      "shndnaurus\n",
      "lapgahanosaurus\n",
      "xufipsosaurus\n",
      "quntbiua\n",
      "\n",
      "Epoch:21\n",
      "wybnjuaataurus\n",
      "zeangluaurus\n",
      "wilkertaurus\n",
      "jilngxalnolaurus\n",
      "coelaus\n",
      "muntosaurus\n",
      "frxuitaurus\n",
      "fetiiraurus\n",
      "blconlcaurus\n",
      "elrorimus\n",
      "zuungsiaos\n",
      "hexinosaurus\n",
      "xixnjhunosaurus\n",
      "hieroreurus\n",
      "lanhaa\n",
      "\n",
      "Epoch:22\n",
      "ubeluurus\n",
      "nempgtasaurus\n",
      "uttspipnorsaptor\n",
      "riagogsooagoa\n",
      "velociaaptor\n",
      "patoeoksydon\n",
      "notpgtodaurus\n",
      "mactecaurus\n",
      "utaotoyatm\n",
      "thmrshosaurus\n",
      "wekioctouaurus\n",
      "xiujiangootan\n",
      "mortesonyoosaurus\n",
      "mardrlrcaurus\n",
      "nomegtesaurus\n",
      "\n",
      "Epoch:23\n",
      "coilnoluurus\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-fcbf46d13156>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-50616d392028>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, optimizer, dataset, epochs)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch:{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-7fcf8aa2e3f9>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mc_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mline_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "Написать функцию ```get_prob()```, оценивающую веростность порождения одной строки (из файла) и найти самую вероятную строку, порождаемую каждой из трех языковых моделей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Источники\n",
    "\n",
    "1. [Speech & Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)\n",
    "2. [Динозавры – 1](https://github.com/furkanu/deeplearning.ai-pytorch/tree/master/5-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model)\n",
    "3. [Динозавры – 2](https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb)\n",
    "4. [Статья, объясняющая LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "## BERT, ELMo, ULMFiT\n",
    "\n",
    "1. [BERT, ELMo & Co в картинках](http://jalammar.github.io/illustrated-bert/)\n",
    "2. ELMo — Embeddings from Language Models\n",
    "    * [Слайды про ELMo](https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018)\n",
    "    * [Оригинальная статья](https://arxiv.org/abs/1802.05365)\n",
    "    * [Англоязычное объяснение на towardsdatascience](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)\n",
    "3. BERT — Bidirectional Encoder Representations from Transformers\n",
    "    * [Русскоязычный тьюториал по BERT](https://habr.com/ru/post/436878/)\n",
    "    * [Оригинальная статья](https://arxiv.org/abs/1810.04805)\n",
    "    * [Англоязычное объяснение на towardsdatascience](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "4. ULMFiT — Universal Language Model Fine-Tuning\n",
    "    * [Оригинальная статья](https://arxiv.org/abs/1801.06146)\n",
    "    * [Англоязычный тьюториал](https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/)\n",
    "    * [Подробный разбор на английском](https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
