{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "САД NLP 1 | морфология.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oserikov/data-science-nlp/blob/master/1_%D0%BC%D0%BE%D1%80%D1%84%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIkpnJD7X3Xj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "df21b7f3-8dd8-4ce7-b2be-94cf92498e3b"
      },
      "source": [
        "!pip -qq install yargy --progress-bar off\n",
        "!pip -qq install pymorphy2 --progress-bar off\n",
        "!pip -qq install -U PyYAML --progress-bar off\n",
        "!pip -qq install rnnmorph --progress-bar off\n",
        "!pip -qq install rusenttokenize --progress-bar off\n",
        "!pip -qq install pymystem3==0.1.10\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from collections import defaultdict as dd\n",
        "from operator import itemgetter\n",
        "from pymorphy2.tokenizers import simple_word_tokenize as word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "import string\n",
        "import pymorphy2\n",
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "\n",
        "\n",
        "import warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.warn = warn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\n",
            "\u001b[?25h\u001b[?25l\n",
            "\u001b[?25h\u001b[?25l\n",
            "\u001b[?25h\u001b[?25l\n",
            "\u001b[?25h  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l\n",
            "\u001b[?25h  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JQXhd1x0f7LE"
      },
      "source": [
        "## Морфологический анализ\n",
        "\n",
        "Задачи морфологического анализа:\n",
        "\n",
        "* Разбор слова — определение нормальной формы (леммы), основы (стема) и грамматических характеристик слова\n",
        "* Синтез словоформы — генерация словоформы по заданным грамматическим характеристикам из леммы\n",
        "\n",
        "Морфологический анализ — не самая сильная сторона NLTK.\n",
        "\n",
        "## POS-tagging\n",
        "\n",
        "**Частеречная разметка**, или **POS-tagging** _(part of speech tagging)_ —  определение части речи и грамматических характеристик слов в тексте (корпусе) с приписыванием им соответствующих тегов.\n",
        "\n",
        "Для большинства слов возможно несколько разборов (т.е. несколько разных лемм, несколько разных частей речи и т.п.). Теггер генерирует  все варианты, ранжирует их по вероятности и по умолчанию выдает наиболее вероятный. Выбор одного разбора из нескольких называется **снятием омонимии**, или **дизамбигуацией**.\n",
        "\n",
        "### Наборы тегов\n",
        "\n",
        "Существует множество наборов грамматических тегов, или тегсетов:\n",
        "* НКРЯ\n",
        "* Mystem\n",
        "* UPenn\n",
        "* OpenCorpora (его использует pymorphy2)\n",
        "* Universal Dependencies\n",
        "* ...\n",
        "\n",
        "Есть даже [библиотека](https://github.com/kmike/russian-tagsets) для преобразования тегов из одной системы в другую для русского языка, `russian-tagsets`. Но важно помнить, что преобразования бывают с потерями.\n",
        "\n",
        "На данный момент стандартом является **Universal Dependencies**. Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги — [вот тут](http://universaldependencies.org/u/pos/). Вот список основных (частереных) тегов UD:\n",
        "\n",
        "* ADJ: adjective\n",
        "* ADP: adposition\n",
        "* ADV: adverb\n",
        "* AUX: auxiliary\n",
        "* CCONJ: coordinating conjunction\n",
        "* DET: determiner\n",
        "* INTJ: interjection\n",
        "* NOUN: noun\n",
        "* NUM: numeral\n",
        "* PART: particle\n",
        "* PRON: pronoun\n",
        "* PROPN: proper noun\n",
        "* PUNCT: punctuation\n",
        "* SCONJ: subordinating conjunction\n",
        "* SYM: symbol\n",
        "* VERB: verb\n",
        "* X: other\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICje2dEgLCw",
        "colab_type": "text"
      },
      "source": [
        "### pymystem3\n",
        "\n",
        "**pymystem3** — это питоновская обертка для яндексовского морфологичекого анализатора Mystem. Его можно скачать отдельно и использовать из консоли. Может работать с незнакомыми словами (out-of-vocabulary words, OOV).\n",
        "\n",
        "\n",
        "* [Документация Mystem](https://tech.yandex.ru/mystem/doc/index-docpage/)\n",
        "* [Документация pymystem3](http://pythonhosted.org/pymystem3/)\n",
        "\n",
        "Инициализируем Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "* mystem_bin - путь к `mystem`, если их несколько\n",
        "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH9EeiCMFC70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8b0e8cc3-e6a7-4324-a7fd-87c26c52fb9f"
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "\n",
        "# сохраняем класс в переменную\n",
        "mystem = Mystem() \n",
        "\n",
        "text = \"\"\"Система состоит из камеры и программного обеспечения, которое анализирует фотографию.\n",
        " Суть технологии — сопоставление лиц, попавших в объектив, с изображениями из базы данных\"\"\"\n",
        "\n",
        "print(text)\n",
        "print(mystem.lemmatize(text))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Система состоит из камеры и программного обеспечения, которое анализирует фотографию.\n",
            " Суть технологии — сопоставление лиц, попавших в объектив, с изображениями из базы данных\n",
            "['система', ' ', 'состоять', ' ', 'из', ' ', 'камера', ' ', 'и', ' ', 'программный', ' ', 'обеспечение', ', ', 'который', ' ', 'анализировать', ' ', 'фотография', '.', '\\n', ' ', 'суть', ' ', 'технология', ' — ', 'сопоставление', ' ', 'лицо', ', ', 'попадать', ' ', 'в', ' ', 'объектив', ', ', 'с', ' ', 'изображение', ' ', 'из', ' ', 'база', ' ', 'данные', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwCGhezJFRKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d1979422-bf00-4e62-a871-1519c82ec2bd"
      },
      "source": [
        "\n",
        "# метод .analyze() даст грамматическую информацию о словах\n",
        "words_analized = mystem.analyze(text)\n",
        "\n",
        "print('Слово - ', words_analized[0]['text'])\n",
        "print('Разбор слова - ', words_analized[0]['analysis'][0])\n",
        "print('Лемма слова - ', words_analized[0]['analysis'][0]['lex'])\n",
        "print('Грамматическая информация слова - ', words_analized[0]['analysis'][0]['gr'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Слово -  Система\n",
            "Разбор слова -  {'lex': 'система', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}\n",
            "Лемма слова -  система\n",
            "Грамматическая информация слова -  S,жен,неод=им,ед\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0FElWVK47AT",
        "colab_type": "text"
      },
      "source": [
        "###  pymorphy2\n",
        "\n",
        "**pymorphy2** — это полноценный морфологический анализатор, целиком написанный на Python. Он также умеет ставить слова в нужную форму (спрягать и склонять). Может работать с незнакомыми словами (out-of-vocabulary words, OOV).\n",
        "\n",
        "[Документация pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "913cWQ-8hiCA",
        "colab_type": "text"
      },
      "source": [
        "### rnnmorph\n",
        "**rnnmorph** &mdash; это морфологический анализатор на нейросетях, занявший первое место на дорожке по морофологическому анализу [\"Диалога 2017\"](http://www.dialog-21.ru/evaluation/2017/morphology/). \n",
        "\n",
        "Предлагает меньшее количество тегов в разборе.\n",
        "\n",
        "Работает заметно медленнее, чем pymorphy и mystem.\n",
        "\n",
        "[Документация rnnmorph](https://github.com/IlyaGusev/rnnmorph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1-wejHiXjq",
        "colab_type": "text"
      },
      "source": [
        "### maru\n",
        "**maru** — это морфологический анализатор на нейросетях, в документации указаны результаты чуть лучше, чем у rnnmorph на той же задаче.\n",
        "\n",
        "Тоже медленный\n",
        "\n",
        "[Документация maru](https://github.com/chomechome/maru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_O8y9-i9xR",
        "colab_type": "text"
      },
      "source": [
        "### Классификация новостей Ленты по темам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p64AMTh8X5lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q -O lenta-ru-news-part.csv https://www.dropbox.com/s/ja23c9l1ppo9ix7/lenta-ru-news-part.csv?dl=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdKArr-RZ8Pj",
        "colab_type": "code",
        "outputId": "577b5cb1-9fbd-42b6-aece-1670c8690913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "lenta = pd.read_csv('lenta-ru-news-part.csv', usecols=['title', 'text', 'topic'])\n",
        "lenta[\"topic_cat\"] = lenta.topic.astype('category').cat.codes\n",
        "\n",
        "def reload_lenta_dataset():\n",
        "    global lenta\n",
        "    lenta = pd.read_csv('lenta-ru-news-part.csv', usecols=['title', 'text', 'topic'])\n",
        "    lenta = lenta.sample(frac=1).reset_index(drop=True).dropna(subset = ['text', 'topic'])\n",
        "    lenta[\"topic_cat\"] = lenta.topic.astype('category').cat.codes\n",
        "\n",
        "\n",
        "\n",
        "lenta.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>topic_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Австрия не представила доказательств вины росс...</td>\n",
              "      <td>Австрийские правоохранительные органы не предс...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Овечкин повторил свой рекорд</td>\n",
              "      <td>Капитан «Вашингтона» Александр Овечкин сделал...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Названы регионы России с самым дорогим и дешев...</td>\n",
              "      <td>Производитель онлайн-касс «Эвотор» проанализир...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Россию и Украину пригласили на переговоры по газу</td>\n",
              "      <td>Вице-президент Еврокомиссии Марош Шефчович при...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Хоккеист НХЛ забросил шайбу с отрицательного угла</td>\n",
              "      <td>Нападающий клуба «Эдмонтон Ойлерс» Коннор Макд...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topic_cat\n",
              "0  Австрия не представила доказательств вины росс...  ...         3\n",
              "1                       Овечкин повторил свой рекорд  ...         3\n",
              "2  Названы регионы России с самым дорогим и дешев...  ...         4\n",
              "3  Россию и Украину пригласили на переговоры по газу  ...         4\n",
              "4  Хоккеист НХЛ забросил шайбу с отрицательного угла  ...         3\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu4Dbciok_9_",
        "colab_type": "code",
        "outputId": "d125de61-1e8d-40ae-dd9b-39b05ad61104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "lenta.topic.value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Экономика          79538\n",
              "Спорт              64421\n",
              "Культура           53803\n",
              "Наука и техника    53136\n",
              "Бизнес              7399\n",
              "Name: topic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClR-YWl4lC5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_test_split(source_col, target_col, source_features_encoder):\n",
        "    X = source_features_encoder(source_col)\n",
        "    y = target_col\n",
        "    \n",
        "    training_size = len(source_col)*70//100\n",
        "    X_train, y_train = X[:training_size], y[:training_size]\n",
        "    X_test, y_test = X[training_size:], y[training_size:]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def run_experiment(dataset, dataset_part_size, source_features_encoder):\n",
        "    \n",
        "    \n",
        "    print(len(dataset))\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(5):\n",
        "        dataset_shuf = dataset.sample(frac=1)[:dataset_part_size]\n",
        "        print(len(dataset_shuf))\n",
        "\n",
        "        X_train, y_train, X_test, y_test = train_test_split(dataset_shuf.text, dataset_shuf.topic_cat, source_features_encoder)\n",
        "\n",
        "        clf = LogisticRegression()\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        predictions = clf.predict(X_test)\n",
        "\n",
        "        precision = precision_score(y_test.values, predictions, average='weighted')\n",
        "        recall = recall_score(y_test.values, predictions, average='weighted')\n",
        "        f1 = f1_score(y_test.values, predictions, average='weighted')\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"precision {str(precision)}\")\n",
        "        print(f\"recall {str(recall)}\")\n",
        "        print(f\"f1-score {str(f1)}\")\n",
        "\n",
        "    print('*' * 10)\n",
        "    print(\"mean precision\", np.mean(precision_scores))\n",
        "    print(\"mean recall\", np.mean(recall_scores))\n",
        "    print(\"mean f1-score\", np.mean(f1_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf5BH0rAlbNY",
        "colab_type": "text"
      },
      "source": [
        "### наивное решение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ZFnwHKkBM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "def naive_features_tfidf_encoder(text_col):\n",
        "    return tfidf_vectorizer.fit_transform(text_col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U4yLtJgmYkJ",
        "colab_type": "code",
        "outputId": "f3572082-e55f-4b54-fe5c-a3d74e1dada5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "run_experiment(lenta, 5000, naive_features_tfidf_encoder)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "258297\n",
            "5000\n",
            "precision 0.9084456093203755\n",
            "recall 0.9333333333333333\n",
            "f1-score 0.9196198747258548\n",
            "5000\n",
            "precision 0.8988434882191244\n",
            "recall 0.924\n",
            "f1-score 0.9104899256324784\n",
            "5000\n",
            "precision 0.9175120999762175\n",
            "recall 0.9373333333333334\n",
            "f1-score 0.9261941123947085\n",
            "5000\n",
            "precision 0.9133043288950023\n",
            "recall 0.9366666666666666\n",
            "f1-score 0.9238255365481983\n",
            "5000\n",
            "precision 0.9041574823929077\n",
            "recall 0.9266666666666666\n",
            "f1-score 0.9141783018186467\n",
            "**********\n",
            "mean precision 0.9084526017607255\n",
            "mean recall 0.9316000000000001\n",
            "mean f1-score 0.9188615502239774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjd9oYKSmYnf",
        "colab_type": "text"
      },
      "source": [
        "**Ого!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKfxThkmV4L",
        "colab_type": "text"
      },
      "source": [
        "### данные посложнее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cdtm-x74zUA8",
        "colab": {}
      },
      "source": [
        "!wget -q https://github.com/BobaZooba/HSE-Deep-Learning-in-NLP-Course/blob/master/week_05/data/answers_subsample.csv?raw=true -O data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRJMuutO3LXZ",
        "colab_type": "code",
        "outputId": "6bc41034-f28e-4af9-9d9c-91e2a0b9f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "data.columns=['topic', 'text']\n",
        "data[\"topic_cat\"] = data.topic.astype('category').cat.codes\n",
        "data.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>text</th>\n",
              "      <th>topic_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>business</td>\n",
              "      <td>Могут ли в россельхозбанке дать в залог норков...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>law</td>\n",
              "      <td>Может ли срочник перевестись на контракт после...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>business</td>\n",
              "      <td>Продажа недвижимости по ипотеки ? ( арестованы...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>В чем смысл криптовалюты, какая от неё выгода ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>law</td>\n",
              "      <td>часть 1 статья 158 похитил телефон</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      topic                                               text  topic_cat\n",
              "0  business  Могут ли в россельхозбанке дать в залог норков...          0\n",
              "1       law  Может ли срочник перевестись на контракт после...          2\n",
              "2  business  Продажа недвижимости по ипотеки ? ( арестованы...          0\n",
              "3  business  В чем смысл криптовалюты, какая от неё выгода ...          0\n",
              "4       law                 часть 1 статья 158 похитил телефон          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlY4qqwDy-Q9",
        "colab_type": "code",
        "outputId": "ec5fb965-6db9-4e32-fe87-1b2c24520a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "run_experiment(data, 50000, naive_features_tfidf_encoder)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "50000\n",
            "precision 0.7646808063770543\n",
            "recall 0.7598\n",
            "f1-score 0.7578620011727235\n",
            "50000\n",
            "precision 0.758751928639502\n",
            "recall 0.7538666666666667\n",
            "f1-score 0.7518202673490374\n",
            "50000\n",
            "precision 0.7636267117739872\n",
            "recall 0.7576\n",
            "f1-score 0.7551476243253603\n",
            "50000\n",
            "precision 0.7592233581815434\n",
            "recall 0.7536666666666667\n",
            "f1-score 0.751759271917705\n",
            "50000\n",
            "precision 0.7665396748384431\n",
            "recall 0.7607333333333334\n",
            "f1-score 0.75861358429415\n",
            "**********\n",
            "mean precision 0.7625644959621061\n",
            "mean recall 0.7571333333333333\n",
            "mean f1-score 0.7550405498117952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2neLlkFVzm--",
        "colab_type": "text"
      },
      "source": [
        "### может, нам поможет морфология?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHXUIpYG4Bom",
        "colab_type": "code",
        "outputId": "6d64900d-50cd-46c2-ab97-4c47b4e96d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "russian_stopwords = stopwords.words('russian')\n",
        "\n",
        "tokenization_results = {}\n",
        "\n",
        "def pymorphy_preprocess_tokenize(text):\n",
        "    \n",
        "    text_preprocessed_tokenized = []\n",
        "        \n",
        "    for sentence in ru_sent_tokenize(text):\n",
        "        \n",
        "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
        "        clean_words = [word for word in clean_words if word]\n",
        "        clean_words = [word.lower() for word in clean_words if word]\n",
        "        clean_words = [word for word in clean_words if word not in russian_stopwords]\n",
        "        \n",
        "        clean_lemmas = []\n",
        "        for word in clean_words:\n",
        "            lemma = tokenization_results.get(word, None) \n",
        "            if lemma is None:\n",
        "                lemma = morph_analyzer.parse(word)[0].normal_form\n",
        "            tokenization_results[word] = lemma\n",
        "            clean_lemmas.append(lemma)\n",
        "        \n",
        "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
        "\n",
        "    return text_preprocessed_tokenized\n",
        "\n",
        "pymorphy_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), tokenizer=pymorphy_preprocess_tokenize)\n",
        "def pymorphy_features_tfidf_encoder(text_col):\n",
        "    return pymorphy_tfidf_vectorizer.fit_transform(text_col)\n",
        "\n",
        "run_experiment(data, 50000, pymorphy_features_tfidf_encoder)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "50000\n",
            "precision 0.8012186882590834\n",
            "recall 0.7970666666666667\n",
            "f1-score 0.7968329866383274\n",
            "50000\n",
            "precision 0.8027384268036231\n",
            "recall 0.7992\n",
            "f1-score 0.7985790636422943\n",
            "50000\n",
            "precision 0.7977775029844452\n",
            "recall 0.7941333333333334\n",
            "f1-score 0.7930188030910998\n",
            "50000\n",
            "precision 0.797166170909154\n",
            "recall 0.794\n",
            "f1-score 0.7930872338867725\n",
            "50000\n",
            "precision 0.797412569862217\n",
            "recall 0.7938666666666667\n",
            "f1-score 0.7932143615796704\n",
            "**********\n",
            "mean precision 0.7992626717637046\n",
            "mean recall 0.7956533333333333\n",
            "mean f1-score 0.7949464897676328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3viV_Uwl4-NM",
        "colab_type": "code",
        "outputId": "8cdc7e05-e2b6-4938-bdf5-f079c5013ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "run_experiment(data, 1000, naive_features_tfidf_encoder)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "1000\n",
            "precision 0.5311291927618388\n",
            "recall 0.42333333333333334\n",
            "f1-score 0.39047818615732827\n",
            "1000\n",
            "precision 0.5810634293242989\n",
            "recall 0.38666666666666666\n",
            "f1-score 0.2977149296973708\n",
            "1000\n",
            "precision 0.5367994294567656\n",
            "recall 0.44666666666666666\n",
            "f1-score 0.3862334715244727\n",
            "1000\n",
            "precision 0.46347095554044654\n",
            "recall 0.46\n",
            "f1-score 0.39365359937272576\n",
            "1000\n",
            "precision 0.5123649389825861\n",
            "recall 0.36666666666666664\n",
            "f1-score 0.2954115122207227\n",
            "**********\n",
            "mean precision 0.5249655892131871\n",
            "mean recall 0.4166666666666667\n",
            "mean f1-score 0.352698339794524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWBAsiqlGtU-",
        "colab_type": "code",
        "outputId": "4e646637-c636-41b4-9b14-44dffa7cce13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "run_experiment(data, 1000, pymorphy_features_tfidf_encoder)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237779\n",
            "1000\n",
            "precision 0.647496431598198\n",
            "recall 0.49333333333333335\n",
            "f1-score 0.4519528861679891\n",
            "1000\n",
            "precision 0.6497538086115672\n",
            "recall 0.49666666666666665\n",
            "f1-score 0.4623174508963028\n",
            "1000\n",
            "precision 0.6383625509972038\n",
            "recall 0.47\n",
            "f1-score 0.4309059671908212\n",
            "1000\n",
            "precision 0.6535286749928223\n",
            "recall 0.53\n",
            "f1-score 0.5046924786249203\n",
            "1000\n",
            "precision 0.6608987943003288\n",
            "recall 0.49333333333333335\n",
            "f1-score 0.4672680676851558\n",
            "**********\n",
            "mean precision 0.6500080521000241\n",
            "mean recall 0.4966666666666667\n",
            "mean f1-score 0.4634273701130378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b8yNA-5kQE",
        "colab_type": "text"
      },
      "source": [
        "### rnnmorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5dE8PoQIfCO",
        "colab_type": "code",
        "outputId": "1dcbd10a-51a7-435d-8670-6902dbf5d484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "\n",
        "\n",
        "\n",
        "def rnnmorph_preprocess_tokenize(text):\n",
        "    \n",
        "    text_preprocessed_tokenized = []\n",
        "        \n",
        "    for sentence in ru_sent_tokenize(text):\n",
        "        \n",
        "        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)]\n",
        "        clean_words = [word for word in clean_words if word]\n",
        "        clean_words = [word.lower() for word in clean_words if word]\n",
        "        \n",
        "        clean_lemmas = [analysis.pos + '_' + analysis.normal_form for analysis in predictor.predict(clean_words)]\n",
        "        text_preprocessed_tokenized.extend(clean_lemmas)\n",
        "\n",
        "    return text_preprocessed_tokenized\n",
        "\n",
        "\n",
        "\n",
        "rnnmorph_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), tokenizer=rnnmorph_preprocess_tokenize)\n",
        "def rnnmorph_features_tfidf_encoder(text_col):\n",
        "    return rnnmorph_tfidf_vectorizer.fit_transform(text_col)\n",
        "\n",
        "run_experiment(data, 1000, rnnmorph_features_tfidf_encoder)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "237779\n",
            "1000\n",
            "precision 0.6489297444847184\n",
            "recall 0.5266666666666666\n",
            "f1-score 0.4774705668861379\n",
            "1000\n",
            "precision 0.6196762347568798\n",
            "recall 0.49\n",
            "f1-score 0.4558070024481685\n",
            "1000\n",
            "precision 0.6083589225589225\n",
            "recall 0.5133333333333333\n",
            "f1-score 0.4657759975144933\n",
            "1000\n",
            "precision 0.5786585480407944\n",
            "recall 0.49\n",
            "f1-score 0.4618377244407359\n",
            "1000\n",
            "precision 0.6094785353535354\n",
            "recall 0.56\n",
            "f1-score 0.5400399389844786\n",
            "**********\n",
            "mean precision 0.6130203970389702\n",
            "mean recall 0.5159999999999999\n",
            "mean f1-score 0.4801862460548028\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}