{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в анализ текстов\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Работа с текстовыми переменными\n",
    "\n",
    "## 1.1 Основные функции и методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word you are looking for is \"Hello\".'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'The word you are looking for is \"Hello\".'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll wait you there\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"I'll wait you there\"\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Тройные кавычки.\\nДля строк с переносами.\\nРусский язык поддерживается из коробки.\\nКак и любой другой'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = '''Тройные кавычки.\n",
    "Для строк с переносами.\n",
    "Русский язык поддерживается из коробки.\n",
    "Как и любой другой'''\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And also you can split them in pieces'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = \"And also\" \" you can \" \"split them in pieces\"\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- спецсимволы **\\n** - перенос  **\\t** - табуляция  **\\r** - возврат каретки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\tsurname\n",
      "Mikey\tMouse\n"
     ]
    }
   ],
   "source": [
    "a = \"name\\tsurname\\nMikey\\tMouse\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Строки можно сравнивать между собой\n",
    "'a' < 'b' , 'test' < 'Hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# не всё так очевидно...\n",
    "'ёжик' < 'медвежонок'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/', '8', 'z', 'ж', 'и', 'к', 'я', 'ё']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted('ёжикz8я/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ВСЕМ ПРИВЕТ!', 'всем привет!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смена регистра\n",
    "hi = 'Всем привет!'\n",
    "hi.upper(), hi.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Всем Привет!', 'вСЕМ ПРИВЕТ!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi.title(), hi.swapcase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Поиск по строкам\n",
    "tea = 'Съешь ещё этих мягких французских булок, да выпей же чаю!'\n",
    "print(tea.count('е')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(tea.rfind('чаю')) ## or tea.find tea.rfind tea.rindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'чаю!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tea[53:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# предикаты\n",
    "\"Телевизор\".endswith(\"визор\")   # also : startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"16E45\".isalnum(), \"16\".isdigit(), \"test\".islower(), \"q\".isalpha(), \"Test Me\".istitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tNAME\tSURNAME\tCITY\tREGION\tAGE\tWEALTH\tREGISTERED\n",
      "['ID', 'NAME', 'SURNAME', 'CITY', 'REGION', 'AGE', 'WEALTH', 'REGISTERED']\n",
      "ID~NAME~SURNAME~CITY~REGION~AGE~WEALTH~REGISTERED\n"
     ]
    }
   ],
   "source": [
    "# split и join \n",
    "\n",
    "a = \"ID\\tNAME\\tSURNAME\\tCITY\\tREGION\\tAGE\\tWEALTH\\tREGISTERED\"\n",
    "print(a)\n",
    "print(a.split())\n",
    "print('~'.join(a.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke Skywalker~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~Luke Skywalker\n",
      "~~~~~~~~~~~~~Luke Skywalker~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# Выравнивание \n",
    "a = \"Luke Skywalker\"\n",
    "print(a.ljust(40, '~'))\n",
    "print(a.rjust(40, '~'))\n",
    "print(a.center(40, '~'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попрубуем избавиться от пунктуации! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В модуле string есть всякие приятные мелочи :3 \n",
    "import string  \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('!')  # функция ord выдаёт номер символа из ASCII таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_translate = {ord(p): None for p in string.punctuation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Паша  Машин папаша Азазаза азазза'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# метод translate позволяет менять одни символы на другие\n",
    "a = 'Паша - Машин папаша! Азазаза, азазза...'\n",
    "a.translate(dct_translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cпособ проще (но это неточно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'апаша!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'папаша!'.strip('п')   # метод strip удаляет символы в начале и конце строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'папаша'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'папаша!'.strip(string.punctuation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Паша', '', 'Машин', 'папаша', 'Азазаза', 'азазза']\n"
     ]
    }
   ],
   "source": [
    "clean_words = [w.strip(string.punctuation) for w in a.split(' ')]\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Паша - Машин папаша! Азазаза, азазза'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.strip(string.punctuation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Регулярные выражения \n",
    "\n",
    "По мотивам [вот этой статьи.](https://tproger.ru/translations/regular-expression-python/)  Прочитайте её. Она прекрасна! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AV', 0, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Метод ищет по заданному шаблону в начале строки.\n",
    "text = 'AV Analytics Vidhya AV'\n",
    "a = re.match('AV', text)\n",
    "a.group(), a.start(), a.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "y = re.match('Analytics', text)\n",
    "print(y) # в начале строки AV поэтому None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics\n",
      "3\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Метод ищет не только в начале строки, а везде. Возвращает первое найденное совпадение.\n",
    "x = re.search('Analytics', text)\n",
    "print(x.group())\n",
    "print(x.start())\n",
    "print(x.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AV', 'AV']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возвращает список из всех найденных совпадений.\n",
    "x = re.findall('AV', text)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An', 'lyt', 'cs V', 'dhy', '']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сплит по указанной закономерности\n",
    "re.split('i|a', 'Analytics Vidhya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An', 'lyt', 'cs Vidhya']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('i|a', 'Analytics Vidhya', maxsplit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AV is largest Analytics community of the World'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ищет шаблон в строке и заменяет его.\n",
    "re.sub('India', 'the World', 'AV is largest Analytics community of India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Специальные символы \n",
    "\n",
    "\n",
    "|символ|описание|\n",
    "|------|--------|\n",
    "|. | один любой символ, кроме новой строки \\n |\n",
    "|? | 0 или 1 вхождение шаблона слева |\n",
    "|+ | 1 и более вхождений шаблона слева|\n",
    "|* | 0 и более вхождений шаблона слева|\n",
    "|\\w| любая цифра или буква |\n",
    "|\\W| всё, кроме цифры или буквы|\n",
    "|\\d| любая цифра|\n",
    "|\\D| всё, кроме цифры|\n",
    "|\\s| любой пробельный символ|\n",
    "|\\S| любой непроблеьный символ|\n",
    "|\\b| граница слова|\n",
    "|[..]| Один из символов в скобках|\n",
    "|[^..] | Любой символ, кроме тех что в скобках|\n",
    "|\\ | Экранирование специальных символов (точки плюсы и тп)|\n",
    "|^и$| Начало и конец строки соответственно |\n",
    "|{n,m}| От n до m вхождений |\n",
    "|{,m} | От 0 до m вхождений |\n",
    "| a \\| b | Соответствует a или b |\n",
    "| (  ) | Группирует выражение и возвращает найденный текст| \n",
    "| \\t \\n \\r | табуляция, новая строка, возврат каретки | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем порешать пару задачек на регулярки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'AV is largest Analytics community of India'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Вернуть первые два символа каждого слова "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['la', 'An', 'co', 'In']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('(\\w\\w)\\w+', text)) # каждая пара "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Вернуть список доменов из списка адресов электронной почты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@gmail', '@test', '@analyticsvidhya', '@rest']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'\n",
    "re.findall('@\\w+', email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('@\\w+.\\w+', email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['com', 'in', 'com', 'biz']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('@\\w+.(\\w+)', email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gmail', 'com'), ('test', 'in'), ('analyticsvidhya', 'com'), ('rest', 'biz')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('@(\\w+).(\\w+)', email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Извлечь дату из строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12-05-2007', '11-11-2011', '12-01-2009']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroka = 'Amit 34-3456 12-05-2007, XYZ 56-4532 11-11-2011, ABC 67-8945 12-01-2009'\n",
    "re.findall('\\d+-\\d+-\\d+',stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12-05-2007', '11-11-2011', '12-01-2009']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{2}-\\d{2}-\\d{4}',stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007', '2011', '2009']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{2}-\\d{2}-(\\d{4})',stroka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Отфильтровать все мусорные символы и оставить только русские буквы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Машасъелапарупирогов'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stroka = 'Маша-/ .,съелhttps:а _=пару .пирогов 228'\n",
    "re.sub('[^А-Яа-я]','',stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Маша съела пару пирогов '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^А-Яа-я ]','',stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Маша съела пару пирогов 228'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^А-Яа-я0-9 ]','',stroka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Ещё примеры!\n",
    "\n",
    "**Пример 1:** автозамена гомоглифов!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "engChars = [ord(char) for char in u'cCyoOBaAKpPeE']\n",
    "rusChars = [ord(char) for char in u'сСуоОВаАКрРеЕ']\n",
    "\n",
    "translate_eng_rus = dict(zip(engChars, rusChars))\n",
    "translate_rus_eng = dict(zip(rusChars, engChars))\n",
    "\n",
    "def correct_word_chars(w):\n",
    "    \"\"\"\n",
    "        Автозамена русских/английских букв в слове\n",
    "    \"\"\"\n",
    "    if len(re.findall('[а-я]', w)) > len(re.findall('[a-z]', w)):\n",
    "        return w.translate(translate_eng_rus)\n",
    "    else:\n",
    "        return w.translate(translate_rus_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99, 1083, 1086, 1074, 111]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'cловo'\n",
    "[ord(char) for char in a] # коды символов до замены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1089, 1083, 1086, 1074, 1086]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(char) for char in correct_word_chars(a)] # коды символов после замены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_word_chars('tеxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_word_chars('correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'валидно'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_word_chars('валидно')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример 2:** очистка от html-мусора в виде тэгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htmlStrip(text):\n",
    "    \"\"\" \n",
    "        Возвращает текст, очещенный от html тэгов\n",
    "    \"\"\"    \n",
    "    return re.sub('<[^<]+?>', '', str(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<div class=\"wall_post_text\">«Индексы в IntelliJ IDEA зависят только\n",
    "от содержимого одного файла. С одной стороны, это очень удобно. С другой стороны\n",
    ", это накладывает большие ограничения на то, что можно поместить в индекс».<br><br>О\n",
    "том, как в IntelliJ IDEA ищут лямбда-выражения: \n",
    "<a href=\"/away.php?to=http%3A%2F%2Famp.gs%2F4iBG&amp;post=-20629724_1179782&amp;cc_k\n",
    "ey=\" target=\"_blank\" rel=\"noopener\">#разработка@habr</a></div> \\n\\n\\n\\n\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'«Индексы в IntelliJ IDEA зависят только\\nот содержимого одного файла. С одной стороны, это очень удобно. С другой стороны\\n, это накладывает большие ограничения на то, что можно поместить в индекс».О\\nтом, как в IntelliJ IDEA ищут лямбда-выражения: \\n#разработка@habr'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmlStrip(text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Предобработка текста\n",
    "\n",
    "\n",
    "1. **Токенизация** — самый первый шаг при обработке текста. \n",
    "2. **Нормализация** — приведение к одному регистру, удаляются пунктуации, исправление опечаток и т.д.\n",
    "3. \n",
    "    * **Стемминг** —  выделение псевдоосновы слова.\n",
    "    * **Лемматизация** — приведение слов к словарной (\"начальной\") форме.\n",
    "4. **Удаление стоп-слов** — слов, которые не несут никакой смысловой нагрузки (предлоги, союзы и т.п.) Список зависит от задачи!\n",
    "5. **Part-of-Speech tagging (морфологическая разметка)** — приписывание частеречного тега или цепочки грамматических тегов (полный грамматический разбор) токену.\n",
    "\n",
    "**NB!** Не всегда нужны все этапы, все зависит от задачи!\n",
    "\n",
    "### NLP-библиотеки\n",
    "\n",
    "NLP-библиотеки для питона:\n",
    "* Natural Language Toolkit (NLTK)\n",
    "* Apache OpenNLP\n",
    "* Stanford NLP suite\n",
    "* Gate NLP library\n",
    "* Spacy\n",
    "* Yargy\n",
    "* DeepPavlov\n",
    "* CLTK (для древних языков)\n",
    "* и т.д.\n",
    "\n",
    "Самая старая и известная — NLTK. В NLTK есть не только различные инструменты для обработки текста, но и данные — текстовые корпуса, предобученные модели для анализа тональности и морфологической разметки, списки стоп-слов для разных языков и т.п.\n",
    "\n",
    "* [Учебник по NLTK](https://www.nltk.org/book/) от авторов библиотеки и [тьюториалы](https://github.com/hb20007/hands-on-nltk-tutorial) по решению разных задач NLP с помощью NLTK.\n",
    "* [Документация Spacy](https://spacy.io/)\n",
    "* [Документация Yargy](https://yargy.readthedocs.io/)\n",
    "* [Документация DeepPavlov](http://docs.deeppavlov.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1 Токенизация\n",
    "\n",
    "### 2.1.1 Слова и не только\n",
    "\n",
    "\n",
    "Токенизация бывает разной!  Вот [куча примеров](https://www.nltk.org/api/nltk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() # нужно только после первой устанвки, чтобы скачать все состовляющие пакет базы вроде стоп-слов\n",
    "# nltk.download(['punkt', 'stopwords', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is Filipp's text, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Filipp's\", 'text,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# токенайзер по пробелам (банально, да)\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Filipp', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n't  's   и т.п.'\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Filipp', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# учитывает пунктуацию \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Filipp', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сложный токенайзер, пытается учесть кучу разнообразных вещей\n",
    "# На вход в каждой строке отдельное предложение\n",
    "# Регулярка, которая лежит внутри есть вот тут: \n",
    "# https://www.nltk.org/_modules/nltk/tokenize/toktok.html\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@remy',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# специальный токенизатор для твитов\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tweet = \"@remy This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# токенизатор на регулярных выражениях\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "s = \"Good muffins cost $3.88 in New York.  Please buy me two of them. \\n\\nThanks.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет под русские тексты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rusenttokenize in c:\\users\\roma\\anaconda3\\lib\\site-packages (0.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rusenttokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Знаете, Воробьянинов...\n",
      "Вот этот стул напоминает мне нашу жизнь.\n",
      "Мы тоже плывём по течению...\n",
      "...\n",
      "Если не считать уголовного розыска... хотя, и он нас не любит.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "\n",
    "data = \"\"\"Знаете, Воробьянинов... Вот этот стул напоминает мне нашу жизнь. Мы тоже плывём по течению... \n",
    "... Если не считать уголовного розыска... хотя, и он нас не любит. \"\"\"\n",
    "\n",
    "for item in ru_sent_tokenize(data):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Знаете, Воробьянинов... Вот этот стул напоминает мне нашу жизнь.\n",
      "Мы тоже плывём по течению... \n",
      "... Если не считать уголовного розыска... хотя, и он нас не любит.\n"
     ]
    }
   ],
   "source": [
    "for item in sent_tokenize(data):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Стоп-слова\n",
    "\n",
    "**Стоп-слова** — высокочастотные слова, которые не дают нам никакой информации о конкретном тексте. Они составляют верхушку частотного списка в любом языке. Набор стоп-слов не универсален, он будет зависеть от вашей задачи!\n",
    "\n",
    "В NLTK есть готовые списки стоп-слов для многих языков. Хороший стписок стоп-слов есть у яндекс.директа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ru = stopwords.words('russian') \n",
    "stopwords_ru[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english') \n",
    "stopwords_en[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_full = frozenset(stopwords_ru + stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'по' in stopwords_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список языков со стоп-словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смотрим, какие языки есть\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Нормализация \n",
    "\n",
    "### 2.3.1  Стемминг\n",
    "\n",
    "**Стемминг** — отсечение от слова окончаний и суффиксов, чтобы оставшаяся часть, называемая stem, была одинаковой для всех грамматических форм слова. Стем необязательно совпадает с морфлогической основой слова. Одинаковый стем может получиться и не у однокоренных слов и наоборот — в этом проблема стемминга. \n",
    "\n",
    "* 1-ый вид ошибки: белый, белка, белье $\\implies$  бел\n",
    "\n",
    "* 2-ой вид ошибки: трудность, трудный $\\implies$  трудност, труд \n",
    "\n",
    "* 3-ий вид ошибки: быстрый, быстрее $\\implies$  быст, побыстрее $\\implies$  побыст\n",
    "\n",
    "Самый простой алгоритм, алгоритм Портера, состоит из 5 циклов команд, на каждом цикле – операция удаления / замены суффикса. Возможны вероятностные расширения алгоритма.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feet', 'cats', 'wolves', 'talked']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"feet cats wolves talked\"\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet cat wolv talk'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стеммер Портера работает только с английским языком. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball stemmer\n",
    "Улучшенный вариант стеммера Портера; в отличие от него умеет работать не только с английским текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer.languages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Филипп', 'пошёл', 'в', 'Авеньон', 'и', 'пленил', 'пап', '!']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Филипп пошёл в Авеньон и пленил пап!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'филипп пошел в авеньон и плен пап !'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Лемматизация\n",
    "\n",
    "**Лемматизация** — процесс приведения словоформы к лемме, т.е. нормальной (словарной) форме. Это более сложная задача, чем стемминг, но и результаты дает гораздо более осмысленные, особенно для языков с богатой морфологией.\n",
    "\n",
    "* кошке, кошку, кошкам, кошкой $\\implies$ кошка\n",
    "* бежал, бежит, бегу $\\implies$  бежать\n",
    "* белому, белым, белыми $\\implies$ белый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"feet cats wolves talked\"\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot cat wolf talked'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(lemmatizator.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pymorphy2\n",
    "\n",
    "**pymorphy2** — это полноценный морфологический анализатор, целиком написанный на Python. Он также умеет ставить слова в нужную форму (спрягать и склонять).\n",
    "\n",
    "[Документация pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет не просто занимается лемматизацией, а полноценным морфологическим анализом слова. К задачам морфологического анализа относятся, например: \n",
    "\n",
    "* Разбор слова — определение нормальной формы (леммы), основы (стема) и грамматических характеристик слова\n",
    "* Синтез словоформы — генерация словоформы по заданным грамматическим характеристикам из леммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'филипп пойти в авеньон и пленить папа !'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "text = \"Филипп пошёл в Авеньон и пленил пап!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "\" \".join(morph.normal_forms(token)[0] for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('стали')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово: стали\n",
      "Тэг: VERB,perf,intr plur,past,indc\n",
      "Лемма: стать\n",
      "Вероятность: 0.984662\n"
     ]
    }
   ],
   "source": [
    "first = p[0]  # первый разбор\n",
    "print('Слово:', first.word)\n",
    "print('Тэг:', first.tag)\n",
    "print('Лемма:', first.normal_form)\n",
    "print('Вероятность:', first.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из каждого тега можно достать более дробную информацию. Если граммема есть в разборе, то вернется ее значение, если ее нет, то вернется None. [Список граммем](https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "first.normalized        # лемма\n",
    "first.tag.POS           # Part of Speech, часть речи\n",
    "first.tag.animacy       # одушевленность\n",
    "first.tag.aspect        # вид: совершенный или несовершенный\n",
    "first.tag.case          # падеж\n",
    "first.tag.gender        # род (мужской, женский, средний)\n",
    "first.tag.involvement   # включенность говорящего в действие\n",
    "first.tag.mood          # наклонение (повелительное, изъявительное)\n",
    "first.tag.number        # число (единственное, множественное)\n",
    "first.tag.person        # лицо (1, 2, 3)\n",
    "first.tag.tense         # время (настоящее, прошедшее, будущее)\n",
    "first.tag.transitivity  # переходность (переходный, непереходный)\n",
    "first.tag.voice         # залог (действительный, страдательный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n",
      "perf\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(first.tag.POS)\n",
    "print(first.tag.aspect)\n",
    "print(first.tag.case)\n",
    "print(first.tag.animacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='бутявковедами', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='бутявковед', score=1.0, methods_stack=((<FakeDictionary>, 'бутявковедами', 52, 10), (<KnownSuffixAnalyzer>, 'едами')))]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пакет делает прогнозы для неизвестных слов\n",
    "morph.parse('бутявковедами')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "['мент', 'мента', 'ментам', 'ментами', 'ментах', 'менте', 'ментов', 'ментом', 'менту', 'менты', 'параш', 'параша', 'парашам', 'парашами', 'парашах', 'параше', 'парашей', 'парашею', 'параши', 'парашу', 'тачек', 'тачка', 'тачкам', 'тачками', 'тачках', 'тачке', 'тачки', 'тачкой', 'тачкою', 'тачку']\n"
     ]
    }
   ],
   "source": [
    "# Если у нас есть какой-то список стоп-слов для фильтрации, его легко можно расширить\n",
    "terms = ['мент', 'тачка', 'параша']\n",
    "\n",
    "unlemm_terms = [ ]\n",
    "for term in terms:\n",
    "    unlemm_terms.extend([item.word for item in morph.parse(term)[0].lexeme])\n",
    "    \n",
    "terms = sorted(set(unlemm_terms))\n",
    "print(len(terms))\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pymystem3\n",
    "\n",
    "**pymystem3** — это питоновская обертка для яндексовского морфологичекого анализатора Mystem. Его можно скачать отдельно и использовать из консоли.\n",
    "\n",
    "* [Документация Mystem](https://tech.yandex.ru/mystem/doc/index-docpage/)\n",
    "* [Документация pymystem3](http://pythonhosted.org/pymystem3/)\n",
    "\n",
    "Инициализируем Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin - путь к `mystem`, если их несколько\n",
    "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
    "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
    "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in c:\\users\\roma\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\roma\\anaconda3\\lib\\site-packages (from pymystem3) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\roma\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\roma\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roma\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\roma\\anaconda3\\lib\\site-packages (from requests->pymystem3) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['филипп', ' ', 'пойти', ' ', 'в', ' ', 'авеньон', ' ', 'и', ' ', 'пленять', ' ', 'папа', '!', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['филипп', 'пойти', 'в', 'авеньон', 'и', 'пленять', 'папа']\n"
     ]
    }
   ],
   "source": [
    "m = Mystem(entire_input=False)\n",
    "lemmas = m.lemmatize(text)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'филипп',\n",
       "    'wt': 0.9999631323,\n",
       "    'gr': 'S,имя,муж,од=им,ед'}],\n",
       "  'text': 'Филипп'},\n",
       " {'analysis': [{'lex': 'пойти', 'wt': 1, 'gr': 'V,сов,нп=прош,ед,изъяв,муж'}],\n",
       "  'text': 'пошёл'},\n",
       " {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'},\n",
       " {'analysis': [{'lex': 'авеньон',\n",
       "    'wt': 0.2270526087,\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,муж,од=им,ед'}],\n",
       "  'text': 'Авеньон'},\n",
       " {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'},\n",
       " {'analysis': [{'lex': 'пленять',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,пе=прош,ед,изъяв,муж,сов'}],\n",
       "  'text': 'пленил'},\n",
       " {'analysis': [{'lex': 'папа',\n",
       "    'wt': 0.9852174551,\n",
       "    'gr': 'S,муж,од=(вин,мн|род,мн|зват,ед)'}],\n",
       "  'text': 'пап'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = m.analyze(text)\n",
    "parsed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Филипп S\n",
      "пошёл V\n",
      "в PR\n",
      "Авеньон S\n",
      "и CONJ\n",
      "пленил V\n",
      "пап S\n"
     ]
    }
   ],
   "source": [
    "# как достать части речи\n",
    "\n",
    "for word in parsed[:20]:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        print(word['text'], pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе это весь классический пайплайн предобработки. Дальше обычно пишут функцию, которая делает всё и сразу. А после начинают учить модели. Однако, если очень сильно хочется, можно попробовать сделать дополнительные итерации очистки. Например, можно дать бой опечаткам. \n",
    "\n",
    "\n",
    "## 2.4 Исправление опечаток\n",
    "\n",
    "Как правило, спеллчекеры основаны на **расстоянии Левенштейна** (редакционное расстояние, edit distance). Это минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую. Модификация этого алгоритма — расстояние Дамерау-Левенштейна — включает также операцию перестановки символов.\n",
    "\n",
    "* [Простейший спеллчекер Норвига](https://norvig.com/spell-correct.html)\n",
    "* [Hunspell](http://hunspell.github.io/)\n",
    "* [JamSpell](https://github.com/bakwc/JamSpell)\n",
    "* [Yandex Speller API](https://tech.yandex.ru/speller/doc/dg/concepts/api-overview-docpage/)\n",
    "* etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Векторизация\n",
    "\n",
    "Последний пункт после предобработки - векторизация текста и построение итоговой матрицы термы-на-документы. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [ \n",
    "    \"good movie\", \"not good movie\", \"did not like\",\n",
    "    \"i like it\", \"good one\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie  like  movie  not\n",
       "0           1     0      1    0\n",
       "1           1     0      1    1\n",
       "2           0     1      0    1\n",
       "3           0     1      0    0\n",
       "4           0     0      0    0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=2, max_df=0.5, ngram_range = (1,2))\n",
    "features = count_vectorizer.fit_transform(texts)\n",
    "\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns = count_vectorizer.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-idf vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range = (1,2))\n",
    "features = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns = tfidf_vectorizer.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "did           0.096428\n",
       "did not       0.096428\n",
       "not like      0.096428\n",
       "not good      0.108445\n",
       "it            0.122838\n",
       "like it       0.122838\n",
       "good one      0.127814\n",
       "one           0.127814\n",
       "not           0.165290\n",
       "like          0.176902\n",
       "good movie    0.209457\n",
       "movie         0.209457\n",
       "good          0.259466\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,2))\n",
    "features = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns = tfidf_vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "df.mean(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.638711</td>\n",
       "      <td>0.769447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.638711</td>\n",
       "      <td>0.769447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       good  good movie  like\n",
       "0  0.638711    0.769447   0.0\n",
       "1  0.638711    0.769447   0.0\n",
       "2  0.000000    0.000000   1.0\n",
       "3  0.000000    0.000000   1.0\n",
       "4  1.000000    0.000000   0.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=3, ngram_range = (1,2))\n",
    "features = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns = tfidf_vectorizer.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и другие векторизаторы. О них мы ещё поговорим в будущем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Немного про частотность\n",
    "\n",
    "**Частотный словарь русского языка**, составленный на основе [НКРЯ](http://ruscorpora.ru/search-main.html) О.Н. Ляшевской и С.А. Шаровым, можно найти [вот тут](http://dict.ruslang.ru/freq.php).\n",
    "\n",
    "\n",
    "## 4.1 Закон Ципфа\n",
    "\n",
    "**Закон Ципфа** («ранг—частота») — эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (т.н. рангу этого слова). Например, второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и т.д.\n",
    "\n",
    "$f = \\frac{a}{r}$\n",
    "\n",
    "$f$ – частота типа, $r$  – ранг типа, $a$  – параметр, для славянских языков – около 0.07\n",
    "\n",
    "![zipf](https://i.pics.livejournal.com/eponim2008/17443609/234916/234916_original.jpg)\n",
    "\n",
    "Закон назван именем американского лингвиста Джорджа Ципфа (правда, популяризировал он данную закономерность не для лингвистических данных, а для описания распределения экономических сил и социального статуса). Если закон Ципфа соблюдается — значит, перед вами нормальный текст на естественном языке. Если нет, то что-то с ним не так. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Закон Хипса\n",
    "\n",
    "**Закон Хипса** — эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины. C увеличением длины текста (количества токенов), количество типов увеличивается сдедующим образом:\n",
    "\n",
    "$|V| = K*N^b$\n",
    "\n",
    "* $N$  –  число токенов,\n",
    "* $|V|$  – количество типов в словаре, \n",
    "* $K, b$  –  свободные параметры (определяются эмпирически), обычно $K \\in [10,100], b \\in [0.4, 0.6]$\n",
    "\n",
    "![heaps](http://nordbotten.com/ADM/ADM_book/figures/F4-5_Heaps.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ссылки да авторские права: \n",
    "\n",
    "* Часть примеров, задание к этой паре, а также текст про частотность я позаимствовал у Оксаны Дерезы [отсюда.](https://github.com/ancatmara/data-science-nlp) В её репозитории довольно много разной годноты по NLP. Рекомендую поизучать его. \n",
    "\n",
    "### Корпусная лингвистика\n",
    "\n",
    "Все написано для первокурсников, с картинками и примерами про котиков.\n",
    "\n",
    "* [Про работу с НКРЯ](https://ancatmara.gitbooks.io/digital-literacy-for-sfl/content/seminar-6.html) \n",
    "* [Корпусные приложения](https://ancatmara.gitbooks.io/digital-literacy-for-sfl/content/seminar-7.html): AntConc, Google Ngrams, SketchEngine\n",
    "* [Корпусные приложения](https://github.com/ancatmara/DL-SFL-2019/blob/master/seminar-10.md): Voyant Tools\n",
    "\n",
    "\n",
    "### Python для лингвистов\n",
    "\n",
    "* [Python для гуманитариев](https://github.com/ancatmara/python-for-dh) (с нуля)\n",
    "* [Для 2 курса бакалавров](https://github.com/ancatmara/learnpython2018) программы \"Компьютерная лингвистика\" НИУ ВШЭ\n",
    "* [Общеуниверситетский факультатив по Python](http://math-info.hse.ru/2015-16/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BD%D0%B0_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B5_Python_%D0%B4%D0%BB%D1%8F_%D1%81%D0%B1%D0%BE%D1%80%D0%B0_%D0%B8_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85#.D0.9C.D0.B0.D1.82.D0.B5.D1.80.D0.B8.D0.B0.D0.BB.D1.8B) от Ильи Щурова (НИУ ВШЭ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "Скачать текст [\"Капитанской дочки\"](https://www.dropbox.com/s/aky2md6724r3yww/%D0%BA%D0%B0%D0%BF%D0%B8%D1%82%D0%B0%D0%BD%D1%81%D0%BA%D0%B0%D1%8F%20%D0%B4%D0%BE%D1%87%D0%BA%D0%B0.txt?dl=0)\n",
    "\n",
    "1. Найти длину текста в токенах и леммах (после удаления пунктуации).\n",
    "2. Проверить, соблюдается ли закон Ципфа и построить диаграмму с 20 самыми частотными словами (после удаления стоп-слов).\n",
    "3. Найти среднюю длину предложения.\n",
    "4. Найти самую частотную часть речи до удаления стоп-слов и после удаления. Постороить графики частотности частей речи. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
