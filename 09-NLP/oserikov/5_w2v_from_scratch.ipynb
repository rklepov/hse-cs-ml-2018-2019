{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "САД NLP 5 | w2v from scratch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oserikov/data-science-nlp/blob/master/5_w2v_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU58jGj86HQe",
        "colab_type": "text"
      },
      "source": [
        "# Skip-gram Word2Vec with numpy from scratch\n",
        "**actually char2vec to make everything small and neat :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP8W2FBX7JTR",
        "colab_type": "text"
      },
      "source": [
        "## Source code\n",
        "imports, math and the model itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mG0EQAb5ZIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gf7mZ5w6GKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    z = x - np.max(x)\n",
        "    sm = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "    return sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OtTzo3m5sQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntermediateComputationLog:\n",
        "    def __init__(self, input_ohe_value, hidden_l_value, output_in_value, prediction):\n",
        "        self.input_value = input_ohe_value\n",
        "        self.hidden_l_value = hidden_l_value\n",
        "        self.output_in_value = output_in_value\n",
        "        self.prediction = prediction\n",
        "\n",
        "\n",
        "class W2vSkipgramModel:\n",
        "    def __init__(self, vocab_size, hidden_size, learning_rate):\n",
        "        # weight matrices are of inverted shape\n",
        "        # to ease the computations treating input, output\n",
        "        # and intermediate layer as single column vectors\n",
        "        # i.e input->hidden is W_ih * input_v\n",
        "        #     results in column vector hidden_v\n",
        "        #     of shape (hidden_size, 1)\n",
        "        self.W_ih = np.random.randn(hidden_size, vocab_size)\n",
        "        self.W_ho = np.random.randn(vocab_size, hidden_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.w2ix = dict()\n",
        "        self.ix2w = dict()\n",
        "\n",
        "    def one_hot_encode_word(self, word: str):\n",
        "        word_ohe = np.zeros((len(self.w2ix.keys()), 1))\n",
        "        word_ohe[self.w2ix[word]] = 1\n",
        "        return word_ohe\n",
        "\n",
        "    def one_hot_decode_word(self, word_ohe):\n",
        "        word_idx = np.where(word_ohe == 1)\n",
        "        return self.ix2w[word_idx]\n",
        "\n",
        "    def forward(self, input_word_ohe):\n",
        "\n",
        "        hidden_value = self.W_ih @ input_word_ohe\n",
        "        output_in_value = self.W_ho @ hidden_value\n",
        "\n",
        "        prediction = softmax(output_in_value)\n",
        "\n",
        "        intermediate_computations_log = IntermediateComputationLog(\n",
        "            input_ohe_value=input_word_ohe,\n",
        "            hidden_l_value=hidden_value,\n",
        "            output_in_value=output_in_value,\n",
        "            prediction=prediction\n",
        "        )\n",
        "\n",
        "        return intermediate_computations_log, prediction\n",
        "\n",
        "    def backward(self,\n",
        "                 intermediate_computations_log: IntermediateComputationLog,\n",
        "                 context_words_ohes):\n",
        "\n",
        "        # just pull out the used log fields to narrow code lines down\n",
        "        input_value = intermediate_computations_log.input_value\n",
        "        prediction = intermediate_computations_log.prediction\n",
        "\n",
        "        dL_dOutputIn = np.mean([prediction - cw_ohe for cw_ohe in context_words_ohes],\n",
        "                               axis=0)\n",
        "\n",
        "        dL_dWho = dL_dOutputIn @ (self.W_ih @ input_value).T\n",
        "\n",
        "        dL_dWih = np.outer(self.W_ho.T @ dL_dOutputIn, input_value)\n",
        "\n",
        "        self.W_ih -= self.learning_rate * dL_dWih\n",
        "        self.W_ho -= self.learning_rate * dL_dWho\n",
        "\n",
        "    def train(self, sentences: List[List[str]], context_window_size, epochs_num):\n",
        "        vocab = {word for sentence in sentences for word in sentence}\n",
        "        for idx, word in enumerate(vocab):\n",
        "            self.w2ix[word] = idx\n",
        "            self.ix2w[idx] = word\n",
        "\n",
        "        training_set = []\n",
        "        for sentence in sentences:\n",
        "            for word_num, word in enumerate(sentence):\n",
        "                context_window_left_bound = max(0, word_num - context_window_size)\n",
        "                context_window_right_bound = min(word_num + context_window_size,\n",
        "                                                 len(sentence))\n",
        "\n",
        "                left_context_word_nums = list(range(context_window_left_bound,\n",
        "                                                    word_num))\n",
        "                right_context_word_nums = list(range(word_num + 1,\n",
        "                                                     context_window_right_bound))\n",
        "\n",
        "                context_word_nums = left_context_word_nums + right_context_word_nums\n",
        "                context_ws_ohes = [sentence[w_num] for w_num in context_word_nums]\n",
        "\n",
        "                word_ohe = self.one_hot_encode_word(word)\n",
        "                context_words_ohes = [self.one_hot_encode_word(context_word)\n",
        "                                      for context_word in context_ws_ohes]\n",
        "\n",
        "                training_set.append((word_ohe, context_words_ohes))\n",
        "\n",
        "        # training set is a list of tuples \n",
        "        #   formed of one-hot encoded words w, \n",
        "        #   and their context words cw1, .., cwn\n",
        "        # like this: \n",
        "        # training_set = [(OHE(w1), [OHE(cw11), OHE(cw12), ..]),\n",
        "        #                 ..,\n",
        "        #                 (OHE(wn), [OHE(cwn1), OHE(cwn2), ..])]\n",
        "\n",
        "        for epoch_num in range(epochs_num):\n",
        "            epoch_mean_losses = []\n",
        "            for input_w_ohe, context_ws_ohes in training_set:\n",
        "                fw_computations_log, prediction = self.forward(input_w_ohe)\n",
        "\n",
        "                losses = [self.calc_loss(prediction, context_w_ohe)\n",
        "                          for context_w_ohe in context_ws_ohes]\n",
        "                epoch_mean_losses.append(np.mean(losses))\n",
        "\n",
        "                self.backward(fw_computations_log, context_ws_ohes)\n",
        "\n",
        "            yield np.mean(epoch_mean_losses)\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_loss(prediction, context_w_ohe):\n",
        "        correct_word_ohe_idx = np.where(context_w_ohe == 1)\n",
        "        loss = -np.log(prediction[correct_word_ohe_idx])\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY-ePsNO8Skh",
        "colab_type": "text"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21q-CFB15utg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/oserikov/nn_harmony_np/master/data/tur_apertium_words.txt\n",
        "\n",
        "sentences = [[c for c in line.strip()]\n",
        "             for line in open(\"tur_apertium_words.txt\", encoding=\"utf-8\")]\n",
        "\n",
        "vocab = {c for sentence in sentences for c in sentence}\n",
        "w2vmodel = W2vSkipgramModel(len(vocab), 10, 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhcdKgWd58KA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for loss_v in w2vmodel.train(sentences, 2, 10):\n",
        "    print(loss_v)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
