{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Порождающая RNN\n",
    "\n",
    "Эта тетрадка основана на 6 главе Николенко. Пришло время обучить на каком-нибудь корпусе текстов порождающую нейронную сетку. Для того, чтобы делать это, нужно переработать тексты в приемлимые для работы датасеты. Идея, находящаяся под капотом очень проста: __давайте будем порождать текст буква за буквой,__ рассматривая его как поток символов. Тогда задача очень легко формализуется. Нужно предсказать следущий символ по текущему, то есть на каждом шаге нужно по накопленной истории решать задачу классификации на десяток классов.\n",
    "\n",
    "Вопрос в том, как для такой модели определить тренировочные данные.\n",
    "\n",
    "* __Первый, самый простой вариант__ - взять текст и разбить его на последовательности фиксированной длинны (окна) и предсказывать следущий по предыдущим. В такой ситуации начало и конец тренировочного примера будут довольно \"внезапными\", и артефакты по краям будут мешать генерировать на выходе хороший текст.\n",
    "* __Второй вариант__ - дробить текст на последовательности разной длины, но не слишком большой. Например, на предложения. Затем мы добавим спецсимволы начала и конца предложения и дополним каждую строчку до ммаксимума спецсимволом, означающим пустоту.\n",
    "* __Третий вариант__ - нарезать текст на последовательности примерно одной длины, но при этом правильным образом инициализировать скрытые состояния сетки. Для этого нужно взять наш длинный-длинный вход и превратить его сначала в достаточно широкий прямоугольник размера $N \\times L$, где $L$ - длина каждого тренировочного примера, а $N$ - число тренировочных примеров в исходной последовательности. Пока что $L$ очень велика. Но мы можем разрезать прямоугольник на более маленькие батчи по вертикали. Получиться, что каждый новый батч - продолжение предыдущего и его нужно обучать не с нуля, а с того скрытого состояния, на котором закончился предыдущий батч. \n",
    "\n",
    "Мы будем использовать второй вариант и просто дробить текст на предложения. \n",
    "\n",
    "## 1. Предобработка выборки\n",
    "\n",
    "Будем строить модельна примери Евгения Онегина. Считаем данные, определим три специальных символа: `START_CHAR` будет подставляться перед началом предложеия, `END_CHAR` после его конца, а `PADDING_CHAR` будет заполнять остаток предложения до максимума длины. Для простоты мы также не будетм различать строчные и заглавные буквы, а просто пропустим каждую строчку через `lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_CHAR = '\\b'\n",
    "END_CHAR = '\\t'\n",
    "PADDING_CHAR = '\\a'\n",
    "\n",
    "chars = set([START_CHAR, '\\n', END_CHAR])\n",
    "\n",
    "with open('onegin.txt') as f:\n",
    "    for line in f:\n",
    "        chars.update(list(line.strip().lower()))\n",
    "    \n",
    "# заведём словари с отображением символов в числа и обратно    \n",
    "char_indices = { c : i for i,c in enumerate(sorted(list(chars))) }\n",
    "char_indices[PADDING_CHAR] = 0\n",
    "indices_to_chars = { i : c for c,i in char_indices.items() }\n",
    "num_chars = len(chars)\n",
    "\n",
    "num_chars # уникальные символы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим векторные представления для символов. Это будет просто OHE представления, в котором каждому символу соответствует вектор с одной единицей, за исключением PADDING_CHAR, который будет представляться просто нулевым вектором. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_one(i, sz):\n",
    "    res = np.zeros(sz)\n",
    "    res[i] = 1\n",
    "    return res\n",
    "\n",
    "char_vectors = {\n",
    "    c : (np.zeros(num_chars) if c == PADDING_CHAR else get_one(v, num_chars))\n",
    "    for c,v in char_indices.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь считаем исходный файл ещё разок. На этот раз разобьём его на предложения. Будем брать только предложения длиннее 10 символов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'не мысля гордый свет забавить,\\nвниманье дружбы возлюбя,\\nхотел бы я тебе представить\\nзалог достойнее тебя,\\nдостойнее души прекрасной,\\nсвятой исполненной мечты,\\nпоэзии живой и ясной,\\nвысоких дум и простоты;\\nно так и быть — рукой пристрастной\\nприми собранье пестрых глав,\\nполусмешных, полупечальных,\\nпростонародных, идеальных,\\nнебрежный плод моих забав,\\nбессониц, легких вдохновений,\\nнезрелых и увядших лет,\\nума холодных наблюдений\\nи сердца горестных замет.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_end_markers = set('.!?')\n",
    "\n",
    "sentences = [ ]\n",
    "current_sentence = ''\n",
    "with open('onegin.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        s = line.strip().lower()\n",
    "        if len(s) > 0:\n",
    "            current_sentence += s + '\\n'\n",
    "        if len(s) == 0 or s[-1] in sentence_end_markers:\n",
    "            current_sentence = current_sentence.strip()\n",
    "            if len(current_sentence) > 10:\n",
    "                sentences.append(current_sentence)\n",
    "            current_sentence = ''\n",
    "            \n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следущий шаг - векторизация. Давайте определим процедуру, которая превращает набор предложений в два тензора: $X$ содержит векторы символов, а $Y$ результат, который нам нужно предсказать. Это на самом деле ровно тот же тензор $X$, только сдвинутый на один вектор влево: во время $t$ мы предсказываем символ, который будет стоять на месте $t+1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(sentences):\n",
    "    max_sentence_len = np.max([len(x) for x in sentences])\n",
    "    X = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype = np.bool)\n",
    "    Y = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype = np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        char_seq = (START_CHAR + sentence + END_CHAR).ljust(\n",
    "                      max_sentence_len + 1, PADDING_CHAR)\n",
    "        for t in range(max_sentence_len):\n",
    "            X[i, t, :] = char_vectors[char_seq[t]]\n",
    "            Y[i, t, :] = char_vectors[char_seq[t+1]]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экономии памяти и ускорения мы вместо единичек и нулей ставим булевы значеия TRUE и FALSE. Обратите внимание, что мы в начало каждого предложения и в его конец добавили особые символы и дополнили каждое из ни пустотами до длины максимального предложения функцией `ljust`. Наконец мы готовы к строительству нейросетки.\n",
    "\n",
    "## 2. Архитектура\n",
    "\n",
    "Построим простую модель. Добавим один уровень LSTM ячеек, результат работы которых будем пропускать через один полносвязный слой, на котором и будет происходить классификация. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", return_sequences=True, input_shape=(None, 75), units=128)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=75)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(output_dim = 128, activation = 'tanh', \n",
    "               return_sequences = True, input_dim = num_chars))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(output_dim = num_chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура! Архитектура собралась. Прокомментируем её. \n",
    "\n",
    "* Параметр `output_dim = 128` означает, что наш слой состоит из 128 LSTM-ячеек, на этом слое в качестве функции активации используем тангенс. Можно при желании поменять на сигмоиду... \n",
    "* Параметр `input_dim = X.shape[2]` задаёт размерность входа, то есть число символов. В нашем случае это `len(chars)`.\n",
    "* Параметр `return_sequences = True` здесь самый интересный. По умолчанию LSTM-ячейка идёт по входной последовательности, в нашем случае предложению, и на выход выдаёт только самый последний рещультат. Мы бы хотели, чтобы она сохраняла всю последовательность по мере продвижения.\n",
    "\n",
    "Слой `TimeDistributed` тоже является для нас новой штукой. Его фишка в том, что веса полносвязного словя не меняются во времени. Для всех выходов из LSTM всегда используются одни и те же веса. \n",
    "\n",
    "Вся наша собранная модель нарисована на картинке ниже: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель осталось скомпилировать и начать процедуру оптимизации. Как вы помните, рекурентные нейронки страдают проблемой взрыва градиентов. Чтобы этой проблемы избежать, их купируют (обрезают). Для этого используют два параметра: \n",
    "\n",
    "* `clipnorm` будет масштабировать вектор градиента так, чтобы его норма не превысила заданного порога\n",
    "* `clipvalue` будет просто обрезать до заданного порога каждую компоненту градиента по отдельности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(clipnorm = 1.), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы почти готовы к обучению. Остались две маленькие детали. Когда мы писали функцию `get_matrices`, мы оформили её так, что размерность тензоров на выходе зависит от максимальной длины предложений на входе. Мы сделали так специально, чтобы подавать в нашу нейросетку данные батчами. Нужно обернуть функцию `get_matrices` в генератор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим тестовую выборку\n",
    "test_indices = np.random.choice(range(len(sentences)), int(len(sentences) * 0.05))\n",
    "\n",
    "sentences_train = [sentences[x] for x in set(range(len(sentences))) - set(test_indices)]\n",
    "sentences_test = [sentences[x] for x in test_indices]\n",
    "\n",
    "sentences_train = sorted(sentences_train, key = lambda x : len(x))\n",
    "X_test, y_test = get_matrices(sentences_test)\n",
    "\n",
    "batch_size = 16\n",
    "def generate_batch():\n",
    "    while True:\n",
    "        for i in range( int(len(sentences_train)/batch_size)):\n",
    "            sentences_batch = sentences_train[i*batch_size : (i+1)*batch_size]\n",
    "            yield get_matrices(sentences_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь функция `generate_batch` последовательно будет проходить наш датасет, разбивая его на мини-батчи по 16 предложений. А чтобы это имело ещё больше смысла мы предварительно отсортировали `sentences_train` по длине, так что теперь мини-батчи будут иметь последовательно увеличивающуюся ширину, предложения в них будут иметь примерно одинаковую длину и заполнять `PADDING_CHAR` придется не так много остатков. \n",
    "\n",
    "В ходе обучения модели нам часто хочется наблюдать за какими-нибудь её особенностями. В этом обычно помогают специальные функции: функции обратного вызова (callbacks). Если в этот метод передать список функций, то они будут запускаться после каждой эпохи обучения. И мы сможем отслеживать интересующую нас при обучении сетки статистику. Давайте добавим две стандартные функции обратного вызова и напишем одну свою. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу же добавляем свою. Как вы могли бы в с помнить, на самой первой паре мы говорили о Softmax. Мы говорили, что это мягкий максимум. Он при трансформации полученных для каждой категории чисел в вероятности, пытается приподнять самое большое число и приопустить маленькие, мягко выделяя максимум. \n",
    "\n",
    "Силу такого выделения можно контролировать. Для этого в формулу вшивается дополнительный параметр $T$, который называют температурой сэмплирования (см пример на странице 267 Николенко). \n",
    "\n",
    "$$ p(w) \\propto e^{-\\frac{1}{T} x_{w}} $$\n",
    "\n",
    "Если взять $T$ очень большим, то показатели экспоненты окажутся небольшими по модулю и возведения в степень будут близки. На выходе получится близкое к равномерному распределение. Сэмплирование будет довольно случайным. Взвинчивание $T$ позволяет делать сэмплирование конкретным. При $T \\to 0$ распределение будет вырожденным. \n",
    "\n",
    "Напишем колбэк, который будет подставлять в сетку разные значения температуры при генерации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class CharSampler(Callback):\n",
    "    def __init__(self, char_vectors, model):\n",
    "        self.char_vectors = char_vectors\n",
    "        self.model = model \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch = 0\n",
    "        if os.path.isfile('output_file'):\n",
    "            os.remove('output_file')\n",
    "            \n",
    "    def sample(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds)/temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "    \n",
    "    def sample_one(self, T):\n",
    "        result = START_CHAR\n",
    "        while len(result) < 500:\n",
    "            Xsampled = np.zeros((1, len(result), num_chars))\n",
    "            for t,c in enumerate(list(result)):\n",
    "                Xsampled[0, t, :] = self.char_vectors[c]\n",
    "            ysampled = self.model.predict(Xsampled, batch_size=1)[0,:]\n",
    "            yv = ysampled[len(result) - 1, :]\n",
    "            selected_char = indices_to_chars[self.sample(yv, T)]\n",
    "            if selected_char == END_CHAR:\n",
    "                break\n",
    "            result = result + selected_char\n",
    "        return result\n",
    "\n",
    "    def on_epoch_end(self, batch, logs = {}):\n",
    "        self.epoch_end = self.epoch + 1\n",
    "        if self.epoch % 50 == 0:\n",
    "            print(\"\\nEpoch %d text sampling:\" % self.epoch)\n",
    "            with open('output_file', 'a') as outf:\n",
    "                outf.write('\\n======= Epoch %d =======\\n' % self.epoch)\n",
    "                for T in [0.3, 0.5, 0.7, 0.9, 1.1]:\n",
    "                    print('\\tsampling, T = %.1f...' % T)\n",
    "                    for _ in range(5):\n",
    "                        self.model.reset_states()\n",
    "                        res = self.sample_one(T)\n",
    "                        outf.write('\\nT = %.1f\\n%s\\n' % (T, res[1:]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в сетку ещё один колбэк для логирования результатов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "cb_logger = CSVLogger('sim/' + 'onegin' + '.log')\n",
    "cb_sampler = CharSampler(char_vectors, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец мы готовы учить. Для обучения по батчам испольщуем `fit_generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 1136, verbose=True, validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=1000)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1136/1136 [==============================] - 120s 106ms/step - loss: 2.7275 - acc: 0.2185 - val_loss: 0.9167 - val_acc: 0.1019\n",
      "\n",
      "Epoch 0 text sampling:\n",
      "\tsampling, T = 0.3...\n",
      "\tsampling, T = 0.5...\n",
      "\tsampling, T = 0.7...\n",
      "\tsampling, T = 0.9...\n",
      "\tsampling, T = 1.1...\n",
      "Epoch 2/1000\n",
      "1136/1136 [==============================] - 117s 103ms/step - loss: 2.3652 - acc: 0.2812 - val_loss: 0.8689 - val_acc: 0.1139\n",
      "\n",
      "Epoch 0 text sampling:\n",
      "\tsampling, T = 0.3...\n",
      "\tsampling, T = 0.5...\n",
      "\tsampling, T = 0.7...\n",
      "\tsampling, T = 0.9...\n",
      "\tsampling, T = 1.1...\n",
      "Epoch 3/1000\n",
      " 754/1136 [==================>...........] - ETA: 37s - loss: 2.2554 - acc: 0.3108"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4771055e80db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                      callbacks=[cb_logger, cb_sampler] )\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generate_batch(),\n",
    "                     int(len(sentences_train) / batch_size) * batch_size,\n",
    "                     nb_epoch=1000,\n",
    "                     verbose=True, \n",
    "                     validation_data = (X_test, y_test), \n",
    "                     callbacks=[cb_logger, cb_sampler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
