{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __tensorflow__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for tensorflow, but you will find it easy to adapt it to almost any python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3672b1ee10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "To train a neural network policy one must have a neural network policy. Let's build it.\n",
    "\n",
    "\n",
    "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/yet_another_week/_resource/qlearning_scheme.png)\n",
    "\n",
    "For your first run, please only use linear layers (L.Dense) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly. \n",
    "\n",
    "Also please avoid using nonlinearities like sigmoid & tanh: agent's observations are not normalized so sigmoids may become saturated from init.\n",
    "\n",
    "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "L = tf.keras.layers\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               1280      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 67,586\n",
      "Trainable params: 67,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "network.add(L.Dense(256))\n",
    "network.add(L.ReLU())\n",
    "network.add(L.Dense(256))\n",
    "network.add(L.ReLU())\n",
    "network.add(L.Dense(env.action_space.n))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-295311f2c5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"please return just one action (integer)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstate_frequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_frequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-295311f2c5c5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"please return just one action (integer)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstate_frequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_frequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-886518d07698>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1113\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m       mode=mode)\n\u001b[1;32m    216\u001b[0m   \u001b[0;31m# TODO(omalleyt): Handle ProgBar as part of Callbacks once hooks are ready.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m   \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m   \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mget_progbar\u001b[0;34m(model, count_mode)\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0mstateful_metric_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mstateful_metric_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Exclude `loss`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgbarLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful_metric_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mmetrics_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;31m# Add metric names from layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# `CheckpointableBase` manages the `_layers` attributes and does filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# over it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     return checkpointable_layer_utils.filter_empty_layer_containers(\n\u001b[0;32m--> 426\u001b[0;31m         self._layers)\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/layer_utils.py\u001b[0m in \u001b[0;36mis_layer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     28\u001b[0m   return (hasattr(obj, \"call\")\n\u001b[1;32m     29\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"build\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           and hasattr(obj, \"variables\"))\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mvariables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \"\"\"\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \"\"\"\n\u001b[0;32m--> 647\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0mnested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning via gradient descent\n",
    "\n",
    "We shall now train our agent's Q-function by minimizing the TD loss:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable (see lectures).\n",
    "\n",
    "To do so, we shall use `tf.stop_gradient` function which basically says \"consider this thing constant when doingbackprop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "v_of_next_state = tf.stop_gradient(\n",
    "    tf.reduce_max(network(next_states_ph), axis=-1)\n",
    ")\n",
    "\n",
    "v_of_next_state = (1. - tf.to_float(is_done_ph)) * v_of_next_state\n",
    "\n",
    "temporal_difference = (predicted_qvalues_for_actions - (rewards_ph + gamma * v_of_next_state))\n",
    "loss = tf.reduce_mean(temporal_difference ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 13.970\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 13.570\tepsilon = 0.450\n",
      "epoch #2\tmean reward = 12.880\tepsilon = 0.405\n",
      "epoch #3\tmean reward = 13.080\tepsilon = 0.365\n",
      "epoch #4\tmean reward = 12.610\tepsilon = 0.328\n",
      "epoch #5\tmean reward = 14.420\tepsilon = 0.295\n",
      "epoch #6\tmean reward = 14.310\tepsilon = 0.266\n",
      "epoch #7\tmean reward = 23.250\tepsilon = 0.239\n",
      "epoch #8\tmean reward = 30.180\tepsilon = 0.215\n",
      "epoch #9\tmean reward = 55.760\tepsilon = 0.194\n",
      "epoch #10\tmean reward = 40.000\tepsilon = 0.174\n",
      "epoch #11\tmean reward = 90.460\tepsilon = 0.157\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.9\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret results\n",
    "\n",
    "\n",
    "Welcome to the f.. world of deep f...n reinforcement learning. Don't expect agent's reward to smoothly go up. Hope for it to go increase eventually. If it deems you worthy.\n",
    "\n",
    "Seriously though,\n",
    "* __ mean reward__ is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture. \n",
    "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record videos\n",
    "\n",
    "As usual, we now use `gym.wrappers.Monitor` to record a video of our agent playing the game. Unlike our previous attempts with state binarization, this time we expect our agent to act ~~(or fail)~~ more smoothly since there's no more binarization error at play.\n",
    "\n",
    "As you already did with tabular q-learning, we set epsilon=0 for final evaluation to prevent agent from exploring himself to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
