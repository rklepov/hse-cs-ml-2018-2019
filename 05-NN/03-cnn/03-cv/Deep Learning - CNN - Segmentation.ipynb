{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глава 2. Сверточные нейронные сети\n",
    "## Часть 2. Сегментация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство людей в сообществах глубокого обучения и компьютерного зрения понимают, что такое классификация изображений: мы хотим, чтобы наша модель сообщала нам, какой отдельный объект или сцена присутствует на изображении. Классификация очень грубая и на высоком уровне.\n",
    "\n",
    "Многие также знакомы с обнаружением объектов (object detection), где мы пытаемся найти и классифицировать несколько объектов в изображении, рисуя ограничивающие рамки вокруг них, а затем классифицируя то, что находится в этом поле. Обнаружение находится на среднем уровне, где у нас есть довольно полезная и подробная информация, но она все еще немного грубовата, поскольку мы только рисуем ограничивающие рамки и не получаем точного представления о форме объекта.\n",
    "\n",
    "Семантическая сегментация является наиболее информативной из этих трех, где мы хотим классифицировать каждый пиксель на изображении, как вы видите на рисунке выше! За последние несколько лет это было сделано полностью с глубоким изучением.\n",
    "\n",
    "В этом руководстве вы познакомитесь с базовой структурой и принципами работы моделей семантической сегментации, а также со всеми новейшими и лучшими современными методами.\n",
    "\n",
    "Если вы хотите самостоятельно опробовать модели, вы можете ознакомиться с моим Semantic Segmentation Suite, дополненным учебным и тестовым кодом TensorFlow для многих моделей этого руководства!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Базовая структура моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Базовая структура моделей семантической сегментации, которую я собираюсь показать вам, присутствует во всех современных методах! Это позволяет очень легко реализовать разные, так как почти все они имеют одинаковую базовую основу, настройку и поток.\n",
    "\n",
    "* Модель U-Net прекрасно иллюстрирует эту структуру. Левая сторона модели представляет собой любую сеть извлечения признаков, подготовленную для классификации изображений. Это включает в себя сети, такие как **VGGNet, ResNets, DenseNets, MobileNets и NASNets**! Вы действительно можете использовать там все, что захотите.\n",
    "\n",
    "* Главное при выборе вашей сети классификации для извлечения признаков - помнить о компромиссах. Использование очень глубокого **ResNet152** даст вам большую точность, но не будет таким же быстрым, как **MobileNet**. Компромиссы, возникающие при применении этих сетей к классификации, также появляются при использовании их для сегментации. Важно помнить, что эти магистральные сети будут главными факторами при проектировании / выборе вашей сети сегментации, и я не могу этого подчеркнуть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После извлечения карт признаков они затем обрабатываются на **разных масштабах**. Причина этого двоякая. Во-первых, ваша модель, скорее всего, столкнется с объектами разных размеров; обработка карт признаков на разных масштабах даст сети возможность обрабатывать разные характерные размеры на изображениях.\n",
    "\n",
    "Во-вторых, при выполнении сегментации существует компромисс. Если вам нужна хорошая точность классификации, то вы определенно захотите обработать высокоуровневые признаки позже в сети, поскольку они более различимы для разных классов и содержат больше полезной семантической информации. С другой стороны, если вы обрабатываете только эти глубокие высокоуровневые признаки, вы не получите хорошую локализацию из-за **низкого разрешения**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все последние современные методы следовали приведенной выше структуре извлечения признаков с последующей многомасштабной обработкой. Таким образом, многие из них довольно просты в реализации и непрервном обучении. Выбор того, какой из них использовать, будет зависеть от вашей потребности в точности по сравнению со скоростью / наличием памяти, поскольку все пытаются найти новые методы решения этой проблемы, сохраняя при этом эффективность.\n",
    "\n",
    "В следующем пошаговом руководстве о современном уровне развития я собираюсь сосредоточиться на новейших методах, поскольку они будут наиболее полезны большинству читателей после понимания базовой структуры, представленной выше. Мы пройдем по грубому хронологическому порядку, который также примерно соответствует прогрессу современного уровня."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Общая архитектура \n",
    "\n",
    "Общая архитектура семантической сегментации может рассматриваться в широком смысле как последовательнось энкодера и декодера:\n",
    "\n",
    "* Энкодер обычно представляет собой предварительно обученную классификационную сеть, такую как VGG / ResNet, за которой следует сеть декодировщика.\n",
    "\n",
    "* Задача декодера состоит в том, чтобы семантически проецировать распознаваемые признаки (более низкое разрешение), изученные энкодером, на пространство пикселей (c более высокое разрешение), чтобы получить плотную попиксельную классификацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от классификации, где конечный результат очень глубокой сети является единственной важной вещью, семантическая сегментация требует не только различения на уровне пикселей, но также и механизма для проецирования распознавающих признаков, изученных на разных этапах энкодера в пространство пикселей.\n",
    "\n",
    "Различные подходы используют разные механизмы как часть механизма декодирования. Давайте рассмотрим 3 основных подхода:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution\n",
    "(a.k.a. deconvolutions or fractionally strided convolutions, транспонированные свертки)\n",
    "\n",
    "Некоторые источники используют название деконволюция, что неуместно, потому что это не деконволюция. Что еще хуже, деконволюции существуют, но они не распространены в области глубокого обучения. Фактическая деконволюция переворачивает процесс свертки. Представьте, что вы вводите изображение в один сверточный слой. Теперь возьмите вывод, бросьте его в черный ящик и снова получите исходное изображение. Этот черный ящик делает деконволюцию. Это математическая обратная сторона того, что делает сверточный слой.\n",
    "\n",
    "Транспонированная свертка в чем-то похожа, потому что она дает такое же пространственное разрешение, которое мог бы дать гипотетический деконволюционный слой. Однако фактическая математическая операция, выполняемая со значениями, отличается. Транспонированный сверточный слой выполняет регулярную свертку, но восстанавливает свое пространственное преобразование."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg26.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg27.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Транспонированная свертка этого не делает. Единственное, что общего, это то, что она гарантирует, что на выходе также будет изображение 5х5, при этом все еще выполняется обычная операция свертки. Чтобы добиться этого, нам нужно выполнить некоторые необычные дополнения (fancy padding) для ввода.\n",
    "\n",
    "Как вы можете себе представить, этот шаг не изменит процедуру сверху. По крайней мере, не относительно числовых значений.\n",
    "\n",
    "Он просто восстанавливает пространственное разрешение, которое было ранее и выполняет свертку.\n",
    "\n",
    "Это не может быть математическим обратным, но для архитектур Encoder-Decoder это все еще очень полезно. \n",
    "\n",
    "Таким образом, мы можем комбинировать масштабирование изображения (**image upscaling**) со сверткой вместо двух отдельных процессов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Region-based Semantic Segmentation\n",
    "\n",
    "Методы на основе выделения областей обычно следуют пайплайну сегментации с использованием распознавания, который сначала извлекает области произвольной формы из изображения (region-based detection) и описывает их, после чего осуществляет классификацию на основе областей. Во время тестирования предсказания на основе областей преобразуются в предсказания по пикселям, как правило, путем маркировки пикселя в соответствии с областью наивысшей оценки, которая его содержит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg17.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большей конкретики, R-CNN сначала использует выборочный поиск (selective search) для извлечения большого количества предложений объектов, а затем применяет CNN для каждого из них.\n",
    "\n",
    "http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf\n",
    "\n",
    "\n",
    "Наконец, классифицирует каждый регион, используя **линейные модели SVM.**\n",
    "\n",
    "По сравнению с традиционными CNN, которые в основном предназначены для классификации изображений, R-CNN может решать более сложные задачи, такие как обнаружение объектов и сегментация изображений, и даже становится одной из важных основ для обеих областей.  Кроме того, R-CNN может быть построен поверх любых эталонных структур CNN, таких как **AlexNet, VGG, GoogLeNet и ResNet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN\n",
    "\n",
    "Учитывая, что Faster R-CNN так хорошо работает для детектирования объектов, можем ли мы расширить подход для выполнения попиксельной сегментации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Mask R-CNN полностью сверточная сеть (FCN) добавляется поверх карт признаков CNN в Faster R-CNN для генерации маски (вывод сегментации). Обратите внимание, что это происходит параллельно с классификация и регрессия bounding boxes в Faster R-CNN. Источник: https://arxiv.org/abs/1703.06870."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask R-CNN делает это, добавляя ветвь к Faster R-CNN, которая выводит двоичную маску, которая сообщает, является ли данный пиксель частью объекта. Ветвь (на белом изображении выше), как и прежде, является просто полностью сверточной сетью поверх карты объектов на основе CNN. Вот его входы и выходы:\n",
    "\n",
    "* Входы: Карта признаков CNN.\n",
    "* Выходы: Матрица с единицами 1 во всех местах, где пиксель принадлежит объекту, и 0 в других местах (это называется двоичной маской).\n",
    "\n",
    "Но авторам Mask R-CNN пришлось внести небольшую корректировку, чтобы заставить этот пайплайн работать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо RoIPool изображение пропускается через RoIAlign, так что области карты объектов, выбранные RoIPool, более точно соответствуют областям исходного изображения. Это необходимо, потому что сегментация на уровне пикселей требует более детального выравнивания, чем ограничивающие рамки. Источник: https://arxiv.org/abs/1703.06870."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При запуске без изменений в исходной архитектуре Faster R-CNN авторы Mask R-CNN поняли, что области карты признаков, выбранные RoIPool, слегка смещены относительно областей исходного изображения.\n",
    "\n",
    "Поскольку сегментация изображения требует специфичности на уровне пикселей, в отличие от ограничивающих рамок, это, естественно, приводит к существенным неточностям.\n",
    "\n",
    "Авторы смогли решить эту проблему, умно настроив RoIPool для более точного выравнивания, используя метод, известный как RoIAlign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg22.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представьте, что у нас есть изображение размером 128х128 и карта признаков размером 25х25. Давайте представим, что нам нужны признаки, соответствующие верхнему левому углу 15x15 пикселей в исходном изображении (см. выше). Как мы можем выбрать эти пиксели на карте признаков?\n",
    "\n",
    "Мы знаем, что каждый пиксель в исходном изображении соответствует ~ 25/128 пикселей в карте объектов. Чтобы выбрать 15 пикселей из исходного изображения, мы просто выбираем 15 * 25/128 ~ = 2,93 пикселей.\n",
    "\n",
    "В RoIPool мы округлили бы это и выбрали 2 пикселя, что приведет к небольшому смещению. Однако в RoIAlign мы избегаем такого округления. Вместо этого мы используем билинейную интерполяцию, чтобы получить точное представление о том, что будет в пикселе 2.93. На высоком уровне это то, что позволяет нам избежать смещений, вызванных RoIPool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как эти маски сгенерированы, Mask R-CNN объединяет их с классификацией и ограничивающими рамками из Faster R-CNN, чтобы создать такие удивительно точные сегментации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fully Convolutional Network-Based Semantic Segmentation\n",
    "\n",
    "Полностью сверточная сеть (FCN) изучает отображение от пикселей к пикселям без извлечения предложений по регионам. \n",
    "\n",
    "Пайплайн FCN является продолжением классического CNN. Основная идея заключается в том, чтобы заставить классический CNN принимать в качестве входных данных изображения произвольного размера. Ограничение CNN - принимать изображения фиксированного размера из-за использования полносвязных слоев, которые являются фиксированными. \n",
    "\n",
    "В отличие от них, FCN имеют только сверточные и субдискретизующие слои пулинга, которые дают им возможность делать прогнозы на входах произвольного размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из проблем в полностью сверточных сетях (FCN) заключается в том, что при распространении через несколько чередующихся сверточных и субдискретизующих слоев разрешение выходных карт признаков снижается.\n",
    "\n",
    "Поэтому прямые прогнозы FCN обычно имеют низкое разрешение, что приводит к относительно нечетким границам объектов. \n",
    "\n",
    "Для решения этой проблемы было предложено множество более продвинутых подходов на основе FCN, в том числе SegNet, DeepLab-CRF и Dilated Convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Weakly Supervised Semantic Segmentation\n",
    "\n",
    "Большинство соответствующих методов в семантической сегментации полагаются на большое количество изображений с пиксельными масками сегментации. Однако ручное комментирование этих масок является довольно трудоемким, разочаровывающим и коммерчески дорогим. Поэтому в последнее время были предложены некоторые слабо контролируемые методы (weakly supervised), предназначенные для выполнения семантической сегментации с использованием аннотированных ограничивающих рамок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg19.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, Boxsup использовал аннотации bounding boxes в качестве контроля (обучения с учителем) для обучения сети и итеративного улучшения (уточнения) оценочных масок для семантической сегментации. \n",
    "\n",
    "Литература\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fully-resolution Resudial Networks (FRRN)\n",
    "\n",
    "Остаточные сети с полным разрешением (FRRN)\n",
    "\n",
    "Модель FRRN является очень наглядным примером многоуровневой технологии обработки. Это достигается с помощью 2 отдельных потоков: **residual stream** и **pooling stream**.\n",
    "\n",
    "Мы хотим обработать семантические признаки для более  **высокой точности классификации**, поэтому FRRN постепенно обрабатывает и сокращает карты признаков в потоке субдискретизации (пулинга). \n",
    "\n",
    "В то же время FRRN обрабатывает карты признаков с полным разрешением в остаточном потоке (residual stream).\n",
    "\n",
    "Таким образом, субдискретизующий поток обрабатывает семантическую информацию высокого уровня (**для высокой точности классификации**), а остаточный поток обрабатывает информацию пикселей низкого уровня (**для высокой точности локализации**)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы обучаем end-to-end сеть, мы не хотим, чтобы эти два потока были полностью отключены. Таким образом, после каждого этапа субдискретизации FRRN выполняет некоторую совместную обработку карт признаков из двух потоков, чтобы **объединить информацию из них.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pyramid Scene Parsing Network (PSPNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully-resolution Residual Network может успешно выполнять многомасштабную обработку данных.\n",
    "\n",
    "Однако выполнение тяжелой обработки на любом масштабе требует больших вычислительных ресурсов. Более того, FRRN выполняет некоторую обработку в полном разрешении, что очень медленно!\n",
    "\n",
    "PSPNet предлагает умный способ обойти это, используя несколько масштабов пулинга. Он начинается со стандартной сети извлечения признаков (ResNet, DenseNet и т. д.) И использует функции третьей понижающей дискретизации для дальнейшей обработки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Литература \n",
    "\n",
    "https://arxiv.org/pdf/1612.01105.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить многомасштабную информацию, PSPNet применяет 4 различные операции Max Pooling с 4 различными размерами окна (window sizes) и шагами (strides).\n",
    "\n",
    "Это эффективно собирает информацию о признаках из 4 различных масштабов без необходимости интенсивной индивидуальной обработки каждого из них! Мы просто делаем легкую свертку для каждой из них после, увеличиваем, чтобы каждая карта объектов имела одинаковое разрешение, и объединяем их все."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вуаля! Мы объединили многомасштабные карты признаков, не применяя много сверток на них!\n",
    "\n",
    "* Все это делается на признаках с более низким разрешением для высокой скорости. \n",
    "\n",
    "* В конце мы масштабируем карту выходной сегментации до желаемого размера, используя билинейную интерполяцию. Этот метод апскейлинга только после того, как вся обработка выполнена, присутствует во многих современных работах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Сто слоистых тирамису (FCDenseNet)\n",
    "\n",
    "Если есть одна удивительная тенденция, вызванная глубоким обучением, это потрясающие названия исследовательских работ! В сотне слоев Tiramisu FCDenseNet (звучит восхитительно!) Используется структура, аналогичная архитектуре U-Net, которую мы видели ранее. Основным вкладом является умное использование плотных связей, аналогичное модели классификации DenseNet.\n",
    "\n",
    "Это действительно подчеркивает сильную тенденцию в компьютерном зрении, где внешний интерфейс извлечения функций является основной основой для успешной работы в любой другой задаче. Таким образом, первое место, где нужно искать повышение точности, - это часто ваш внешний интерфейс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Переосмысление расширяющихся сверток Atrous / dilated convolutions (DeepLabV3)\n",
    "\n",
    "DeepLabV3 и еще один умный способ сделать многомасштабную обработку, на этот раз без увеличения параметров.\n",
    "\n",
    "Эта модель очень легкая. Мы снова начнем с интерфейса извлечения признаков, взяв функции 4-й понижающей дискретизации для дальнейшей обработки. Это разрешение очень низкое (в 16 раз меньше входного), и поэтому было бы здорово, если бы мы могли просто обработать здесь! Сложность в том, что при таком низком разрешении трудно получить хорошую локализацию из-за низкой точности пикселей.\n",
    "\n",
    "Вот в чем заключается основной вклад DeepLabV3, благодаря умному использованию извилистых сверток (atrous convolutions). Регулярные свертки могут обрабатывать только локальную информацию, так как веса всегда находятся рядом друг с другом. Например, в стандартной свертке 3х3 расстояние между одним весом и любым другим составляет всего один шаг / пиксель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg24.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширяющиеся свертки вводят в сверточные слои еще один параметр, называемый степенью расширения. Это определяет интервал между значениями в ядре. Ядро 3x3 с коэффициентом расширения 2 будет иметь то же поле зрения, что и ядро 5x5, используя только 9 параметров.\n",
    "\n",
    "Это обеспечивает более широкое поле зрения при той же вычислительной трудоемкости. Расширенные свертки особенно популярны в сегментации в реальном времени. Используйте их, если вам нужно широкое поле зрения, и вы не можете позволить себе много сверток или более крупных ядер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При свернутых сверлениях мы будем напрямую увеличивать расстояние между сверточными весами, фактически не увеличивая количество взвешиваний в операции. Таким образом, мы все еще используем 3x3 с 9 общими параметрами, мы просто разносим веса, с которыми мы умножаемся дальше друг от друга! Расстояние между каждым весом называется степенью расширения. Диаграмма модели ниже хорошо иллюстрирует эту идею."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы используем низкий коэффициент расширения, мы будем обрабатывать очень локальную / низкомасштабную информацию. \n",
    "\n",
    "Когда мы используем высокую степень расширения, мы обрабатываем более глобальную / крупномасштабную информацию. Таким образом, модель DeepLabV3 сочетает в себе извилистые извилины с различными степенями расширения для сбора информации нескольких масштабов.\n",
    "\n",
    "Техника апскейлинга в конце после всей обработки, как было объяснено для PSPNet, также выполняется здесь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multi-Path Refinement Networks (RefineNet)\n",
    "\n",
    "Ранее мы показали как FRRN способна к непосредственному объединению информации из нескольких разрешений и объединению этой информации воедино. \n",
    "\n",
    "Недостатком было то, что обработка с таким высоким разрешением требовала значительных вычислительных ресурсов, и нам все еще приходилось обрабатывать и комбинировать эти функции с низкими разрешениями!\n",
    "\n",
    "Модель RefineNet говорит, что нам не нужно этого делать. Когда мы запускаем наше входное изображение через сеть извлечения карт признаков, мы, естественно, получаем многомасштабные карты после каждой понижающей дискретизации (downsamling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем RefineNet обрабатывает эти карты признаков с несколькими разрешениями по принципу «снизу вверх», чтобы объединить информацию c нескольких масштабов.\n",
    "\n",
    "Во-первых, каждая карта признаков обрабатывается независимо. Затем, когда мы увеличиваем масштаб, мы объединяем карту объектов с низким разрешением с картой с более высоким разрешением, выполняя некоторую дальнейшую обработку для них обоих вместе. \n",
    "\n",
    "Таким образом, многомасштабные карты объектов обрабатываются как независимо, так и вместе. Весь процесс движется слева направо на диаграмме ниже.\n",
    "\n",
    "Техника апскейлинга в конце после всей обработки, как было объяснено для PSPNet и DeepLabV3, также выполняется здесь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Large Kernel Matters (GCN)\n",
    "Ранее мы видели, как модель DeepLabV3 использовала atrous / dilated convolutions (расширенные свертки) с различной степенью расширения для сбора многомасштабной информации.\n",
    "\n",
    "Сложность в том, что мы можем обрабатывать только один масштаб за раз, а затем комбинировать их позже. Например, atrous convolution с инкрементов 16, например, не очень хорошо справляется с локальной информацией и должна быть позже объединена с информацией из свертки с гораздо меньшим инкрементом для применения в семантической сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в предыдущих вариантах, многомасштабная обработка сначала выполняется отдельно, а затем результаты объединяются.\n",
    "\n",
    "Было бы больше смысла, если бы мы могли получить многомасштабную информацию за один раз.\n",
    "\n",
    "Чтобы сделать это, Глобальная Сверточная Сеть (GCN) умно предлагает использовать большие одномерные ядра вместо квадратных. С квадратными свертками, такими как 3x3, 7x7 и т.д., мы не сможем сделать их слишком большими без огромного снижения скорости и потребления памяти. С другой стороны, одномерные ядра масштабируются гораздо эффективнее, и мы можем сделать их достаточно большими, не сильно замедляя работу сети. В статье дошли до размера 15!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важная вещь, которую вы должны обязательно сделать, это сбалансировать горизонтальную и вертикальную свертки. Кроме того, в статье используются маленькие свертки 3х3 с небольшим количеством фильтров для эффективного уточнения всего, что могут пропустить одномерные свертки.\n",
    "\n",
    "GCN следует тому же стилю, что и предыдущие работы, обрабатывая каждый масштаб из интерфейса извлечения признаков.\n",
    "\n",
    "Из-за эффективности одномерных сверток GCN выполняет обработку на всех масштабах вплоть до полного разрешения, а не остается маленьким и масштабируется (upscaling) впоследствии. \n",
    "\n",
    "Это учитывает постоянное уточнение сегментации по мере увеличения масштаба, вместо возможного bottlenecking, которое может произойти, если мы сохраним более низкое разрешение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 DeepLabV3 +\n",
    "\n",
    "Модель **DeepLabV3+**, как следует из названия, является улучшением DeepLabV3, заимствуя некоторые концептуальные идеи из достижений, которые были до нее. \n",
    "\n",
    "Как мы видели ранее, существует потенциальное узкое место, если мы просто подождем повышения с билинейной интерполяцией в конце сети. Фактически, результат оригинальной модели DeepLabV3 был увеличен до x16 на выходе!\n",
    "\n",
    "Для решения этой проблемы DeepLabV3 + предлагает добавить промежуточный модуль декодера поверх DeepLabV3. \n",
    "\n",
    "После обработки через DeepLabV3 признаки затем подвергаются дискретизации (upsampling) с увеличением x4. Затем они дополнительно обрабатываются вместе с исходными признаками из интерфейса извлечения признаков, а затем снова увеличиваются в 4 раза.\\\n",
    "\n",
    "Это снижает нагрузку на конец сети и обеспечивает быстрый путь (shortcut connection) от внешнего интерфейса извлечения карт признаков до ближайшего конца сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 CVPR и ECCV 2018\n",
    "\n",
    "Сети, которые мы рассмотрели в предыдущем разделе, представляют большую часть техник, которые вам необходимо знать для выполнения семантической сегментации!\n",
    "\n",
    "Многое из того, что было выпущено в этом году на конференциях по компьютерному зрению, было незначительными обновлениями и небольшими изменениями в точности, что не очень важно для начала работы. \n",
    "\n",
    "Для тщательности я предоставляю краткий обзор их вкладов здесь для всех, кто заинтересован!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Image Cascade Network (ICNet)** - управляет входным изображением на разных масштабах, каждый из которых масштабируется через собственную подсеть и постепенно объединяет результаты\n",
    "\n",
    "https://arxiv.org/pdf/1704.08545.pdf\n",
    "\n",
    "**2. Discriminative Feature Network (DFN)** - использует глубокий контроль и пытается обрабатывать гладкие и краевые части сегментов отдельно\n",
    "\n",
    "http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_a_Discriminative_CVPR_2018_paper.pdf\n",
    "\n",
    "**3. DenseASPP** - сочетает в себе плотные соединения (dense connections) с atrous convolutions.\n",
    "\n",
    "http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf\n",
    "\n",
    "**4. Context encoding** - использует глобальный контекст (global context) для повышения точности, добавляя модуль внимания канала (channel attention module), который вызывает внимание на определенных картах признаков, основанных на недавно разработанной функции потерь. Функция потерь основана на ветви сети, которая предсказывает, какие классы присутствуют в изображении (то есть глобальный контекст более высокого уровня).\n",
    "\n",
    "http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf\n",
    "\n",
    "\n",
    "**5. Dense Decoder Shortcut Connections** - используются плотные связи (dense connections) на этапе декодирования для более высокой точности (ранее это делалось только во время извлечения / кодирования признаков)\n",
    "\n",
    "http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf\n",
    "\n",
    "**6. Bilateral Segmentation Network (BiSeNet)** - имеет 2 ветви: одна глубокая - для получения семантической информации, а другая - выполняет очень незначительную / минорную обработку входного изображения, чтобы сохранить низкоуровневую информацию о пикселях\n",
    "\n",
    "https://arxiv.org/pdf/1808.00897v1.pdf\n",
    "\n",
    "**7. ExFuse** - явно комбинирует многомасштабные признаки от внешнего интерфейса трансферлернинга для извелчения признаков до обработки, чтобы гарантировать, что многомасштабная информация обрабатывается вместе на всех уровнях.\n",
    "\n",
    "http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenli_Zhang_ExFuse_Enhancing_Feature_ECCV_2018_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/seg16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Будьте в курсе преимуществ и недостатков модели классификации. Ваша классификационная сеть является вашим основным драйвером для получения признаков из изобразения, и большая часть ваших выигрышей / потерь будет происходить отсюда\n",
    "\n",
    "* Обрабатывайте карты признаков на разных масштабах и объединяйте информацию\n",
    "\n",
    "* Многомасштабный пулинг, atrous convolutions, большие одномерные свертки хороши для семантической сегментации\n",
    "\n",
    "* Вам не нужно много обрабатывать в высоком разрешении. Делайте большую часть при низком разрешении для скорости, затем увеличивайте масштаб (upscaling) и выполняйте более легкую обработку в конце, в случае необходимости\n",
    "\n",
    "* Deep supervision может немного повысить вашу точность (хотя и сложнее настроить обучение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ссылки \n",
    "\n",
    "https://github.com/GeorgeSeif/Semantic-Segmentation-Suite\n",
    "\n",
    "https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
