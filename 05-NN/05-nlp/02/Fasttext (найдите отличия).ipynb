{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8P5Supj85eK"
   },
   "outputs": [],
   "source": [
    "# http://study.mokoron.com/\n",
    "# https://elibrary.ru/item.asp?id=20399632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "vXujw9uL8_W9",
    "outputId": "6e2d7d7c-3273-4205-c926-29a6778b6d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-26 17:37:47--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.1, 2620:100:6030:1::a27d:5001\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
      "--2019-04-26 17:37:48--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc78d5cde0e20ce7938997201bdd.dl.dropboxusercontent.com/cd/0/inline/Afx8d7wGvYs-loSXCtP4DCOgNFIgK5GXzaKSl_ySzZ50yGMtZhRpzlCiLtqUu_cv8Nmui2v9VQ-TBjZGiev6YvRUa6K1PoeUUVoQ_v8JHnj9cgl_0SjZsufNa0W1YTd5XLs/file# [following]\n",
      "--2019-04-26 17:37:48--  https://uc78d5cde0e20ce7938997201bdd.dl.dropboxusercontent.com/cd/0/inline/Afx8d7wGvYs-loSXCtP4DCOgNFIgK5GXzaKSl_ySzZ50yGMtZhRpzlCiLtqUu_cv8Nmui2v9VQ-TBjZGiev6YvRUa6K1PoeUUVoQ_v8JHnj9cgl_0SjZsufNa0W1YTd5XLs/file\n",
      "Resolving uc78d5cde0e20ce7938997201bdd.dl.dropboxusercontent.com (uc78d5cde0e20ce7938997201bdd.dl.dropboxusercontent.com)... 162.125.80.6, 2620:100:6030:6::a27d:5006\n",
      "Connecting to uc78d5cde0e20ce7938997201bdd.dl.dropboxusercontent.com (uc78d5cde0e20ce7938997201bdd.dl.dropboxusercontent.com)|162.125.80.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26233379 (25M) [text/plain]\n",
      "Saving to: ‘positive.csv?dl=0’\n",
      "\n",
      "positive.csv?dl=0   100%[===================>]  25.02M  30.9MB/s    in 0.8s    \n",
      "\n",
      "2019-04-26 17:37:49 (30.9 MB/s) - ‘positive.csv?dl=0’ saved [26233379/26233379]\n",
      "\n",
      "--2019-04-26 17:37:49--  http://positive.csv/\n",
      "Resolving positive.csv (positive.csv)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘positive.csv’\n",
      "FINISHED --2019-04-26 17:37:49--\n",
      "Total wall clock time: 1.7s\n",
      "Downloaded: 1 files, 25M in 0.8s (30.9 MB/s)\n",
      "--2019-04-26 17:37:50--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.1, 2620:100:6030:1::a27d:5001\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
      "--2019-04-26 17:37:51--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc8737605469a2d2282e5f0be11f.dl.dropboxusercontent.com/cd/0/inline/AfzpYfM93Pn3g5NAptOJM5RTMM-Z1LSAMVbv0-k_ZVIFC4Cj6HEiDXpQwNttxFKDTZOLM00NoMaUIftwI3Zat3V6qam3-kGzPjzzt5yEsTne4CcxdkCkKOL9e1fRX-_nRic/file# [following]\n",
      "--2019-04-26 17:37:51--  https://uc8737605469a2d2282e5f0be11f.dl.dropboxusercontent.com/cd/0/inline/AfzpYfM93Pn3g5NAptOJM5RTMM-Z1LSAMVbv0-k_ZVIFC4Cj6HEiDXpQwNttxFKDTZOLM00NoMaUIftwI3Zat3V6qam3-kGzPjzzt5yEsTne4CcxdkCkKOL9e1fRX-_nRic/file\n",
      "Resolving uc8737605469a2d2282e5f0be11f.dl.dropboxusercontent.com (uc8737605469a2d2282e5f0be11f.dl.dropboxusercontent.com)... 162.125.80.6, 2620:100:6030:6::a27d:5006\n",
      "Connecting to uc8737605469a2d2282e5f0be11f.dl.dropboxusercontent.com (uc8737605469a2d2282e5f0be11f.dl.dropboxusercontent.com)|162.125.80.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24450101 (23M) [text/plain]\n",
      "Saving to: ‘negative.csv?dl=0’\n",
      "\n",
      "negative.csv?dl=0   100%[===================>]  23.32M  32.7MB/s    in 0.7s    \n",
      "\n",
      "2019-04-26 17:37:52 (32.7 MB/s) - ‘negative.csv?dl=0’ saved [24450101/24450101]\n",
      "\n",
      "--2019-04-26 17:37:52--  http://negative.csv/\n",
      "Resolving negative.csv (negative.csv)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘negative.csv’\n",
      "FINISHED --2019-04-26 17:37:52--\n",
      "Total wall clock time: 1.5s\n",
      "Downloaded: 1 files, 23M in 0.7s (32.7 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0' positive.csv\n",
    "!wget 'https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0' negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wlM1YPkF83Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBjWD44vFmXD"
   },
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive.csv?dl=0',header=None,encoding='utf-8',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Nlh71mYGffg"
   },
   "outputs": [],
   "source": [
    "negative = pd.read_csv('negative.csv?dl=0',header=None,encoding='utf-8',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xWwL_3GXGp_3",
    "outputId": "115dcc03-a8b2-4368-d9ac-da71cd6fc2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114911, 12), (111923, 12))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.shape,negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-andPdElHbiD"
   },
   "outputs": [],
   "source": [
    "positive=positive[[positive.columns[3]]]\n",
    "negative=negative[[negative.columns[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uaR8rWsG6gL"
   },
   "outputs": [],
   "source": [
    "positive['target']=1\n",
    "negative['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pm-tnaf4HCJ2"
   },
   "outputs": [],
   "source": [
    "positive.columns=['text','target']\n",
    "negative.columns=['text','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "evE48wEQHEZc",
    "outputId": "945f5837-b0ba-4418-e57a-e1b7a41a8b33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  @first_timee хоть я и школота, но поверь, у на...       1\n",
       "1  Да, все-таки он немного похож на него. Но мой ...       1\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...       1\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...       1\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cfm5kOKgHXSW"
   },
   "outputs": [],
   "source": [
    "full_set = pd.concat([positive,negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh5zId7IH_pm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lv5pxDQ6IAkj"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(full_set['text'],full_set.target.values,test_size=0.2,shuffle=True,random_state=2121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_dJbZkEIbIV"
   },
   "outputs": [],
   "source": [
    "clf = make_pipeline(TfidfVectorizer(ngram_range=(1,2)),MultinomialNB())\n",
    "clf.fit(x_train,y_train)\n",
    "preds = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y7HZJLUXIcXj",
    "outputId": "476271e2-896d-408f-ee5a-e3da5fb97d36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7739546366301496"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь опробуем на этих же данных простейшую сеть с предобученными ембедингами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "Mu0ph5ILD1QP",
    "outputId": "01fc7acd-dfa9-4f26-adb4-804e20802a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-410\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following additional packages will be installed:\n",
      "  libc-ares2\n",
      "The following NEW packages will be installed:\n",
      "  aria2 libc-ares2\n",
      "0 upgraded, 2 newly installed, 0 to remove and 6 not upgraded.\n",
      "Need to get 1,274 kB of archives.\n",
      "After this operation, 4,912 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libc-ares2 amd64 1.14.0-1 [37.1 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aria2 amd64 1.33.1-1 [1,236 kB]\n",
      "Fetched 1,274 kB in 2s (545 kB/s)\n",
      "Selecting previously unselected package libc-ares2:amd64.\n",
      "(Reading database ... 131304 files and directories currently installed.)\n",
      "Preparing to unpack .../libc-ares2_1.14.0-1_amd64.deb ...\n",
      "Unpacking libc-ares2:amd64 (1.14.0-1) ...\n",
      "Selecting previously unselected package aria2.\n",
      "Preparing to unpack .../aria2_1.33.1-1_amd64.deb ...\n",
      "Unpacking aria2 (1.33.1-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Setting up libc-ares2:amd64 (1.14.0-1) ...\n",
      "Setting up aria2 (1.33.1-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt install aria2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6749
    },
    "colab_type": "code",
    "id": "vAhJxVFILrFy",
    "outputId": "e1d14a81-262c-4c4a-93cf-efef9fc37f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "04/26 17:38:34 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
      " *** Download Progress Summary as of Fri Apr 26 17:39:35 2019 *** \n",
      "=\n",
      "[#b1d975 136MiB/2.5GiB(5%) CN:1 DL:2.4MiB ETA:16m45s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:40:36 2019 *** \n",
      "=\n",
      "[#b1d975 168MiB/2.5GiB(6%) CN:1 DL:54KiB ETA:12h58m57s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:41:37 2019 *** \n",
      "=\n",
      "[#b1d975 172MiB/2.5GiB(6%) CN:1 DL:60KiB ETA:11h37m45s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:42:37 2019 *** \n",
      "=\n",
      "[#b1d975 176MiB/2.5GiB(6%) CN:1 DL:64KiB ETA:10h54m2s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:43:38 2019 *** \n",
      "=\n",
      "[#b1d975 180MiB/2.5GiB(6%) CN:1 DL:78KiB ETA:8h52m2s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:44:38 2019 *** \n",
      "=\n",
      "[#b1d975 183MiB/2.5GiB(6%) CN:1 DL:56KiB ETA:12h18m2s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:45:38 2019 *** \n",
      "=\n",
      "[#b1d975 186MiB/2.5GiB(7%) CN:1 DL:35KiB ETA:19h39m59s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:46:39 2019 *** \n",
      "=\n",
      "[#b1d975 189MiB/2.5GiB(7%) CN:1 DL:79KiB ETA:8h45m9s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:47:39 2019 *** \n",
      "=\n",
      "[#b1d975 192MiB/2.5GiB(7%) CN:1 DL:52KiB ETA:13h17m10s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:48:40 2019 *** \n",
      "=\n",
      "[#b1d975 195MiB/2.5GiB(7%) CN:1 DL:101KiB ETA:6h49m33s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:49:41 2019 *** \n",
      "=\n",
      "[#b1d975 199MiB/2.5GiB(7%) CN:1 DL:93KiB ETA:7h26m2s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:50:41 2019 *** \n",
      "=\n",
      "[#b1d975 202MiB/2.5GiB(7%) CN:1 DL:54KiB ETA:12h37m38s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:51:42 2019 *** \n",
      "=\n",
      "[#b1d975 204MiB/2.5GiB(7%) CN:1 DL:41KiB ETA:16h36m2s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:52:43 2019 *** \n",
      "=\n",
      "[#b1d975 208MiB/2.5GiB(7%) CN:1 DL:62KiB ETA:10h59m45s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:53:44 2019 *** \n",
      "=\n",
      "[#b1d975 212MiB/2.5GiB(8%) CN:1 DL:55KiB ETA:12h23m7s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:54:44 2019 *** \n",
      "=\n",
      "[#b1d975 217MiB/2.5GiB(8%) CN:1 DL:56KiB ETA:12h14m14s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:55:46 2019 *** \n",
      "=\n",
      "[#b1d975 221MiB/2.5GiB(8%) CN:1 DL:47KiB ETA:14h24m35s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:56:46 2019 *** \n",
      "=\n",
      "[#b1d975 224MiB/2.5GiB(8%) CN:1 DL:71KiB ETA:9h33m2s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:57:46 2019 *** \n",
      "=\n",
      "[#b1d975 227MiB/2.5GiB(8%) CN:1 DL:46KiB ETA:14h53m18s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:58:46 2019 *** \n",
      "=\n",
      "[#b1d975 251MiB/2.5GiB(9%) CN:1 DL:531KiB ETA:1h16m37s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 17:59:47 2019 *** \n",
      "=\n",
      "[#b1d975 308MiB/2.5GiB(11%) CN:1 DL:851KiB ETA:46m42s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:00:47 2019 *** \n",
      "=\n",
      "[#b1d975 335MiB/2.5GiB(12%) CN:1 DL:597KiB ETA:1h5m46s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:01:48 2019 *** \n",
      "=\n",
      "[#b1d975 387MiB/2.5GiB(14%) CN:1 DL:0.9MiB ETA:41m31s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:02:48 2019 *** \n",
      "=\n",
      "[#b1d975 443MiB/2.5GiB(16%) CN:1 DL:1.9MiB ETA:18m37s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:03:49 2019 *** \n",
      "=\n",
      "[#b1d975 485MiB/2.5GiB(18%) CN:1 DL:186KiB ETA:3h17m34s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:04:49 2019 *** \n",
      "=\n",
      "[#b1d975 498MiB/2.5GiB(18%) CN:1 DL:196KiB ETA:3h6m24s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:05:50 2019 *** \n",
      "=\n",
      "[#b1d975 517MiB/2.5GiB(19%) CN:1 DL:178KiB ETA:3h22m46s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:06:51 2019 *** \n",
      "=\n",
      "[#b1d975 545MiB/2.5GiB(20%) CN:1 DL:733KiB ETA:48m43s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:07:52 2019 *** \n",
      "=\n",
      "[#b1d975 644MiB/2.5GiB(24%) CN:1 DL:1.3MiB ETA:24m12s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:08:52 2019 *** \n",
      "=\n",
      "[#b1d975 671MiB/2.5GiB(25%) CN:1 DL:265KiB ETA:2h6m21s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:09:53 2019 *** \n",
      "=\n",
      "[#b1d975 717MiB/2.5GiB(27%) CN:1 DL:1.1MiB ETA:28m41s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:10:53 2019 *** \n",
      "=\n",
      "[#b1d975 771MiB/2.5GiB(29%) CN:1 DL:2.1MiB ETA:14m29s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:11:54 2019 *** \n",
      "=\n",
      "[#b1d975 806MiB/2.5GiB(30%) CN:1 DL:378KiB ETA:1h22m41s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:12:54 2019 *** \n",
      "=\n",
      "[#b1d975 828MiB/2.5GiB(31%) CN:1 DL:499KiB ETA:1h1m51s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:13:55 2019 *** \n",
      "=\n",
      "[#b1d975 861MiB/2.5GiB(32%) CN:1 DL:515KiB ETA:58m53s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:14:56 2019 *** \n",
      "=\n",
      "[#b1d975 891MiB/2.5GiB(33%) CN:1 DL:640KiB ETA:46m32s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:15:56 2019 *** \n",
      "=\n",
      "[#b1d975 0.9GiB/2.5GiB(35%) CN:1 DL:554KiB ETA:52m28s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:16:57 2019 *** \n",
      "=\n",
      "[#b1d975 0.9GiB/2.5GiB(38%) CN:1 DL:624KiB ETA:44m29s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:17:58 2019 *** \n",
      "=\n",
      "[#b1d975 1.0GiB/2.5GiB(39%) CN:1 DL:517KiB ETA:52m49s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:18:58 2019 *** \n",
      "=\n",
      "[#b1d975 1.0GiB/2.5GiB(40%) CN:1 DL:658KiB ETA:40m39s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:19:59 2019 *** \n",
      "=\n",
      "[#b1d975 1.0GiB/2.5GiB(42%) CN:1 DL:508KiB ETA:50m47s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:20:59 2019 *** \n",
      "=\n",
      "[#b1d975 1.1GiB/2.5GiB(43%) CN:1 DL:634KiB ETA:39m54s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:22:00 2019 *** \n",
      "=\n",
      "[#b1d975 1.1GiB/2.5GiB(45%) CN:1 DL:728KiB ETA:33m54s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:23:01 2019 *** \n",
      "=\n",
      "[#b1d975 1.2GiB/2.5GiB(46%) CN:1 DL:565KiB ETA:42m14s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:24:01 2019 *** \n",
      "=\n",
      "[#b1d975 1.2GiB/2.5GiB(48%) CN:1 DL:280KiB ETA:1h23m31s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:25:02 2019 *** \n",
      "=\n",
      "[#b1d975 1.3GiB/2.5GiB(50%) CN:1 DL:1.6MiB ETA:13m5s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:26:03 2019 *** \n",
      "=\n",
      "[#b1d975 1.4GiB/2.5GiB(55%) CN:1 DL:2.6MiB ETA:7m28s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:27:03 2019 *** \n",
      "=\n",
      "[#b1d975 1.4GiB/2.5GiB(57%) CN:1 DL:661KiB ETA:29m5s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:28:03 2019 *** \n",
      "=\n",
      "[#b1d975 1.5GiB/2.5GiB(59%) CN:1 DL:370KiB ETA:49m32s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:29:04 2019 *** \n",
      "=\n",
      "[#b1d975 1.5GiB/2.5GiB(60%) CN:1 DL:252KiB ETA:1h10m51s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:30:04 2019 *** \n",
      "=\n",
      "[#b1d975 1.5GiB/2.5GiB(60%) CN:1 DL:509KiB ETA:34m30s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:31:05 2019 *** \n",
      "=\n",
      "[#b1d975 1.5GiB/2.5GiB(61%) CN:1 DL:279KiB ETA:1h2m17s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:32:05 2019 *** \n",
      "=\n",
      "[#b1d975 1.6GiB/2.5GiB(62%) CN:1 DL:574KiB ETA:29m21s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:33:06 2019 *** \n",
      "=\n",
      "[#b1d975 1.6GiB/2.5GiB(64%) CN:1 DL:0.9MiB ETA:16m12s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:34:07 2019 *** \n",
      "=\n",
      "[#b1d975 1.7GiB/2.5GiB(66%) CN:1 DL:848KiB ETA:17m58s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:35:08 2019 *** \n",
      "=\n",
      "[#b1d975 1.8GiB/2.5GiB(70%) CN:1 DL:840KiB ETA:15m39s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:36:08 2019 *** \n",
      "=\n",
      "[#b1d975 1.8GiB/2.5GiB(72%) CN:1 DL:807KiB ETA:15m21s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:37:08 2019 *** \n",
      "=\n",
      "[#b1d975 1.9GiB/2.5GiB(74%) CN:1 DL:693KiB ETA:16m43s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:38:09 2019 *** \n",
      "=\n",
      "[#b1d975 1.9GiB/2.5GiB(76%) CN:1 DL:1.3MiB ETA:7m51s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:39:09 2019 *** \n",
      "=\n",
      "[#b1d975 2.0GiB/2.5GiB(77%) CN:1 DL:568KiB ETA:17m38s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:40:09 2019 *** \n",
      "=\n",
      "[#b1d975 2.0GiB/2.5GiB(79%) CN:1 DL:524KiB ETA:17m48s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:41:10 2019 *** \n",
      "=\n",
      "[#b1d975 2.0GiB/2.5GiB(81%) CN:1 DL:0.9MiB ETA:8m18s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:42:10 2019 *** \n",
      "=\n",
      "[#b1d975 2.2GiB/2.5GiB(88%) CN:1 DL:3.3MiB ETA:1m28s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      " *** Download Progress Summary as of Fri Apr 26 18:43:11 2019 *** \n",
      "=\n",
      "[#b1d975 2.4GiB/2.5GiB(96%) CN:1 DL:2.3MiB ETA:41s]\n",
      "FILE: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "-\n",
      "\n",
      "\u001b[0m\n",
      "04/26 18:43:53 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "b1d975|\u001b[1;32mOK\u001b[0m  |   689KiB/s|/content/ft_native_300_ru_twitter_nltk_word_tokenize.vec\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    }
   ],
   "source": [
    "!aria2c http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tUzCNKajNGgD",
    "outputId": "c25994d2-a2a8-4f6c-b69c-f76e62f97cb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "5GO0Cd4SOfKW",
    "outputId": "ce3df56f-19ab-449a-849d-e286984387d8"
   },
   "outputs": [],
   "source": [
    "f = open('ft_native_300_ru_twitter_nltk_word_tokenize.vec')\n",
    "embedding_values = {}\n",
    "for line in tqdm_notebook(f):\n",
    "    value = line.replace('\\n','').split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:-1],dtype = 'float32')\n",
    "    embedding_values[word]=coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RgicMRoPClk"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(full_set['text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0ynvmUVIKPY"
   },
   "outputs": [],
   "source": [
    "lower=True\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 100),dtype=np.float32)\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    if lower:\n",
    "        word = word.lower()\n",
    "    try:\n",
    "        embedding_vector = embedding_values[word]\n",
    "    except:\n",
    "        embedding_vector = embedding_values[\"unknown\"]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1174
    },
    "colab_type": "code",
    "id": "tN6om-FHTPmu",
    "outputId": "380d5a09-3bc0-4c0f-a4ff-a72e9007f1e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 181467 samples, validate on 45367 samples\n",
      "Epoch 1/25\n",
      "181467/181467 [==============================] - 89s 488us/step - loss: 0.5562 - acc: 0.7069 - val_loss: 0.5141 - val_acc: 0.7386\n",
      "Epoch 2/25\n",
      "181467/181467 [==============================] - 86s 476us/step - loss: 0.5097 - acc: 0.7405 - val_loss: 0.4803 - val_acc: 0.7612\n",
      "Epoch 3/25\n",
      "181467/181467 [==============================] - 86s 475us/step - loss: 0.4922 - acc: 0.7531 - val_loss: 0.4716 - val_acc: 0.7668\n",
      "Epoch 4/25\n",
      "181467/181467 [==============================] - 88s 484us/step - loss: 0.4827 - acc: 0.7597 - val_loss: 0.4634 - val_acc: 0.7727\n",
      "Epoch 5/25\n",
      "181467/181467 [==============================] - 87s 481us/step - loss: 0.4736 - acc: 0.7652 - val_loss: 0.4579 - val_acc: 0.7754\n",
      "Epoch 6/25\n",
      "181467/181467 [==============================] - 86s 475us/step - loss: 0.4676 - acc: 0.7694 - val_loss: 0.4528 - val_acc: 0.7793\n",
      "Epoch 7/25\n",
      "181467/181467 [==============================] - 87s 477us/step - loss: 0.4637 - acc: 0.7708 - val_loss: 0.4543 - val_acc: 0.7773\n",
      "Epoch 8/25\n",
      "181467/181467 [==============================] - 87s 479us/step - loss: 0.4593 - acc: 0.7751 - val_loss: 0.4646 - val_acc: 0.7709\n",
      "Epoch 9/25\n",
      "181467/181467 [==============================] - 86s 476us/step - loss: 0.4561 - acc: 0.7770 - val_loss: 0.4462 - val_acc: 0.7824\n",
      "Epoch 10/25\n",
      "181467/181467 [==============================] - 86s 476us/step - loss: 0.4529 - acc: 0.7785 - val_loss: 0.4457 - val_acc: 0.7834\n",
      "Epoch 11/25\n",
      "181467/181467 [==============================] - 87s 477us/step - loss: 0.4509 - acc: 0.7798 - val_loss: 0.4424 - val_acc: 0.7857\n",
      "Epoch 12/25\n",
      "181467/181467 [==============================] - 87s 482us/step - loss: 0.4474 - acc: 0.7829 - val_loss: 0.4459 - val_acc: 0.7821\n",
      "Epoch 13/25\n",
      "181467/181467 [==============================] - 87s 479us/step - loss: 0.4449 - acc: 0.7848 - val_loss: 0.4397 - val_acc: 0.7871\n",
      "Epoch 14/25\n",
      "181467/181467 [==============================] - 87s 477us/step - loss: 0.4428 - acc: 0.7868 - val_loss: 0.4379 - val_acc: 0.7891\n",
      "Epoch 15/25\n",
      "181467/181467 [==============================] - 87s 482us/step - loss: 0.4418 - acc: 0.7867 - val_loss: 0.4405 - val_acc: 0.7870\n",
      "Epoch 16/25\n",
      "181467/181467 [==============================] - 87s 479us/step - loss: 0.4399 - acc: 0.7874 - val_loss: 0.4378 - val_acc: 0.7900\n",
      "Epoch 17/25\n",
      "181467/181467 [==============================] - 87s 478us/step - loss: 0.4382 - acc: 0.7889 - val_loss: 0.4429 - val_acc: 0.7862\n",
      "Epoch 18/25\n",
      "181467/181467 [==============================] - 87s 477us/step - loss: 0.4356 - acc: 0.7904 - val_loss: 0.4398 - val_acc: 0.7869\n",
      "Epoch 19/25\n",
      "181467/181467 [==============================] - 87s 482us/step - loss: 0.4347 - acc: 0.7904 - val_loss: 0.4353 - val_acc: 0.7899\n",
      "Epoch 20/25\n",
      "181467/181467 [==============================] - 86s 474us/step - loss: 0.4339 - acc: 0.7910 - val_loss: 0.4362 - val_acc: 0.7902\n",
      "Epoch 21/25\n",
      "181467/181467 [==============================] - 87s 477us/step - loss: 0.4320 - acc: 0.7923 - val_loss: 0.4393 - val_acc: 0.7878\n",
      "Epoch 22/25\n",
      " 17920/181467 [=>............................] - ETA: 1:10 - loss: 0.4256 - acc: 0.8022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-095f01550c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m model.fit(X_train, y_train, batch_size=batch_size, epochs=25,\n\u001b[0;32m---> 51\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     52\u001b[0m score, acc = model.evaluate(X_test, y_test,\n\u001b[1;32m     53\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337) \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 40  \n",
    "batch_size = 256 \n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    len(word_index)+1,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxlen,\n",
    "    trainable=False\n",
    "))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(GRU(32)))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-3,clipnorm=3,clipvalue=3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=25,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpGXnS8mUz7l"
   },
   "outputs": [],
   "source": [
    "# Классический вариант для кэгла\n",
    "# x = L.SpatialDropout1D(0.2)(x)\n",
    "# x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n",
    "\n",
    "# avg_pool1 = L.GlobalAveragePooling1D()(x)\n",
    "# max_pool1 = L.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# x = L.concatenate([att,avg_pool1, max_pool1])\n",
    "\n",
    "# preds = L.Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Твиты.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
